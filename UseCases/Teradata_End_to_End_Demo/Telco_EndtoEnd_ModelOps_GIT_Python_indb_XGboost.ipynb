{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc5ef2d4-514a-4c8a-8741-4336263ec7ec",
   "metadata": {},
   "source": [
    "<header>\n",
    "   <p  style='font-size:36px;font-family:Arial; color:#F0F0F0; background-color: #00233c; padding-left: 20pt; padding-top: 20pt;padding-bottom: 10pt; padding-right: 20pt;'>\n",
    "       ModelOps : In-Database XGBoost using Git for Telco Churn\n",
    "  <br>\n",
    "       <img id=\"teradata-logo\" src=\"https://storage.googleapis.com/clearscape_analytics_demo_data/DEMO_Logo/teradata.svg\" alt=\"Teradata\" style=\"width: 125px; height: auto; margin-top: 20pt;\">\n",
    "    </p>\n",
    "</header>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e628a88",
   "metadata": {
    "tags": []
   },
   "source": [
    "<p style ='font-size:18px;font-family:Arial;color:#00233C'><b>Introduction</b></p>\n",
    "\n",
    "<p style ='font-size:16px;font-family:Arial;color:#00233C'>This Notebook is a part of the Teradata End-to-End Telco Customer Churn usecase and should be executed only after the Traditional Approach notebook is executed.</p>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial;color:#00233C'>In this Notebook will we go throught the process on how to work with ClearScape Analytics in-database functions with ModelOps. With in-database analytics you can solve your scalable challenges by using Vantage to train and score your models. Whether you have a big volume of data or you want to avoid the data movement implementation to train models outside Vantage, you can use ModelOps to manage your Catalog of Models from multiple platforms including in-database algorithms.<br>To know more about in-database algorithms review teradata official documentation.</p>\n",
    " \n",
    "<p style='font-size:16px;font-family:Arial;color:#00233C'>This notebook will cover the Operationalization of the Telco Customer Churn use case with Python using the Teradata In-database XGBoost model. <strong>XGBoost</strong> is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It is one of the most used libraries by the community that solve many data science problems in a fast and accurate way.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c675a412-ac1c-4dbb-b4e3-21e847c3295b",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;background-color:#00233C;\">\n",
    "<p><b style = 'font-size:20px;font-family:Arial;color:#00233C'>1. Configure the Environment</b></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5200df2-2a75-4c78-8c86-2e4382dbbc6c",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#00233C'><b>1.1 Set up Git repository</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We will need to set up git repository for using the In-Db functions in the ModelOps cycle.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbdc8f5-c1d3-4c15-906b-a58629eea689",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'><b>Note: </b><i>In case  the <b>git repository</b> for your user is not set up, please refer to the notebook located at</i> <b>ModelOps/06_ModelOps_GIT_Project_Setup.ipynb</b> and set up the git repository using the steps mentioned in the notebook. This will be required for execution of the steps below to create the ModelOps cycle for this End-to_End Demo.</p>\n",
    "    <a href=\"./06_ModelOps_GIT_Project_Setup.ipynb\" style=\"font-size:16px; font-family:Arial; color:white; background-color:#017373; padding:10px 20px; border-radius:5px; text-decoration:none; display:inline-block;\">\n",
    "  06_ModelOps_GIT_Project_Setup.ipynb &gt;&gt;\n",
    "    </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be62e431-e39a-4baa-b02d-1dc25a36f559",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#00233C'><b>1.2 Libraries installation</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'><b>A restart of the Kernel is needed to confirm changes</b>. We use -q parameter for a non-verbose log of the installation command, you may remove this parameter if you want to know all the steps of the pip installation.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b499f2f4-45a0-49ba-bdf8-7a786cf01e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#%pip install -q teradataml==17.20.0.6 teradatamodelops==7.0.3 matplotlib==3.8.2\n",
    "%pip install -q teradataml==20.0.0.3 teradatasqlalchemy==20.0.0.3 teradatamodelops==7.0.6 matplotlib==3.8.2 scikit-learn==1.1.3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5691c81c-09e7-4acc-ae80-f4007cfc4522",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'><b>Hint:</b><i>The easy way to restart the kernel to bring the above installed software into memory is to type zero zero (<b> 0 0 </b>). </i></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b392c630-2106-4619-b4a8-ceaba6cb7ce2",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;background-color:#00233C;\">\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233C'><b>1.2 Libraries import</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Here, we import the required libraries, set environment variables and environment paths (if required).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6fbde24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from teradataml import (\n",
    "    create_context, \n",
    "    remove_context,\n",
    "    get_context,\n",
    "    get_connection,\n",
    "    DataFrame,\n",
    "    TrainTestSplit,\n",
    "    copy_to_sql,\n",
    "    db_drop_table,\n",
    "    configure,\n",
    "    execute_sql\n",
    ")\n",
    "import os\n",
    "import getpass\n",
    "import logging\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245c1005-e835-430e-a48f-c3962b72f3c8",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;background-color:#00233C;\">\n",
    "<p><b style = 'font-size:20px;font-family:Arial;color:#00233C'>2. Connect to Vantage</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a1f46c-7798-4fc9-a710-750c56e8a299",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>You will be prompted to provide the password. Enter your password, press the Enter key, then use down arrow to go to next cell. Begin running steps with Shift + Enter keys.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "702a57c1-6013-4f92-bbac-b4278f1fb86b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing setup ...\n",
      "Setup complete\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter password:  ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Logon successful\n",
      "Connected as: xxxxxsql://demo_user:xxxxx@host.docker.internal/dbc\n",
      "Engine(teradatasql://demo_user:***@host.docker.internal)\n"
     ]
    }
   ],
   "source": [
    "%run -i ../startup.ipynb\n",
    "eng = create_context(host = 'host.docker.internal', username='demo_user', password = password)\n",
    "print(eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1be9f1e-8920-4d00-85ba-6f0e168231cc",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;background-color:#00233C;\">\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233C'><b>2.1 Set up Install locations and model local path</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Here, we will configure the install locations for VAL and BYOM. We will also create a local path(similar to the git path) for the code which will be used later to commit the code to the git repository.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce03f507-ff4c-4d5d-902f-8c362fe59fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "execute_sql('''SET query_band='DEMO=PP_Telco_EndtoEnd_ModelOps_GIT_Python_indb_XGboost.ipynb;' UPDATE FOR SESSION; ''')\n",
    "\n",
    "# configure byom/val installation\n",
    "configure.val_install_location = \"VAL\"\n",
    "configure.byom_install_location = \"MLDB\"\n",
    "\n",
    "# set the path to the local project repository for this model demo\n",
    "model_local_path = '~/modelops-demo-models/model_definitions/telco_python_indb_xgboost'\n",
    "res = os.system(f'mkdir -p {model_local_path}/model_modules')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42d0b9d5-3141-4342-a275-cdb1367ce0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~/modelops-demo-models/model_definitions/telco_python_indb_xgboost\n"
     ]
    }
   ],
   "source": [
    "print(model_local_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4587dd9-1c0d-44d0-be40-26e574c1096e",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;background-color:#00233C;\">\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233C'><b>2.2 Getting Data for This Demo</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Since this is a continuation of the Traditional approach notebook, we will be using the data from the same table we created in the earlier notebook.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c65dde8-c2e9-4bc6-b1f8-b331a8d6ae19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "\ttable {border:ridge 5px;}\n",
       "\ttable td {border:inset 1px;}\n",
       "\ttable tr#HeaderRow {background-color:grey; color:white;}</style>\n",
       "<html><table>\n",
       "\t<tr id=\"HeaderRow\">\n",
       "\t\t<th>CustomerID</th>\n",
       "\t\t<th>Tenure</th>\n",
       "\t\t<th>InternetService</th>\n",
       "\t\t<th>OnlineSecurity</th>\n",
       "\t\t<th>SeniorCitizen</th>\n",
       "\t\t<th>PaymentMethod</th>\n",
       "\t\t<th>OnlineBackup</th>\n",
       "\t\t<th>Dependents</th>\n",
       "\t\t<th>Partner</th>\n",
       "\t\t<th>MultipleLines</th>\n",
       "\t\t<th>StreamingMovies</th>\n",
       "\t\t<th>Gender</th>\n",
       "\t\t<th>PhoneService</th>\n",
       "\t\t<th>TotalCharges</th>\n",
       "\t\t<th>Contract</th>\n",
       "\t\t<th>MonthlyCharges</th>\n",
       "\t\t<th>Churn</th>\n",
       "\t\t<th>DeviceProtection</th>\n",
       "\t\t<th>PaperlessBilling</th>\n",
       "\t\t<th>StreamingTV</th>\n",
       "\t\t<th>TechSupport</th>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>4829-AUOAX</td>\n",
       "\t\t<td>46</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>0.5065747052321298</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>0.7741293532338308</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>0</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>1658-BYGOY</td>\n",
       "\t\t<td>18</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>2</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>0.2017950902726603</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>0.7681592039800995</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>0</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>2990-IAJSV</td>\n",
       "\t\t<td>72</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>0.7637193717759765</td>\n",
       "\t\t<td>2</td>\n",
       "\t\t<td>0.7338308457711443</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>1</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>6330-JKLPC</td>\n",
       "\t\t<td>11</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>0.10327238393515108</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>0.6194029850746269</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>0</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>6685-GBWJZ</td>\n",
       "\t\t<td>63</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>0.5122512896094327</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>0.5228855721393034</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>1</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>4973-MGTON</td>\n",
       "\t\t<td>71</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>0.6873272844509949</td>\n",
       "\t\t<td>2</td>\n",
       "\t\t<td>0.6582089552238807</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>1</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>0654-PQKDW</td>\n",
       "\t\t<td>62</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>0.49090940493736185</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>0.5223880597014925</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>1</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>2651-ZCBXV</td>\n",
       "\t\t<td>54</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>0.6633025515843773</td>\n",
       "\t\t<td>2</td>\n",
       "\t\t<td>0.8930348258706468</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>1</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>4009-ALQFH</td>\n",
       "\t\t<td>25</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>2</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>0.27278118091378045</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>0.8084577114427861</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>0</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>5736-YEJAX</td>\n",
       "\t\t<td>69</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>0.6335839627855564</td>\n",
       "\t\t<td>2</td>\n",
       "\t\t<td>0.6089552238805971</td>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>1</td>\n",
       "\t</tr>\n",
       "</table></html>"
      ],
      "text/plain": [
       "            Tenure  InternetService  OnlineSecurity  SeniorCitizen  PaymentMethod  OnlineBackup  Dependents  Partner  MultipleLines  StreamingMovies  Gender  PhoneService  TotalCharges  Contract  MonthlyCharges  Churn  DeviceProtection  PaperlessBilling  StreamingTV  TechSupport\n",
       "CustomerID                                                                                                                                                                                                                                                                             \n",
       "4829-AUOAX      46                1               0              0              0             0           0        0              1                1       0             1      0.506575         0        0.774129      1                 0                 0            1            0\n",
       "1658-BYGOY      18                1               0              1              2             0           0        0              1                1       1             1      0.201795         0        0.768159      1                 0                 1            1            0\n",
       "2990-IAJSV      72                0               1              0              0             1           0        0              1                1       1             1      0.763719         2        0.733831      0                 1                 0            1            1\n",
       "6330-JKLPC      11                1               0              0              0             1           0        1              1                0       1             1      0.103272         0        0.619403      1                 0                 1            0            0\n",
       "6685-GBWJZ      63                0               1              0              1             1           0        1              0                0       1             1      0.512251         1        0.522886      0                 0                 0            1            1\n",
       "4973-MGTON      71                0               1              0              1             1           0        1              0                1       0             1      0.687327         2        0.658209      0                 1                 1            1            1\n",
       "0654-PQKDW      62                0               1              0              0             0           1        1              0                0       0             1      0.490909         1        0.522388      0                 1                 1            1            1\n",
       "2651-ZCBXV      54                1               1              0              1             1           0        0              1                1       1             1      0.663303         2        0.893035      0                 0                 1            1            1\n",
       "4009-ALQFH      25                1               0              0              2             1           0        0              0                1       0             1      0.272781         0        0.808458      1                 1                 1            1            0\n",
       "5736-YEJAX      69                0               1              0              1             1           1        0              1                0       1             1      0.633584         2        0.608955      0                 1                 1            1            1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_data = DataFrame('Transformed_data')\n",
    "transformed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79bd119-d754-4572-be30-0d3c7084c24f",
   "metadata": {},
   "source": [
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We use the TrainTestSplit function to divide the dataset into train and test dataset which will be copied to Vantge to be used in the ModelOps cycle.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d34fc839-a6d0-4e05-8b71-afd8201923a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainTestSplit_out = TrainTestSplit(\n",
    "                                    data = DataFrame('Transformed_data'),\n",
    "                                    id_column = \"CustomerID\",\n",
    "                                    train_size = 0.75,\n",
    "                                    test_size = 0.25,\n",
    "                                    seed = 21\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13eb4243-b345-487c-809d-1223f4a7e94b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split into 2 virtual dataframes\n",
    "df_train = TrainTestSplit_out.result[TrainTestSplit_out.result['TD_IsTrainRow'] == 1].drop(['TD_IsTrainRow'], axis = 1)\n",
    "df_test = TrainTestSplit_out.result[TrainTestSplit_out.result['TD_IsTrainRow'] == 0].drop(['TD_IsTrainRow'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea5f31c2-7f0a-4225-81c8-a6027af4d1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_to_sql(df_train, table_name='transform_data_train', if_exists='replace')\n",
    "copy_to_sql(df_test, table_name='transform_data_test', if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abc3f95-2694-4f13-af04-3de523a04430",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;background-color:#00233C;\">\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233C'><b>2.3 Creating predictions and model table</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We will create a predictions table where we get our model predictions and the model table where we will upload the model created.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07a3e8f5-2ee8-4ace-83b7-e56c083effab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ddl for Aoa_Byom_Models \n",
    "query = '''CREATE SET TABLE DEMO_USER.Aoa_Byom_Models \n",
    "     (\n",
    "      model_version VARCHAR(255) CHARACTER SET LATIN NOT CASESPECIFIC,\n",
    "      model_id VARCHAR(255) CHARACTER SET LATIN NOT CASESPECIFIC,\n",
    "      model_type VARCHAR(255) CHARACTER SET LATIN NOT CASESPECIFIC,\n",
    "      project_id VARCHAR(255) CHARACTER SET LATIN NOT CASESPECIFIC,\n",
    "      deployed_at TIMESTAMP(6) DEFAULT CURRENT_TIMESTAMP(6),\n",
    "      model BLOB(2097088000))\n",
    "UNIQUE PRIMARY INDEX ( model_version );\n",
    "'''\n",
    " \n",
    "try:\n",
    "    execute_sql(query)\n",
    "except:\n",
    "    execute_sql('DROP TABLE DEMO_USER.Aoa_Byom_Models;')\n",
    "    execute_sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3d6c0b9-f6b9-416c-b788-11cd8a6a9723",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ddl for Telco_Churn_Predictions\n",
    "query = '''CREATE MULTISET TABLE Telco_Churn_Predictions \n",
    "     (\n",
    "      job_id VARCHAR(255) CHARACTER SET LATIN NOT CASESPECIFIC,\n",
    "      CustomerID VARCHAR(10) CHARACTER SET LATIN,\n",
    "      Churn BYTEINT,\n",
    "      json_report CLOB(1048544000) CHARACTER SET LATIN)\n",
    "PRIMARY INDEX ( job_id );;\n",
    "'''\n",
    "try:\n",
    "    execute_sql(query)\n",
    "except:\n",
    "    db_drop_table('Telco_Churn_Predictions')\n",
    "    execute_sql(query) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c1c700-ba57-4c31-9fd2-3434023e6640",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Next is an optional step – if you want to see the status of databases/tables created and space used.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be4b98a7-ed3a-4873-89b9-538bfbdc51a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have:  #databases=9 #tables=169 #views=261  You have used 80.4 MB of 30,678.3 MB available - 0.3%  ... Space Usage OK\n",
      " \n",
      "   Database Name                  #tables  #views     Avail MB      Used MB\n",
      "   demo_user                          125     256  27,598.5 MB      66.2 MB \n",
      "   DEMO_GLM_Fraud                       0       1       0.0 MB       0.0 MB \n",
      "   DEMO_GLM_Fraud_db                    1       0     195.9 MB       7.3 MB \n",
      "   DEMO_ModelOps                        0       3       0.0 MB       0.0 MB \n",
      "   DEMO_ModelOps_db                     3       0      19.1 MB       0.6 MB \n",
      "   DEMO_Telco                           0       1       0.0 MB       0.0 MB \n",
      "   DEMO_Telco_db                        1       0       3.8 MB       0.8 MB \n",
      "   FinFraud                            13       0     953.7 MB       2.0 MB \n",
      "   FinRepo                             13       0     953.7 MB       1.6 MB \n",
      "   TelcoFS                             13       0     953.7 MB       2.0 MB \n"
     ]
    }
   ],
   "source": [
    "%run -i ../run_procedure.py \"call space_report();\"        # Takes 10 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6047debc-f5c6-4fd3-850b-8c191b69e8b7",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;background-color:#00233C;\">\n",
    "<p><b style = 'font-size:20px;font-family:Arial;color:#00233C'>3. Define Training, Evaluation and Scoring functions </b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We will need to create below 3 .py files to be used in the ModelOps cycle.</p>\n",
    "\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'><code>training.py</code>: The code using In-Db functions to train the model.</li>\n",
    "\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'><code>evaluation.py</code>: The code using In-Db functions to evaluate the model.</li>\n",
    "\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'><code>scoring.py</code>: The code using In-Db functions for scoring new data.</li>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The steps below show the way of creating these 3 files and test the code before commiting it to the repository. After testing the code we set up various configuration files and than push the .py and configuration files to the git to be used from the ModelOps UI.</p> \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfea1e5-d74c-4b41-88f3-b40ed80f0d9b",
   "metadata": {
    "tags": []
   },
   "source": [
    "<hr style=\"height:1px;border:none;background-color:#00233C;\">\n",
    "<p><b style = 'font-size:20px;font-family:Arial;color:#00233C'>3.1 Define Training Function</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The training function takes the following shape </p>\n",
    "\n",
    "```python\n",
    "def train(context: ModelContext, **kwargs):\n",
    "    aoa_create_context()\n",
    "    \n",
    "    # your training code using teradataml indDB function\n",
    "    model = <InDB Function>(...)\n",
    "    \n",
    "    # save your model\n",
    "    model.result.to_sql(f\"model_${context.model_version}\", if_exists=\"replace\")  \n",
    "    \n",
    "    record_training_stats(...)\n",
    "```\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>You can execute this from the CLI or directly within the notebook as shown. The below code will created the training.py file in the local model path.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a7fb724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /home/jovyan/modelops-demo-models/model_definitions/telco_python_indb_xgboost/model_modules/training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $model_local_path/model_modules/training.py\n",
    "from teradataml import (\n",
    "    DataFrame,\n",
    "    XGBoost,\n",
    "    ScaleFit,\n",
    "    ScaleTransform,\n",
    "    OrdinalEncodingFit,\n",
    "    ColumnTransformer,\n",
    "    Shap\n",
    ")\n",
    "\n",
    "from aoa import (\n",
    "    record_training_stats,\n",
    "    aoa_create_context,\n",
    "    ModelContext\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def compute_feature_importance(feat_df):\n",
    "    df = feat_df.to_pandas()\n",
    "    df = df.T.reset_index()\n",
    "    df=df.rename(columns={'index': 'Feature', 0: 'Importance'})\n",
    "    df['Feature'] = df['Feature'].str.replace('TD_', '')\n",
    "    df['Feature'] = df['Feature'].str.replace('_SHAP', '')\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_feature_explain(explain_df):\n",
    "    explain_df = explain_df.drop(['CustomerID','Label','tree_num'],axis=1)\n",
    "    shap_mean = explain_df.agg(['min', 'max'])\n",
    "    df = shap_mean.to_pandas()\n",
    "    df = df.T.reset_index()\n",
    "    df=df.rename(columns={'index': 'Feature', 0: 'Importance'})\n",
    "    mean_positive = df[df['Importance'] > 0]\n",
    "    mean_negative = df[df['Importance'] < 0]\n",
    "    mean_positive['Feature'] = mean_positive.loc[:,'Feature'].str.replace('max_TD_', '')\n",
    "    mean_positive['Feature'] = mean_positive.loc[:,'Feature'].str.replace('_SHAP', '')\n",
    "    mean_negative['Feature'] = mean_negative.loc[:,'Feature'].str.replace('min_TD_', '')\n",
    "    mean_negative['Feature'] = mean_negative.loc[:,'Feature'].str.replace('_SHAP', '')\n",
    "    # mean_positive['Feature'] = mean_positive['Feature'].str.replace('max_TD_', '')\n",
    "    # mean_positive['Feature'] = mean_positive['Feature'].str.replace('_SHAP', '')\n",
    "    # mean_negative['Feature'] = mean_negative['Feature'].str.replace('min_TD_', '')\n",
    "    # mean_negative['Feature'] = mean_negative['Feature'].str.replace('_SHAP', '')\n",
    "    return mean_positive,mean_negative\n",
    "    \n",
    "\n",
    "def plot_feature_importance(df, img_filename):\n",
    "    df = df.sort_values(by=\"Importance\", ascending=False)\n",
    "    # Plot the bar graph\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(x=\"Importance\",y=\"Feature\",data=df, palette=\"viridis\")\n",
    "    plt.title(\"Feature Importance\")\n",
    "    plt.xlabel(\"SHAP Importance Value\")\n",
    "    plt.ylabel(\"Features\")\n",
    "    plt.tight_layout()\n",
    "    fig = plt.gcf()\n",
    "    fig.savefig(img_filename, dpi=500)\n",
    "    plt.clf()\n",
    "    \n",
    "def plot_feature_explain(mean_positive,mean_negative, img_filename):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    bar_width = 0.35\n",
    "\n",
    "    ax.barh(mean_positive[\"Feature\"], mean_positive[\"Importance\"],color='salmon', label='-1 (positive)') \n",
    "    ax.barh(mean_negative[\"Feature\"], mean_negative[\"Importance\"],color='cyan', label='1 (negative)')\n",
    "    ax.set_xlabel(\"mean(|SHAP value|)\")\n",
    "    ax.set_title(\"Mean shap for all samples\")\n",
    "    ax.legend(title=\"sign\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    # plt.show()\n",
    "    fig = plt.gcf()\n",
    "    fig.savefig(img_filename, dpi=500)\n",
    "    plt.clf()    \n",
    "    \n",
    "def train(context: ModelContext, **kwargs):\n",
    "    aoa_create_context()\n",
    "    \n",
    "    # Extracting feature names, target name, and entity key from the context\n",
    "    feature_names = context.dataset_info.feature_names\n",
    "    target_name = context.dataset_info.target_names[0]\n",
    "    entity_key = context.dataset_info.entity_key\n",
    "\n",
    "    # Load the training data from Teradata\n",
    "    train_df = DataFrame.from_query(context.dataset_info.sql)\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "\n",
    "    # Train the model using XGBoost\n",
    "    model = XGBoost(\n",
    "                   data = train_df,\n",
    "                   input_columns = [\"Tenure\", \"InternetService\", \"OnlineSecurity\", \"SeniorCitizen\",\n",
    "                                    \"PaymentMethod\", \"OnlineBackup\", \"Dependents\", \"Partner\", \"MultipleLines\", \n",
    "                                    \"StreamingMovies\", \"Gender\", \"PhoneService\", \"TotalCharges\", \"Contract\", \n",
    "                                    \"MonthlyCharges\", \"DeviceProtection\", \"PaperlessBilling\", \"StreamingTV\", \n",
    "                                    \"TechSupport\"],\n",
    "                   response_column = 'Churn',\n",
    "                   model_type = 'CLASSIFICATION',\n",
    "                     )\n",
    "\n",
    "    # Save the trained model to SQL\n",
    "    model.result.to_sql(f\"model_${context.model_version}\", if_exists=\"replace\")  \n",
    "    print(\"Saved trained model\")\n",
    "    \n",
    "    #Shap explainer \n",
    "    Shap_out = Shap(data=train_df, \n",
    "                object=model.result, \n",
    "                id_column='CustomerID',\n",
    "                training_function=\"TD_XGBOOST\", \n",
    "                model_type=\"Classification\",\n",
    "                input_columns=feature_names, \n",
    "                detailed=True)\n",
    "    \n",
    "    feat_df = Shap_out.output_data\n",
    "    explain_df = Shap_out.result\n",
    "    # print(explain_df)\n",
    "\n",
    " \n",
    "    df = compute_feature_importance(feat_df)\n",
    "    plot_feature_importance(df, f\"{context.artifact_output_path}/feature_importance\")\n",
    "    pos_expl_df, neg_expl_df = compute_feature_explain(explain_df)\n",
    "    # print(pos_expl_df)\n",
    "    # print(neg_expl_df)\n",
    "    plot_feature_explain(pos_expl_df,neg_expl_df, f\"{context.artifact_output_path}/feature_explainability\")\n",
    "\n",
    "    record_training_stats(\n",
    "        train_df,\n",
    "        features=feature_names,\n",
    "        targets=[target_name],\n",
    "        categorical=[target_name],\n",
    "        # feature_importance=feature_importance,\n",
    "        context=context\n",
    "    )\n",
    "    \n",
    "    print(\"All done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09955e13-76c3-491e-a945-06fe06e1a961",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'><b> Test the train </b><code>train(context: ModelContext, **kwargs) </code> <b>function</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The below code is for testing the train function and will not be a part of the git</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c437521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Saved trained model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't read statistics metadata, assuming it's empty\n",
      "Feature streamingmovies doesn't have statistics metadata defined, and will not be monitored.\n",
      "In order to enable monitoring for this feature, make sure that statistics metadata is availabe in DEMO_ModelOps.aoa_statistics_metadata\n",
      "Feature contract doesn't have statistics metadata defined, and will not be monitored.\n",
      "In order to enable monitoring for this feature, make sure that statistics metadata is availabe in DEMO_ModelOps.aoa_statistics_metadata\n",
      "Feature paperlessbilling doesn't have statistics metadata defined, and will not be monitored.\n",
      "In order to enable monitoring for this feature, make sure that statistics metadata is availabe in DEMO_ModelOps.aoa_statistics_metadata\n",
      "Feature seniorcitizen doesn't have statistics metadata defined, and will not be monitored.\n",
      "In order to enable monitoring for this feature, make sure that statistics metadata is availabe in DEMO_ModelOps.aoa_statistics_metadata\n",
      "Feature gender doesn't have statistics metadata defined, and will not be monitored.\n",
      "In order to enable monitoring for this feature, make sure that statistics metadata is availabe in DEMO_ModelOps.aoa_statistics_metadata\n",
      "Feature totalcharges doesn't have statistics metadata defined, and will not be monitored.\n",
      "In order to enable monitoring for this feature, make sure that statistics metadata is availabe in DEMO_ModelOps.aoa_statistics_metadata\n",
      "Feature multiplelines doesn't have statistics metadata defined, and will not be monitored.\n",
      "In order to enable monitoring for this feature, make sure that statistics metadata is availabe in DEMO_ModelOps.aoa_statistics_metadata\n",
      "Feature dependents doesn't have statistics metadata defined, and will not be monitored.\n",
      "In order to enable monitoring for this feature, make sure that statistics metadata is availabe in DEMO_ModelOps.aoa_statistics_metadata\n",
      "Feature paymentmethod doesn't have statistics metadata defined, and will not be monitored.\n",
      "In order to enable monitoring for this feature, make sure that statistics metadata is availabe in DEMO_ModelOps.aoa_statistics_metadata\n",
      "Feature phoneservice doesn't have statistics metadata defined, and will not be monitored.\n",
      "In order to enable monitoring for this feature, make sure that statistics metadata is availabe in DEMO_ModelOps.aoa_statistics_metadata\n",
      "Feature techsupport doesn't have statistics metadata defined, and will not be monitored.\n",
      "In order to enable monitoring for this feature, make sure that statistics metadata is availabe in DEMO_ModelOps.aoa_statistics_metadata\n",
      "Feature onlinebackup doesn't have statistics metadata defined, and will not be monitored.\n",
      "In order to enable monitoring for this feature, make sure that statistics metadata is availabe in DEMO_ModelOps.aoa_statistics_metadata\n",
      "Feature partner doesn't have statistics metadata defined, and will not be monitored.\n",
      "In order to enable monitoring for this feature, make sure that statistics metadata is availabe in DEMO_ModelOps.aoa_statistics_metadata\n",
      "Feature tenure doesn't have statistics metadata defined, and will not be monitored.\n",
      "In order to enable monitoring for this feature, make sure that statistics metadata is availabe in DEMO_ModelOps.aoa_statistics_metadata\n",
      "Feature monthlycharges doesn't have statistics metadata defined, and will not be monitored.\n",
      "In order to enable monitoring for this feature, make sure that statistics metadata is availabe in DEMO_ModelOps.aoa_statistics_metadata\n",
      "Feature deviceprotection doesn't have statistics metadata defined, and will not be monitored.\n",
      "In order to enable monitoring for this feature, make sure that statistics metadata is availabe in DEMO_ModelOps.aoa_statistics_metadata\n",
      "Feature internetservice doesn't have statistics metadata defined, and will not be monitored.\n",
      "In order to enable monitoring for this feature, make sure that statistics metadata is availabe in DEMO_ModelOps.aoa_statistics_metadata\n",
      "Feature streamingtv doesn't have statistics metadata defined, and will not be monitored.\n",
      "In order to enable monitoring for this feature, make sure that statistics metadata is availabe in DEMO_ModelOps.aoa_statistics_metadata\n",
      "Feature onlinesecurity doesn't have statistics metadata defined, and will not be monitored.\n",
      "In order to enable monitoring for this feature, make sure that statistics metadata is availabe in DEMO_ModelOps.aoa_statistics_metadata\n",
      "Couldn't read statistics metadata, assuming it's empty\n",
      "Feature churn doesn't have statistics metadata defined, and will not be monitored.\n",
      "In order to enable monitoring for this feature, make sure that statistics metadata is availabe in DEMO_ModelOps.aoa_statistics_metadata\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the ModelContext to test with. The ModelContext is created and managed automatically by ModelOps \n",
    "# when it executes your code via CLI / UI. However, for testing in the notebook, you can define as follows\n",
    "\n",
    "# define the training dataset \n",
    "sql = \"\"\"\n",
    "SELECT \n",
    "    * from demo_user.transform_data_train;\n",
    "\"\"\"\n",
    "\n",
    "feature_metadata =  {\n",
    "    \"database\": \"DEMO_ModelOps\",\n",
    "    \"table\": \"aoa_statistics_metadata\"\n",
    "}\n",
    "hyperparams = {\n",
    "    \"model_type\": \"Classification\",\n",
    "    \"scale_method\":\"RANGE\",\n",
    "    \"miss_value\":\"KEEP\",\n",
    "    \"global_scale\": \"False\",\n",
    "    \"multiplier\":\"1\",\n",
    "    \"intercept\":\"0\",\n",
    "    \"max_depth\": 8,\n",
    "    \"num_boosted_trees\": 100,\n",
    "    \"tree_size\": 0.5,\n",
    "    \"lambda1\" : 1.0\n",
    "}\n",
    "\n",
    "entity_key = \"CustomerID\"\n",
    "target_names = [\"Churn\"]\n",
    "feature_names = [\"Tenure\", \"InternetService\", \"OnlineSecurity\", \"SeniorCitizen\", \"PaymentMethod\", \"OnlineBackup\", \"Dependents\", \n",
    "\"Partner\", \"MultipleLines\", \"StreamingMovies\", \"Gender\", \"PhoneService\", \"TotalCharges\", \"Contract\", \n",
    "\"MonthlyCharges\", \"DeviceProtection\", \"PaperlessBilling\", \"StreamingTV\", \"TechSupport\"]\n",
    " \n",
    "from aoa import ModelContext, DatasetInfo\n",
    "\n",
    "dataset_info = DatasetInfo(sql=sql,\n",
    "                           entity_key=entity_key,\n",
    "                           feature_names=feature_names,\n",
    "                           target_names=target_names,\n",
    "                           feature_metadata=feature_metadata)\n",
    "\n",
    "ctx = ModelContext(hyperparams=hyperparams,\n",
    "                   dataset_info=dataset_info,\n",
    "                   artifact_output_path=\"./artifacts\",\n",
    "                   model_version=\"indb_xgboost_v1\",\n",
    "                   model_table=\"model_indb_xgboost_v1\")\n",
    "\n",
    "sys.path.append(os.path.expanduser(f\"{model_local_path}/model_modules\"))\n",
    "import training\n",
    "training.train(context=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15cf5f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 700K\n",
      "-rw-r--r-- 1 jovyan users 120K Feb 11 13:03 confusion_matrix.png\n",
      "-rw-r--r-- 1 jovyan users  11K Feb 12 05:57 data_stats.json\n",
      "-rw-r--r-- 1 jovyan users 262K Feb 12 05:57 feature_explainability.png\n",
      "-rw-r--r-- 1 jovyan users 261K Feb 12 05:57 feature_importance.png\n",
      "-rw-r--r-- 1 jovyan users  242 Feb 11 13:03 metrics.json\n",
      "-rw-r--r-- 1 jovyan users  33K Feb 11 13:03 roc_curve.png\n"
     ]
    }
   ],
   "source": [
    "# Check the generated files\n",
    "!ls -lh artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bf4a4f-7e46-4b2a-b5a2-98a5afb42cfb",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;background-color:#00233C;\">\n",
    "<p><b style = 'font-size:18px;font-family:Arial;color:#00233C'>3.2 Define Evaluation Function</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The evaluation function takes the following shape</p>\n",
    "\n",
    "```python\n",
    "def evaluate(context: ModelContext, **kwargs):\n",
    "    aoa_create_context()\n",
    "\n",
    "    # read your model from Vantage\n",
    "    model = DataFrame(f\"model_${context.model_version}\")\n",
    "    \n",
    "    # your evaluation logic\n",
    "    \n",
    "    record_evaluation_stats(...)\n",
    "```\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>You can execute this from the CLI or directly within the notebook as shown. The below code will created the evaluation.py file in the local model path.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d751b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /home/jovyan/modelops-demo-models/model_definitions/telco_python_indb_xgboost/model_modules/evaluation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $model_local_path/model_modules/evaluation.py\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from teradataml import(\n",
    "    DataFrame, \n",
    "    copy_to_sql, \n",
    "    get_context, \n",
    "    get_connection, \n",
    "    OrdinalEncodingFit, \n",
    "    ScaleFit,\n",
    "    ColumnTransformer,\n",
    "    XGBoostPredict, \n",
    "    ConvertTo, \n",
    "    ClassificationEvaluator,\n",
    "    ROC,\n",
    "    Shap\n",
    ")\n",
    "from aoa import (\n",
    "    record_evaluation_stats,\n",
    "    save_plot,\n",
    "    aoa_create_context,\n",
    "    ModelContext\n",
    ")\n",
    "\n",
    "import joblib\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "# Define function to plot a confusion matrix from given data\n",
    "def plot_confusion_matrix(cf, img_filename):\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
    "    ax.matshow(cf, cmap=plt.cm.Blues, alpha=0.3)\n",
    "    for i in range(cf.shape[0]):\n",
    "        for j in range(cf.shape[1]):\n",
    "            ax.text(x=j, y=i,s=cf[i, j], va='center', ha='center', size='xx-large')\n",
    "    ax.set_xlabel('Predicted labels');\n",
    "    ax.set_ylabel('True labels'); \n",
    "    ax.set_title('Confusion Matrix');\n",
    "    fig = plt.gcf()\n",
    "    fig.savefig(img_filename, dpi=500)\n",
    "    plt.clf()\n",
    "\n",
    "    \n",
    "def plot_roc_curve(roc_out, img_filename):\n",
    "    from teradataml import Figure\n",
    "    figure = Figure(width=500, height=400, heading=\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "    auc = roc_out.result.get_values()[0][0]\n",
    "    plot = roc_out.output_data.plot(\n",
    "        x=roc_out.output_data.fpr,\n",
    "        y=[roc_out.output_data.tpr, roc_out.output_data.fpr],\n",
    "        xlabel='False Positive Rate',\n",
    "        ylabel='True Positive Rate',\n",
    "        color='carolina blue',\n",
    "        figure=figure,\n",
    "        legend=[f'XGBoost AUC = {round(auc, 4)}', 'AUC Baseline'],\n",
    "        legend_style='lower right',\n",
    "        grid_linestyle='--',\n",
    "        grid_linewidth=0.5\n",
    "    )\n",
    "    plot.save(img_filename)\n",
    "    # plot.show()\n",
    "    # fig = plt.gcf()\n",
    "    # fig.savefig(img_filename, dpi=500)\n",
    "    # plt.clf()    \n",
    "\n",
    "def evaluate(context: ModelContext, **kwargs):\n",
    "\n",
    "    aoa_create_context()\n",
    "\n",
    "    # Load the trained model from SQL\n",
    "    model = DataFrame(f\"model_${context.model_version}\")\n",
    "\n",
    "    feature_names = context.dataset_info.feature_names\n",
    "    target_name = context.dataset_info.target_names[0]\n",
    "    entity_key = context.dataset_info.entity_key\n",
    "\n",
    "    # Load the test data from Teradata\n",
    "    test_df = DataFrame.from_query(context.dataset_info.sql)\n",
    "\n",
    "    # Make predictions using the XGBoostPredict function\n",
    "    print(\"Evaluating ...........\")\n",
    "    predictions = XGBoostPredict(\n",
    "        object=model,\n",
    "        newdata=test_df,\n",
    "        model_type = 'Classification',\n",
    "        accumulate=target_name,\n",
    "        id_column=entity_key,\n",
    "        output_prob=True,\n",
    "        output_responses=['0','1'],\n",
    "        object_order_column=['task_index', 'tree_num', 'iter', 'class_num', 'tree_order']\n",
    "    )\n",
    "\n",
    "    # Convert the predicted data into the specified format\n",
    "    predicted_data = ConvertTo(\n",
    "        data = predictions.result,\n",
    "        target_columns = [target_name,'Prediction'],\n",
    "        target_datatype = [\"INTEGER\"]\n",
    "    )\n",
    "\n",
    "    # Evaluate classification metrics using ClassificationEvaluator\n",
    "    ClassificationEvaluator_obj = ClassificationEvaluator(\n",
    "        data=predicted_data.result,\n",
    "        observation_column=target_name,\n",
    "        prediction_column='Prediction',\n",
    "        num_labels=2\n",
    "    )\n",
    "\n",
    "     # Extract and store evaluation metrics\n",
    "        \n",
    "    metrics_pd = ClassificationEvaluator_obj.output_data.to_pandas()\n",
    "\n",
    "    evaluation = {\n",
    "        'Accuracy': '{:.2f}'.format(metrics_pd.MetricValue[0]),\n",
    "        'Micro-Precision': '{:.2f}'.format(metrics_pd.MetricValue[1]),\n",
    "        'Micro-Recall': '{:.2f}'.format(metrics_pd.MetricValue[2]),\n",
    "        'Micro-F1': '{:.2f}'.format(metrics_pd.MetricValue[3]),\n",
    "        'Macro-Precision': '{:.2f}'.format(metrics_pd.MetricValue[4]),\n",
    "        'Macro-Recall': '{:.2f}'.format(metrics_pd.MetricValue[5]),\n",
    "        'Macro-F1': '{:.2f}'.format(metrics_pd.MetricValue[6]),\n",
    "        'Weighted-Precision': '{:.2f}'.format(metrics_pd.MetricValue[7]),\n",
    "        'Weighted-Recall': '{:.2f}'.format(metrics_pd.MetricValue[8]),\n",
    "        'Weighted-F1': '{:.2f}'.format(metrics_pd.MetricValue[9]),\n",
    "    }\n",
    "\n",
    "     # Save evaluation metrics to a JSON file\n",
    "    with open(f\"{context.artifact_output_path}/metrics.json\", \"w+\") as f:\n",
    "        json.dump(evaluation, f)\n",
    "        \n",
    "    # Generate and save confusion matrix plot\n",
    "    cm_df = ClassificationEvaluator_obj.result\n",
    "    # print(cm_df)\n",
    "    cm_df = cm_df.select(['CLASS_1','CLASS_2'])\n",
    "    # print(cm_df.get_values())\n",
    "    cm_df_t = cm_df.to_pandas().T\n",
    "    # print(cm_df_t.values)\n",
    "    cm = confusion_matrix(predicted_data.result.to_pandas()['Churn'], predicted_data.result.to_pandas()['Prediction'])\n",
    "    # print(cm)\n",
    "    plot_confusion_matrix(cm_df_t.values, f\"{context.artifact_output_path}/confusion_matrix\")\n",
    "\n",
    "    # Generate and save ROC curve plot\n",
    "    roc_out = ROC(\n",
    "        data=predictions.result,\n",
    "        probability_column='Prob_1',\n",
    "        observation_column=target_name,\n",
    "        positive_class='1',\n",
    "        num_thresholds=1000\n",
    "    )\n",
    "    plot_roc_curve(roc_out, f\"{context.artifact_output_path}/roc_curve\")\n",
    "\n",
    "    # Calculate feature importance and generate plot\n",
    "    # try:\n",
    "    #     model_pdf = model.result.to_pandas()['classification_tree']\n",
    "    #     feature_importance = compute_feature_importance(model_pdf)\n",
    "    #     feature_importance_df = pd.DataFrame(list(feature_importance.items()), columns=['Feature', 'Importance'])\n",
    "    #     plot_feature_importance(feature_importance, f\"{context.artifact_output_path}/feature_importance\")\n",
    "    # except:\n",
    "    #     feature_importance = {}\n",
    "\n",
    "    predictions_table = \"Telco_Churn_Predictions\"\n",
    "    copy_to_sql(df=predicted_data.result, table_name=predictions_table, index=False, if_exists=\"replace\", temporary=True)\n",
    "\n",
    "    # calculate stats if training stats exist\n",
    "    if os.path.exists(f\"{context.artifact_input_path}/data_stats.json\"):\n",
    "        record_evaluation_stats(\n",
    "            features_df=test_df,\n",
    "            predicted_df=DataFrame.from_query(f\"SELECT * FROM {predictions_table}\"),\n",
    "            # feature_importance=feature_importance,\n",
    "            context=context\n",
    "        )\n",
    "\n",
    "    print(\"All done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78762f2b-dc78-4dc8-be86-c2fe64be9167",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'><b> Test the evaluation </b><code>evaluate(context: ModelContext, **kwargs) </code> <b>function</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The below code is for testing the evaluation function and will not be a part of the git</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12ba7a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ...........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't read statistics metadata, assuming it's empty\n",
      "Feature streamingmovies doesn't have statistics metadata defined, and will not be monitored.\n",
      "In order to enable monitoring for this feature, make sure that statistics metadata is availabe in DEMO_ModelOps.aoa_statistics_metadata\n",
      "Feature contract doesn't have statistics metadata defined, and will not be monitored.\n",
      "In order to enable monitoring for this feature, make sure that statistics metadata is availabe in DEMO_ModelOps.aoa_statistics_metadata\n",
      "Feature paperlessbilling doesn't have statistics metadata defined, and will not be monitored.\n",
      "In order to enable monitoring for this feature, make sure that statistics metadata is availabe in DEMO_ModelOps.aoa_statistics_metadata\n",
      "Feature seniorcitizen doesn't have statistics metadata defined, and will not be monitored.\n",
      "In order to enable monitoring for this feature, make sure that statistics metadata is availabe in DEMO_ModelOps.aoa_statistics_metadata\n",
      "Feature gender doesn't have statistics metadata defined, and will not be monitored.\n",
      "In order to enable monitoring for this feature, make sure that statistics metadata is availabe in DEMO_ModelOps.aoa_statistics_metadata\n",
      "Feature totalcharges doesn't have statistics metadata defined, and will not be monitored.\n",
      "In order to enable monitoring for this feature, make sure that statistics metadata is availabe in DEMO_ModelOps.aoa_statistics_metadata\n",
      "Feature multiplelines doesn't have statistics metadata defined, and will not be monitored.\n",
      "In order to enable monitoring for this feature, make sure that statistics metadata is availabe in DEMO_ModelOps.aoa_statistics_metadata\n",
      "Feature dependents doesn't have statistics metadata defined, and will not be monitored.\n",
      "In order to enable monitoring for this feature, make sure that statistics metadata is availabe in DEMO_ModelOps.aoa_statistics_metadata\n",
      "Feature paymentmethod doesn't have statistics metadata defined, and will not be monitored.\n",
      "In order to enable monitoring for this feature, make sure that statistics metadata is availabe in DEMO_ModelOps.aoa_statistics_metadata\n",
      "Feature phoneservice doesn't have statistics metadata defined, and will not be monitored.\n",
      "In order to enable monitoring for this feature, make sure that statistics metadata is availabe in DEMO_ModelOps.aoa_statistics_metadata\n",
      "Feature techsupport doesn't have statistics metadata defined, and will not be monitored.\n",
      "In order to enable monitoring for this feature, make sure that statistics metadata is availabe in DEMO_ModelOps.aoa_statistics_metadata\n",
      "Feature onlinebackup doesn't have statistics metadata defined, and will not be monitored.\n",
      "In order to enable monitoring for this feature, make sure that statistics metadata is availabe in DEMO_ModelOps.aoa_statistics_metadata\n",
      "Feature partner doesn't have statistics metadata defined, and will not be monitored.\n",
      "In order to enable monitoring for this feature, make sure that statistics metadata is availabe in DEMO_ModelOps.aoa_statistics_metadata\n",
      "Feature tenure doesn't have statistics metadata defined, and will not be monitored.\n",
      "In order to enable monitoring for this feature, make sure that statistics metadata is availabe in DEMO_ModelOps.aoa_statistics_metadata\n",
      "Feature monthlycharges doesn't have statistics metadata defined, and will not be monitored.\n",
      "In order to enable monitoring for this feature, make sure that statistics metadata is availabe in DEMO_ModelOps.aoa_statistics_metadata\n",
      "Feature deviceprotection doesn't have statistics metadata defined, and will not be monitored.\n",
      "In order to enable monitoring for this feature, make sure that statistics metadata is availabe in DEMO_ModelOps.aoa_statistics_metadata\n",
      "Feature internetservice doesn't have statistics metadata defined, and will not be monitored.\n",
      "In order to enable monitoring for this feature, make sure that statistics metadata is availabe in DEMO_ModelOps.aoa_statistics_metadata\n",
      "Feature streamingtv doesn't have statistics metadata defined, and will not be monitored.\n",
      "In order to enable monitoring for this feature, make sure that statistics metadata is availabe in DEMO_ModelOps.aoa_statistics_metadata\n",
      "Feature onlinesecurity doesn't have statistics metadata defined, and will not be monitored.\n",
      "In order to enable monitoring for this feature, make sure that statistics metadata is availabe in DEMO_ModelOps.aoa_statistics_metadata\n",
      "Couldn't read statistics metadata, assuming it's empty\n",
      "Feature churn doesn't have statistics metadata defined, and will not be monitored.\n",
      "In order to enable monitoring for this feature, make sure that statistics metadata is availabe in DEMO_ModelOps.aoa_statistics_metadata\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!\n",
      "{'Accuracy': '0.80', 'Micro-Precision': '0.80', 'Micro-Recall': '0.80', 'Micro-F1': '0.80', 'Macro-Precision': '0.75', 'Macro-Recall': '0.76', 'Macro-F1': '0.76', 'Weighted-Precision': '0.80', 'Weighted-Recall': '0.80', 'Weighted-F1': '0.80'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 540x540 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the ModelContext to test with. The ModelContext is created and managed automatically by ModelOps \n",
    "# when it executes your code via CLI / UI. However, for testing in the notebook, you can define as follows\n",
    "\n",
    "# define the evaluation dataset \n",
    "sql = \"\"\"\n",
    "SELECT \n",
    "   * from transform_data_test sample 0.6;\n",
    "\"\"\"\n",
    "\n",
    "dataset_info = DatasetInfo(sql=sql,\n",
    "                           entity_key=entity_key,\n",
    "                           feature_names=feature_names,\n",
    "                           target_names=target_names,\n",
    "                           feature_metadata=feature_metadata)\n",
    "\n",
    "ctx = ModelContext(hyperparams=hyperparams,\n",
    "                   dataset_info=dataset_info,\n",
    "                   artifact_output_path=\"./artifacts\",\n",
    "                   artifact_input_path=\"./artifacts\",\n",
    "                   model_version=\"indb_xgboost_v1\",\n",
    "                   model_table=\"model_indb_xgboost_v1\")\n",
    "\n",
    "import evaluation\n",
    "evaluation.evaluate(context=ctx)\n",
    "\n",
    "# view evaluation results\n",
    "import json\n",
    "with open(f\"{ctx.artifact_output_path}/metrics.json\") as f:\n",
    "    print(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ce4ba2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 696K\n",
      "-rw-r--r-- 1 jovyan users 116K Feb 12 05:58 confusion_matrix.png\n",
      "-rw-r--r-- 1 jovyan users  11K Feb 12 05:58 data_stats.json\n",
      "-rw-r--r-- 1 jovyan users 262K Feb 12 05:57 feature_explainability.png\n",
      "-rw-r--r-- 1 jovyan users 261K Feb 12 05:57 feature_importance.png\n",
      "-rw-r--r-- 1 jovyan users  242 Feb 12 05:58 metrics.json\n",
      "-rw-r--r-- 1 jovyan users  33K Feb 12 05:58 roc_curve.png\n"
     ]
    }
   ],
   "source": [
    "# Check the generated files\n",
    "!ls -lh artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae441da-9858-4a91-8f70-d51b92e0ee90",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;background-color:#00233C;\">\n",
    "<p><b style = 'font-size:18px;font-family:Arial;color:#00233C'>3.3 Define Scoring Function</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The scoring function takes the following shape</p>\n",
    "\n",
    "```python\n",
    "def score(context: ModelContext, **kwargs):\n",
    "    aoa_create_context()\n",
    "\n",
    "    # read your model\n",
    "    model = DataFrame(f\"model_${context.model_version}\")\n",
    "    \n",
    "    # your evaluation logic\n",
    "    \n",
    "    record_scoring_stats(...)\n",
    "```\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>You can execute this from the CLI or directly within the notebook as shown. The below code will created the scoring.py file in the local model path.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb4d4c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /home/jovyan/modelops-demo-models/model_definitions/telco_python_indb_xgboost/model_modules/scoring.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $model_local_path/model_modules/scoring.py\n",
    "from teradataml import (\n",
    "    copy_to_sql,\n",
    "    DataFrame,\n",
    "    XGBoostPredict,\n",
    "    OrdinalEncodingFit,\n",
    "    ScaleFit,\n",
    "    ColumnTransformer,\n",
    "    ConvertTo,\n",
    "    translate\n",
    ")\n",
    "from aoa import (\n",
    "    record_scoring_stats,\n",
    "    aoa_create_context,\n",
    "    ModelContext\n",
    ")\n",
    "import pandas as pd\n",
    "from teradatasqlalchemy import INTEGER\n",
    "\n",
    "\n",
    "def score(context: ModelContext, **kwargs):\n",
    "    \n",
    "    aoa_create_context()\n",
    "\n",
    "    # Load the trained model from SQL\n",
    "    model = DataFrame(f\"model_${context.model_version}\")\n",
    "\n",
    "    # Extract feature names, target name, and entity key from the context\n",
    "    feature_names = context.dataset_info.feature_names\n",
    "    target_name = context.dataset_info.target_names[0]\n",
    "    entity_key = context.dataset_info.entity_key\n",
    "\n",
    "    # Load the test dataset\n",
    "    test_df = DataFrame.from_query(context.dataset_info.sql)\n",
    "    features_tdf = DataFrame.from_query(context.dataset_info.sql)\n",
    "\n",
    "    print(\"Scoring...\")\n",
    "    # Make predictions using the XGBoostPredict function\n",
    "    predictions = XGBoostPredict(\n",
    "        object=model,\n",
    "        newdata=test_df,\n",
    "        model_type = 'Classification',\n",
    "        # accumulate=target_name,\n",
    "        id_column=entity_key,\n",
    "        output_prob=True,\n",
    "        output_responses=['0','1'],\n",
    "        object_order_column=['task_index', 'tree_num', 'iter', 'class_num', 'tree_order']\n",
    "    )\n",
    "    \n",
    "    # Convert predictions to pandas DataFrame and process\n",
    "    # predictions_pdf = predictions.result.to_pandas(all_rows=True).rename(columns={\"Prediction\": target_name}).astype(int)\n",
    "    predictions_df = predictions.result\n",
    "    # print(predictions_df)\n",
    "    predictions_pdf = predictions_df.assign(drop_columns=True,\n",
    "                                             job_id=translate(context.job_id),\n",
    "                                             CustomerID=predictions_df.CustomerID,\n",
    "                                             Churn=predictions_df.Prediction.cast(type_=INTEGER),\n",
    "                                             json_report=translate(\"  \"))\n",
    "                                             \n",
    "    \n",
    "    \n",
    "    # converted_data = ConvertTo(data = predictions_pdf,\n",
    "    #                            target_columns = ['job_id','PatientId', 'HasDiabetes','json_report'],\n",
    "    #                            target_datatype = [\"VARCHAR(charlen=255,charset=LATIN,casespecific=NO)\"\n",
    "    #                                               ,\"integer\",\"integer\",\"VARCHAR(charlen=5000,charset=LATIN)\"])\n",
    "    # df=converted_data.result\n",
    "    \n",
    "    # print(predictions_pdf)\n",
    "    print(\"Finished Scoring\")\n",
    "    # print(predictions_pdf)\n",
    "\n",
    "    # store the predictions\n",
    "\n",
    "#     # teradataml doesn't match column names on append.. and so to match / use same table schema as for byom predict\n",
    "#     # example (see README.md), we must add empty json_report column and change column order manually (v17.0.0.4)\n",
    "#     # CREATE MULTISET TABLE pima_patient_predictions\n",
    "#     # (\n",
    "#     #     job_id VARCHAR(255), -- comes from airflow on job execution\n",
    "#     #     PatientId BIGINT,    -- entity key as it is in the source data\n",
    "#     #     HasDiabetes BIGINT,   -- if model automatically extracts target\n",
    "#     #     json_report CLOB(1048544000) CHARACTER SET UNICODE  -- output of\n",
    "#     # )\n",
    "#     # PRIMARY INDEX ( job_id );\n",
    "\n",
    "    copy_to_sql(\n",
    "        df=predictions_pdf,\n",
    "        schema_name=context.dataset_info.predictions_database,\n",
    "        table_name=context.dataset_info.predictions_table,\n",
    "        index=False,\n",
    "        if_exists=\"append\"\n",
    "    )\n",
    "    \n",
    "    print(\"Saved predictions in Teradata\")\n",
    "\n",
    "    # calculate stats\n",
    "    predictions_df = DataFrame.from_query(f\"\"\"\n",
    "        SELECT \n",
    "            * \n",
    "        FROM {context.dataset_info.get_predictions_metadata_fqtn()} \n",
    "            WHERE job_id = '{context.job_id}'\n",
    "    \"\"\")\n",
    "\n",
    "    record_scoring_stats(features_df=features_tdf, predicted_df=predictions_df, context=context)\n",
    "\n",
    "    print(\"All done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de37750-bf1c-4d3d-9ee4-510e64fec827",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'><b> Test the scoring </b><code>score(context: ModelContext, **kwargs) </code> <b>function</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The below code is for testing the score function and will not be a part of the git</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0edf735d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring...\n",
      "Finished Scoring\n"
     ]
    },
    {
     "ename": "TeradataMlException",
     "evalue": "[Teradata][teradataml](TDML_2020) Failed to copy dataframe to Teradata Vantage.[Version 20.0.0.24] [Session 2079] [Teradata Database] [Error 9124] Segmentation Violation in AMP: Please do not resubmit the last request.\n at gosqldriver/teradatasql.formatError ErrorUtil.go:92\n at gosqldriver/teradatasql.(*teradataConnection).formatDatabaseError ErrorUtil.go:252\n at gosqldriver/teradatasql.(*teradataConnection).makeChainedDatabaseError ErrorUtil.go:268\n at gosqldriver/teradatasql.(*teradataConnection).processErrorParcel TeradataConnection.go:751\n at gosqldriver/teradatasql.(*TeradataRows).processResponseBundle TeradataRows.go:2308\n at gosqldriver/teradatasql.(*TeradataRows).executeSQLRequest TeradataRows.go:874\n at gosqldriver/teradatasql.newTeradataRows TeradataRows.go:720\n at gosqldriver/teradatasql.(*teradataStatement).QueryContext TeradataStatement.go:122\n at gosqldriver/teradatasql.(*teradataConnection).QueryContext TeradataConnection.go:1261\n at database/sql.ctxDriverQuery ctxutil.go:48\n at database/sql.(*DB).queryDC.func1 sql.go:1778\n at database/sql.withLock sql.go:3566\n at database/sql.(*DB).queryDC sql.go:1773\n at database/sql.(*Conn).QueryContext sql.go:2029\n at main.createRows goside.go:1080\n at main.goCreateRows goside.go:959\n at _cgoexp_e3ee842aae7c_goCreateRows _cgo_gotypes.go:414\n at runtime.cgocallbackg1 cgocall.go:437\n at runtime.cgocallbackg cgocall.go:350\n at runtime.cgocallback asm_amd64.s:1084\n at runtime.goexit asm_amd64.s:1700",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/teradataml/dataframe/copy_to.py:583\u001b[0m, in \u001b[0;36mcopy_to_sql\u001b[0;34m(df, table_name, schema_name, if_exists, index, index_label, primary_index, temporary, types, primary_time_index_name, timecode_column, timebucket_duration, timezero_date, columns_list, sequence_column, seq_max, set_table, chunksize, match_column_order)\u001b[0m\n\u001b[1;32m    581\u001b[0m     from_schema_name \u001b[38;5;241m=\u001b[39m UtilFuncs\u001b[38;5;241m.\u001b[39m_extract_db_name(df\u001b[38;5;241m.\u001b[39m_table_name)\n\u001b[0;32m--> 583\u001b[0m     \u001b[43mdf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_insert_all_from_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_tbl_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_column_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mto_schema_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mfrom_schema_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_schema_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mtemporary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemporary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;66;03m# While table name conflict is present, Delete the source table after creation of temporary table.\u001b[39;00m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;66;03m# Rename the temporary table to destination table name. \u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/teradataml/dataframe/dataframe_utils.py:1668\u001b[0m, in \u001b[0;36mDataFrameUtils._insert_all_from_table\u001b[0;34m(to_table_name, from_table_name, column_list, to_schema_name, from_schema_name, temporary)\u001b[0m\n\u001b[1;32m   1667\u001b[0m \u001b[38;5;66;03m# Execute INSERT command.\u001b[39;00m\n\u001b[0;32m-> 1668\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mUtilFuncs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_ddl_statement\u001b[49m\u001b[43m(\u001b[49m\u001b[43minsert_sql\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/teradataml/common/utils.py:522\u001b[0m, in \u001b[0;36mUtilFuncs._execute_ddl_statement\u001b[0;34m(ddl_statement)\u001b[0m\n\u001b[1;32m    521\u001b[0m cursor \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mcursor()\n\u001b[0;32m--> 522\u001b[0m \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mddl_statement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;66;03m# Warnings are displayed when the \"suppress_vantage_runtime_warnings\" attribute is set to 'False'.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/teradatasql/__init__.py:752\u001b[0m, in \u001b[0;36mTeradataCursor.execute\u001b[0;34m(self, sOperation, params, ignoreErrors)\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m params:\n\u001b[0;32m--> 752\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecutemany\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msOperation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignoreErrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    754\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m (params [\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m]:\n\u001b[1;32m    755\u001b[0m     \u001b[38;5;66;03m# Excerpt from PEP 249 DBAPI documentation:\u001b[39;00m\n\u001b[1;32m    756\u001b[0m     \u001b[38;5;66;03m#  The parameters may also be specified as list of tuples to e.g. insert multiple rows in a single\u001b[39;00m\n\u001b[1;32m    757\u001b[0m     \u001b[38;5;66;03m#  operation, but this kind of usage is deprecated: .executemany() should be used instead.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/teradatasql/__init__.py:1007\u001b[0m, in \u001b[0;36mTeradataCursor.executemany\u001b[0;34m(self, sOperation, seqOfParams, ignoreErrors)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m-> 1007\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OperationalError (sErr)\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection\u001b[38;5;241m.\u001b[39mbTimingLog:\n",
      "\u001b[0;31mOperationalError\u001b[0m: [Version 20.0.0.24] [Session 2079] [Teradata Database] [Error 9124] Segmentation Violation in AMP: Please do not resubmit the last request.\n at gosqldriver/teradatasql.formatError ErrorUtil.go:92\n at gosqldriver/teradatasql.(*teradataConnection).formatDatabaseError ErrorUtil.go:252\n at gosqldriver/teradatasql.(*teradataConnection).makeChainedDatabaseError ErrorUtil.go:268\n at gosqldriver/teradatasql.(*teradataConnection).processErrorParcel TeradataConnection.go:751\n at gosqldriver/teradatasql.(*TeradataRows).processResponseBundle TeradataRows.go:2308\n at gosqldriver/teradatasql.(*TeradataRows).executeSQLRequest TeradataRows.go:874\n at gosqldriver/teradatasql.newTeradataRows TeradataRows.go:720\n at gosqldriver/teradatasql.(*teradataStatement).QueryContext TeradataStatement.go:122\n at gosqldriver/teradatasql.(*teradataConnection).QueryContext TeradataConnection.go:1261\n at database/sql.ctxDriverQuery ctxutil.go:48\n at database/sql.(*DB).queryDC.func1 sql.go:1778\n at database/sql.withLock sql.go:3566\n at database/sql.(*DB).queryDC sql.go:1773\n at database/sql.(*Conn).QueryContext sql.go:2029\n at main.createRows goside.go:1080\n at main.goCreateRows goside.go:959\n at _cgoexp_e3ee842aae7c_goCreateRows _cgo_gotypes.go:414\n at runtime.cgocallbackg1 cgocall.go:437\n at runtime.cgocallbackg cgocall.go:350\n at runtime.cgocallback asm_amd64.s:1084\n at runtime.goexit asm_amd64.s:1700",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTeradataMlException\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 36\u001b[0m\n\u001b[1;32m     27\u001b[0m ctx \u001b[38;5;241m=\u001b[39m ModelContext(hyperparams\u001b[38;5;241m=\u001b[39mhyperparams,\n\u001b[1;32m     28\u001b[0m                    dataset_info\u001b[38;5;241m=\u001b[39mdataset_info,\n\u001b[1;32m     29\u001b[0m                    artifact_output_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./artifacts\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m                    model_table\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_indb_xgboost_v1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     33\u001b[0m                    job_id\u001b[38;5;241m=\u001b[39mjob_id)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscoring\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[43mscoring\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/modelops-demo-models/model_definitions/telco_python_indb_xgboost/model_modules/scoring.py:84\u001b[0m, in \u001b[0;36mscore\u001b[0;34m(context, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished Scoring\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# print(predictions_pdf)\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# store the predictions\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m#     # )\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m#     # PRIMARY INDEX ( job_id );\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m     \u001b[43mcopy_to_sql\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredictions_pdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mschema_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictions_database\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtable_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictions_table\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_exists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved predictions in Teradata\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;66;03m# calculate stats\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/teradataml/telemetry_utils/queryband.py:50\u001b[0m, in \u001b[0;36mcollect_queryband.<locals>.outer_wrapper.<locals>.inner_wrapper\u001b[0;34m(*func_args, **func_kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39mfunc_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfunc_kwargs):\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# Pass the required argument 'session_queryband' along with other\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# expected arguments to collect_queryband() decorator which is\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# imported as tdsqlalchemy_collect_queryband.\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtdsqlalchemy_collect_queryband\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession_queryband\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mqb_deco_pos_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mqb_deco_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/teradatasqlalchemy/telemetry/queryband.py:382\u001b[0m, in \u001b[0;36mcollect_queryband.<locals>.qb_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;66;03m# Append queryband to buffer.\u001b[39;00m\n\u001b[1;32m    380\u001b[0m     session_queryband\u001b[38;5;241m.\u001b[39mappend_qb(qb_str)\n\u001b[0;32m--> 382\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mexposed_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/teradataml/dataframe/copy_to.py:599\u001b[0m, in \u001b[0;36mcopy_to_sql\u001b[0;34m(df, table_name, schema_name, if_exists, index, index_label, primary_index, temporary, types, primary_time_index_name, timecode_column, timebucket_duration, timezero_date, columns_list, sequence_column, seq_max, set_table, chunksize, match_column_order)\u001b[0m\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 599\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TeradataMlException(Messages\u001b[38;5;241m.\u001b[39mget_message(MessageCodes\u001b[38;5;241m.\u001b[39mCOPY_TO_SQL_FAIL) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(err),\n\u001b[1;32m    600\u001b[0m                               MessageCodes\u001b[38;5;241m.\u001b[39mCOPY_TO_SQL_FAIL) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "\u001b[0;31mTeradataMlException\u001b[0m: [Teradata][teradataml](TDML_2020) Failed to copy dataframe to Teradata Vantage.[Version 20.0.0.24] [Session 2079] [Teradata Database] [Error 9124] Segmentation Violation in AMP: Please do not resubmit the last request.\n at gosqldriver/teradatasql.formatError ErrorUtil.go:92\n at gosqldriver/teradatasql.(*teradataConnection).formatDatabaseError ErrorUtil.go:252\n at gosqldriver/teradatasql.(*teradataConnection).makeChainedDatabaseError ErrorUtil.go:268\n at gosqldriver/teradatasql.(*teradataConnection).processErrorParcel TeradataConnection.go:751\n at gosqldriver/teradatasql.(*TeradataRows).processResponseBundle TeradataRows.go:2308\n at gosqldriver/teradatasql.(*TeradataRows).executeSQLRequest TeradataRows.go:874\n at gosqldriver/teradatasql.newTeradataRows TeradataRows.go:720\n at gosqldriver/teradatasql.(*teradataStatement).QueryContext TeradataStatement.go:122\n at gosqldriver/teradatasql.(*teradataConnection).QueryContext TeradataConnection.go:1261\n at database/sql.ctxDriverQuery ctxutil.go:48\n at database/sql.(*DB).queryDC.func1 sql.go:1778\n at database/sql.withLock sql.go:3566\n at database/sql.(*DB).queryDC sql.go:1773\n at database/sql.(*Conn).QueryContext sql.go:2029\n at main.createRows goside.go:1080\n at main.goCreateRows goside.go:959\n at _cgoexp_e3ee842aae7c_goCreateRows _cgo_gotypes.go:414\n at runtime.cgocallbackg1 cgocall.go:437\n at runtime.cgocallbackg cgocall.go:350\n at runtime.cgocallback asm_amd64.s:1084\n at runtime.goexit asm_amd64.s:1700"
     ]
    }
   ],
   "source": [
    "# Define the ModelContext to test with. The ModelContext is created and managed automatically by ModelOps \n",
    "# when it executes your code via CLI / UI. However, for testing in the notebook, you can define as follows\n",
    "\n",
    "# define the scoring dataset \n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT \n",
    "   * from transform_data_test sample 0.6;\n",
    "\"\"\"\n",
    "\n",
    "# where to store predictions\n",
    "predictions = {\n",
    "    \"database\": \"demo_user\",\n",
    "    \"table\": \"Telco_Churn_Predictions_tmp\"\n",
    "}\n",
    "\n",
    "import uuid\n",
    "job_id=str(uuid.uuid4())\n",
    "\n",
    "dataset_info = DatasetInfo(sql=sql,\n",
    "                           entity_key=entity_key,\n",
    "                           feature_names=feature_names,\n",
    "                           target_names=target_names,\n",
    "                           feature_metadata=feature_metadata,\n",
    "                           predictions=predictions)\n",
    "\n",
    "ctx = ModelContext(hyperparams=hyperparams,\n",
    "                   dataset_info=dataset_info,\n",
    "                   artifact_output_path=\"./artifacts\",\n",
    "                   artifact_input_path=\"./artifacts\",\n",
    "                   model_version=\"indb_xgboost_v1\",\n",
    "                   model_table=\"model_indb_xgboost_v1\",\n",
    "                   job_id=job_id)\n",
    "\n",
    "import scoring\n",
    "scoring.score(context=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27b2370",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame.from_query(f\"SELECT * FROM Telco_Churn_Predictions_tmp WHERE job_id='{job_id}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06db76e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "\n",
    "os.system('rm -f artifacts/*')\n",
    "\n",
    "try:\n",
    "    get_context().execute(f\"DROP TABLE model_indb_xgboost_v1\")\n",
    "except: \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    get_context().execute(f\"DROP TABLE pima_patient_predictions_tmp\")\n",
    "except: \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5019e9-654f-4260-8a95-848539f709ba",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;background-color:#00233C;\">\n",
    "<p><b style = 'font-size:20px;font-family:Arial;color:#00233C'>4. Define Model Metadata</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Now let's create the configuration files.<br>Requirements file contains the various libraries/packages needed for execution of the code created above. We will specify the libraries with the dependencies and versions:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "157dafbf-a633-43f6-94d7-68c0e89668b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /home/jovyan/modelops-demo-models/model_definitions/telco_python_indb_xgboost/model_modules/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile $model_local_path/model_modules/requirements.txt\n",
    "pandas==2.1.4\n",
    "matplotlib==3.8.2\n",
    "PyYAML==5.4.1\n",
    "scikit-learn==1.1.3\n",
    "teradataml==20.0.0.3 \n",
    "teradatasqlalchemy==20.0.0.3 \n",
    "teradatamodelops==7.0.6\n",
    "seaborn==0.12.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f9f7d1",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The config file will contain the hyper parameter configuration (default values): which will used in the training of the model.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8711bb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /home/jovyan/modelops-demo-models/model_definitions/telco_python_indb_xgboost/config.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile $model_local_path/config.json\n",
    "{\n",
    "   \"hyperParameters\": {\n",
    "    \"model_type\": \"Classification\",\n",
    "    \"scale_method\":\"RANGE\",\n",
    "    \"miss_value\":\"KEEP\",\n",
    "    \"global_scale\": \"False\",\n",
    "    \"multiplier\":\"1\",\n",
    "    \"intercept\":\"0\",\n",
    "    \"max_depth\": 8,\n",
    "    \"num_boosted_trees\": 100,\n",
    "    \"tree_size\": 0.5,\n",
    "    \"lambda1\" : 1.5\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ed3d03",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The model file will contain the configuration details of the model created</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "224d5f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /home/jovyan/modelops-demo-models/model_definitions/telco_python_indb_xgboost/model.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile $model_local_path/model.json\n",
    "{\n",
    "    \"id\": \"5b41d4d4-7236-54ab-846a-01c3151e1fd9\",\n",
    "    \"name\": \"In-database Telco Churn Prediction XGBoost\",\n",
    "    \"description\": \"In-database XGBoost for Telco Customer Churn Prediction\",\n",
    "    \"language\": \"python\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336a5344-9860-4835-a156-19547b5f6930",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;background-color:#00233C;\">\n",
    "<p><b style = 'font-size:20px;font-family:Arial;color:#00233C'>5. Commit and Push to Git to let ModelOps manage</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The files which are created in the code above will be committed to the git repository</p>\n",
    "\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>training.py</li>\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>evaluation.py</li>\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>scoring.py</li>\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>requirements.txt</li>\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>config.json</li>\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>model.json</li>\n",
    "\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Run the command below to commit and push changes to our forked repository, so ModelOps can fetch the changes to the model.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36decf79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 6cd7f2e] Added PIMA XGboost in database demo model\n",
      " 1 file changed, 5 insertions(+), 1 deletion(-)\n",
      "Enumerating objects: 11, done.\n",
      "Counting objects: 100% (11/11), done.\n",
      "Delta compression using up to 4 threads\n",
      "Compressing objects: 100% (6/6), done.\n",
      "Writing objects: 100% (6/6), 722 bytes | 722.00 KiB/s, done.\n",
      "Total 6 (delta 5), reused 0 (delta 0)\n",
      "remote: Resolving deltas: 100% (5/5), completed with 5 local objects.\u001b[K\n",
      "To https://github.com/shilpa-nalkande/modelops-demo-models.git\n",
      "   04aca4c..6cd7f2e  master -> master\n"
     ]
    }
   ],
   "source": [
    "!cd $model_local_path/../.. && git add . && git commit -m \"Added PIMA XGboost in database demo model \" && git push"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c7d023",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Now that changes are pushed, you can make the lifecycle inside <strong>ModelOps User Interface</strong>, plan for new trainings, evaluations, scorings. Compare models and operationalize into Production with automated Monitoring and alerting capabilities.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c9013c-31bb-4324-a1b4-1cb6065b4294",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;background-color:#00233C;\">\n",
    "<p><b style = 'font-size:20px;font-family:Arial;color:#00233C'>6. ModelOps full lifecycle till deployment</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a34b63",
   "metadata": {},
   "source": [
    "<p style='font-size:16px;font-family:Arial'>Use or Create a Project with the git code repository with the model code, then you should see the model in the catalog already created</p>\n",
    "\n",
    "<img src=\"images/08_01.png\" alt=\"Model Catalog with inDB\"/>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>Select the Model and then click Train a new Model. Use default hyper-parameters. This will launch the training job with the training script we generated and pushed to Git.</p>\n",
    "\n",
    "<img src=\"images/08_02.png\" alt=\"Train\"/>\n",
    "\n",
    "<img src=\"images/08_03.png\" alt=\"Train job\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "<img src=\"images/08_04.png\" alt=\"Train finished\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>When Model is trained a new Model Id is created and you can get inside the Model Lifecycle screen to review artifacts and other details</p>\n",
    "\n",
    "<img src=\"images/08_06.png\" alt=\"Model lifecycle\"/>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>Now, let's evaluate the Model, click the button and select the evaluation dataset. This will launch the evaluation job with the training script we generated and pushed to Git.</p>\n",
    "\n",
    "<img src=\"images/08_07.png\" alt=\"Evaluation\" width=\"500\" height=\"500\"/> <img src=\"images/08_08.png\" alt=\"Evaluation job\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>When evaluation job is finished a Model evaluation Report is generated with the metrics and charts that evaluation script generates</p>\n",
    "\n",
    "<img src=\"images/08_26.png\" alt=\"Model Report\" />\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>Now, let's approve the model and provide an approval description</p>\n",
    "\n",
    "<img src=\"images/08_09.png\" alt=\"Approval\" />\n",
    "\n",
    "<img src=\"images/08_10.png\" alt=\"Approval description\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>The model is ready to be deployed. Let's deploy using a Batch scheduling option - Run it manual</p>\n",
    "\n",
    "<img src=\"images/08_11.png\" alt=\"Deployment Engine\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "<img src=\"images/08_12.png\" alt=\"Deployment Publish\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "<img src=\"images/08_13.png\" alt=\"Deployment Schedule\" width=\"500\" height=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe60441-f62e-46c1-8baa-ff3cb0f1b809",
   "metadata": {},
   "source": [
    "<p style='font-size:16px;font-family:Arial'>Go and try this Step by yourself. Launch ModelOps from this button below:</p>\n",
    "<a href=\"/modelops\"><img src=\"images/launchModelOps.png\" alt=\"Launch ModelOps\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b69248e-606c-4e46-99ea-dbed58ed3935",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;background-color:#00233C;\">\n",
    "<p><b style = 'font-size:20px;font-family:Arial;color:#00233C'>9. ModelOps Monitoring</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Now the model is deployed and a new Deployment appears in the deployment screen</p>\n",
    "\n",
    "\n",
    "<img src=\"images/08_15.png\" alt=\"Deploymet\" />\n",
    "\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>You can run jobs manually from here, review history of executions and view the predictions for a specific job</p>\n",
    "\n",
    "<img src=\"images/08_16.png\" alt=\"Deployment Run\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "<img src=\"images/08_17.png\" alt=\"Deployment Jobs\" />\n",
    "\n",
    "<img src=\"images/08_18.png\" alt=\"Deployment view\" width=\"500\" height=\"500\" />\n",
    "\n",
    "<img src=\"images/08_19.png\" alt=\"Deployment predictions\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "<img src=\"images/08_20.png\" alt=\"Deployment\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>From the Feature Drift and Prediction Drift tabs you can check on the monitoring of the data drift</p>\n",
    "\n",
    "<img src=\"images/08_22.png\" alt=\"Feature Drift\" />\n",
    "\n",
    "<img src=\"images/08_21.png\" alt=\"Prediction Drift\" />\n",
    "\n",
    "<img src=\"images/08_23.png\" alt=\"Performance Monitoring\" />\n",
    "\n",
    "\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>From the Performance Drift, you can review multiple evaluations, let's evaluate the model with a new dataset. We create a new evaluation dataset with this query:</p>\n",
    "    \n",
    "```sql\n",
    "SELECT * FROM pima_patient_diagnoses F WHERE F.patientid MOD 8 <> 0  \n",
    "```\n",
    "\n",
    "<img src=\"images/08_24.png\" alt=\"Evaluate\" width=\"500\" height=\"500\" />\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>and now see the evolution of the metrics</p>\n",
    "\n",
    "<img src=\"images/08_25.png\" alt=\"Metrics monitoring\" />\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>\n",
    "With ModelOps you can close the cycle and review make decisions when you need to replace yor model in production, For example, You could get alerting from Data Drift of Performance Drift and you can create multiple versions and compare them, select a champion and deploy new versions that replace existing in Production.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97beb6b4-ce0a-4787-bb8f-1653b9794dcd",
   "metadata": {},
   "source": [
    "<p style='font-size:16px;font-family:Arial'>Go and try this Step by yourself. Launch ModelOps from this button below:</p>\n",
    "<a href=\"/modelops\"><img src=\"images/launchModelOps.png\" alt=\"Launch ModelOps\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bd6e3e-d0b4-43ff-a4d3-b8a369c0a19c",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;background-color:#00233C;\">\n",
    "<p><b style = 'font-size:20px;font-family:Arial;color:#00233C'>10. Cleanup</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d1318f-fe9e-4c73-9428-9734ec1fbb8f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>If you are done with ModelOps usecase, please uncomment and run the below cleanup section.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71990bc-09a9-4e58-bd20-5969c83523c0",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#00233C'><b>Work Tables</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Cleanup work tables to prevent errors next time.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420dd53e-f099-4f1b-a3f9-1757b47a4dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# db_drop_table(table_name = 'aoa_byom_models', schema_name = 'demo_user')\n",
    "# db_drop_table(table_name = 'pima_patient_predictions', schema_name = 'demo_user')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b6f7b6-d739-4035-aef1-555f3bbf46c6",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#00233C'> <b>Databases and Tables </b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The following code will clean up tables and databases created above.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0377bf79-cd6c-41c7-8684-84e7eb432b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run -i ../UseCases/run_procedure.py \"call remove_data('DEMO_ModelOps');\"        # Takes 10 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f1e45aa-131d-45c8-b494-7014d32a988b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'remove_context' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mremove_context\u001b[49m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'remove_context' is not defined"
     ]
    }
   ],
   "source": [
    "remove_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc01570c-13b8-4ba3-b07a-2245959a5a51",
   "metadata": {},
   "source": [
    "[<< Back to Git PIMA R GBM](./10_ModelOps_GIT_PIMA_R_GBM.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b46c6d-eab2-4fa4-9e40-2d9064305c46",
   "metadata": {},
   "source": [
    "<footer style=\"padding:10px;background:#f9f9f9;border-bottom:3px solid #394851\">Copyright © Teradata Corporation - 2023. All Rights Reserved.</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
