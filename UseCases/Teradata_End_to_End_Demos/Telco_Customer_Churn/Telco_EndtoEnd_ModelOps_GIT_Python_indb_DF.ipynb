{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc5ef2d4-514a-4c8a-8741-4336263ec7ec",
   "metadata": {},
   "source": [
    "<header>\n",
    "   <p  style='font-size:36px;font-family:Arial; color:#F0F0F0; background-color: #00233c; padding-left: 20pt; padding-top: 20pt;padding-bottom: 10pt; padding-right: 20pt;'>\n",
    "       ModelOps : In-Database DecisionForest using Git for Telco Churn\n",
    "  <br>\n",
    "       <img id=\"teradata-logo\" src=\"https://storage.googleapis.com/clearscape_analytics_demo_data/DEMO_Logo/teradata.svg\" alt=\"Teradata\" style=\"width: 125px; height: auto; margin-top: 20pt;\">\n",
    "    </p>\n",
    "</header>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e628a88",
   "metadata": {
    "tags": []
   },
   "source": [
    "<p style ='font-size:18px;font-family:Arial'><b>Introduction</b></p>\n",
    "\n",
    "<p style ='font-size:16px;font-family:Arial'>This Notebook is a part of the Teradata End-to-End Telco Customer Churn usecase and should be executed only after the Traditional Approach notebook is executed.</p>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>In this Notebook will we go throught the process on how to work with ClearScape Analytics in-database functions with ModelOps. With in-database analytics you can solve your scalable challenges by using Vantage to train and score your models. Whether you have a big volume of data or you want to avoid the data movement implementation to train models outside Vantage, you can use ModelOps to manage your Catalog of Models from multiple platforms including in-database algorithms.<br>To know more about in-database algorithms review teradata official documentation.</p>\n",
    " \n",
    "<p style='font-size:16px;font-family:Arial'>This notebook will cover the Operationalization of the Telco Customer Churn use case with Python using the Teradata In-database DecisionForest model. <strong>DecisionForest</strong> is an ensemble algorithm used for classification and regression predictive modelling problems. It is an extension of bootstrap aggregation (bagging) of decision trees.\n",
    "The function supports regression, binary, and multiclass classification.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c675a412-ac1c-4dbb-b4e3-21e847c3295b",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p><b style = 'font-size:20px;font-family:Arial'>1. Configure the Environment</b></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5200df2-2a75-4c78-8c86-2e4382dbbc6c",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial'><b>1.1 Set up Git repository</b></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbdc8f5-c1d3-4c15-906b-a58629eea689",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<p style = 'font-size:16px;font-family:Arial'><b>Note:</b> <i>You must have a personal git repository to continue with this demo</i>. To create one, follow the steps and execute through sections 1, 2 and 3 in this notebook:</p>\n",
    "   <a href=\"../../../ModelOps/06_ModelOps_GIT_Project_Setup.ipynb\" style=\"font-size:16px; font-family:Arial; color:white; background-color:#017373; padding:10px 20px; border-radius:5px; text-decoration:none; display:inline-block;\">06_ModelOps_GIT_Project_Setup.ipynb</a><br><br>\n",
    "    <p style = 'font-size:16px;font-family:Arial'>When you have finished executing <b>section 3</b>, please return here and continue executing this notebook.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be62e431-e39a-4baa-b02d-1dc25a36f559",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial'><b>1.2 Libraries installation</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>We use the magic command <b>%%capture</b> and the <b>-q</b> parameter for a non-verbose log of the installation command. The parameter can be remove if you want to observe the steps of the pip installation.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b499f2f4-45a0-49ba-bdf8-7a786cf01e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install -q teradataml==20.0.0.4 teradatasqlalchemy==20.0.0.4 teradatamodelops==7.0.6 matplotlib==3.8.2 scikit-learn==1.1.3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c742c3e-b65d-49aa-9323-1afa0eb0583b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<p style = 'font-size:16px;font-family:Arial;'><b>Note: </b><i>Please execute the above pip install to get the latest version of the required library. Be sure to restart the kernel after executing those lines to bring the installed libraries into memory. The simplest way to restart the Kernel is by typing zero zero: <b> 0 0</b></i></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b392c630-2106-4619-b4a8-ceaba6cb7ce2",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>1.2 Libraries import</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Here, we import the required libraries, set environment variables and environment paths (if required).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fbde24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from teradataml import (\n",
    "    create_context, \n",
    "    remove_context,\n",
    "    get_context,\n",
    "    get_connection,\n",
    "    DataFrame,\n",
    "    TrainTestSplit,\n",
    "    copy_to_sql,\n",
    "    db_drop_table,\n",
    "    configure,\n",
    "    execute_sql\n",
    ")\n",
    "import os\n",
    "import getpass\n",
    "import logging\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245c1005-e835-430e-a48f-c3962b72f3c8",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p><b style = 'font-size:20px;font-family:Arial'>2. Connect to Vantage</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a1f46c-7798-4fc9-a710-750c56e8a299",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>You will be prompted to provide the password. Enter your password, press the Enter key, then use down arrow to go to next cell. Begin running steps with Shift + Enter keys.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702a57c1-6013-4f92-bbac-b4278f1fb86b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run -i ../../startup.ipynb\n",
    "eng = create_context(host = 'host.docker.internal', username='demo_user', password = password)\n",
    "print(eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1be9f1e-8920-4d00-85ba-6f0e168231cc",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>2.1 Set up Install locations and model local path</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Here, we will configure the install locations for VAL and BYOM. We will also create a local path(similar to the git path) for the code which will be used later to commit the code to the git repository.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce03f507-ff4c-4d5d-902f-8c362fe59fa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "execute_sql('''SET query_band='DEMO=EE_Telco_EndtoEnd_ModelOps_GIT_Python_indb_DF.ipynb;' UPDATE FOR SESSION; ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d202dd08-811d-469a-bb52-32c200eb5c79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# configure byom/val installation\n",
    "configure.val_install_location = \"VAL\"\n",
    "configure.byom_install_location = \"MLDB\"\n",
    "\n",
    "# set the path to the local project repository for this model demo\n",
    "model_local_path = '~/modelops-demo-models/model_definitions/telco_python_indb_decisionForest'\n",
    "print(model_local_path)\n",
    "try:\n",
    "    res = os.system(f'mkdir -p {model_local_path}/model_modules')\n",
    "except:\n",
    "    print(f\"Could not mkdir -p {model_local_path}/model_modules\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3365d1-addc-4045-9e69-810b30707539",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;background-color:#00233C;\">\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233C'><b>2.2 Getting Data for This Demo</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Since this is a continuation of the Traditional approach notebook, we will be using the data from the same table we created in the earlier notebook.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6549cc5c-7f23-40ec-bba0-3365cfe05516",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<p style = 'font-size:16px;font-family:Arial'><b>Note: </b>If the Traditional Approach notebook has not been executed, please do it now.  This will get the data required for the remaining steps in this workflow.</p>\n",
    "    <a href=\"Telco_Customer_Churn_Traditional_Approach.ipynb\" style=\"font-size:16px; font-family:Arial; color:white; background-color:#017373; padding:10px 20px; border-radius:5px; text-decoration:none; display:inline-block;\">Traditional Approach notebook</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f21228-8646-4de7-90e8-de3df6efed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data = DataFrame('Transformed_data')\n",
    "transformed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79bd119-d754-4572-be30-0d3c7084c24f",
   "metadata": {},
   "source": [
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>We use the TrainTestSplit function to divide the dataset into train and test dataset which will be copied to Vantge to be used in the ModelOps cycle.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34fc839-a6d0-4e05-8b71-afd8201923a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TrainTestSplit_out = TrainTestSplit(\n",
    "                                    data = DataFrame('Transformed_data'),\n",
    "                                    id_column = \"CustomerID\",\n",
    "                                    train_size = 0.75,\n",
    "                                    test_size = 0.25,\n",
    "                                    seed = 21\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eb4243-b345-487c-809d-1223f4a7e94b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split into 2 virtual dataframes\n",
    "df_train = TrainTestSplit_out.result[TrainTestSplit_out.result['TD_IsTrainRow'] == 1].drop(['TD_IsTrainRow'], axis = 1)\n",
    "df_test = TrainTestSplit_out.result[TrainTestSplit_out.result['TD_IsTrainRow'] == 0].drop(['TD_IsTrainRow'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5f31c2-7f0a-4225-81c8-a6027af4d1a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "copy_to_sql(df_train, table_name='transform_data_train', if_exists='replace')\n",
    "copy_to_sql(df_test, table_name='transform_data_test', if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abc3f95-2694-4f13-af04-3de523a04430",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>2.3 Creating predictions and model table</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We will create a predictions table where we get our model predictions and the model table where we will upload the model created.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f5228f-63f9-4280-bb2a-befe5bf305c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %run -i ../run_procedure.py \"call get_data('DEMO_ModelOps_local');\"        # Takes 30 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a3e8f5-2ee8-4ace-83b7-e56c083effab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ddl for Aoa_Byom_Models \n",
    "query = '''CREATE SET TABLE DEMO_USER.Aoa_Byom_Models \n",
    "     (\n",
    "      model_version VARCHAR(255) CHARACTER SET LATIN NOT CASESPECIFIC,\n",
    "      model_id VARCHAR(255) CHARACTER SET LATIN NOT CASESPECIFIC,\n",
    "      model_type VARCHAR(255) CHARACTER SET LATIN NOT CASESPECIFIC,\n",
    "      project_id VARCHAR(255) CHARACTER SET LATIN NOT CASESPECIFIC,\n",
    "      deployed_at TIMESTAMP(6) DEFAULT CURRENT_TIMESTAMP(6),\n",
    "      model BLOB(2097088000))\n",
    "UNIQUE PRIMARY INDEX ( model_version );\n",
    "'''\n",
    " \n",
    "try:\n",
    "    execute_sql(query)\n",
    "except:\n",
    "    execute_sql('DROP TABLE DEMO_USER.Aoa_Byom_Models;')\n",
    "    execute_sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d6c0b9-f6b9-416c-b788-11cd8a6a9723",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ddl for Telco_Churn_Predictions\n",
    "query = '''CREATE MULTISET TABLE Telco_Churn_Predictions \n",
    "     (\n",
    "      job_id VARCHAR(255) CHARACTER SET LATIN NOT CASESPECIFIC,\n",
    "      CustomerID VARCHAR(10) CHARACTER SET LATIN,\n",
    "      Churn BYTEINT,\n",
    "      json_report CLOB(1048544000) CHARACTER SET LATIN)\n",
    "PRIMARY INDEX ( CustomerID );\n",
    "'''\n",
    "try:\n",
    "    execute_sql(query)\n",
    "except:\n",
    "    db_drop_table('Telco_Churn_Predictions')\n",
    "    execute_sql(query) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b91779b-9531-447d-97c8-da97af90f482",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ddl for collecting feature stats\n",
    "query = '''CREATE SET TABLE demo_user.aoa_stats \n",
    "     (\n",
    "      column_name VARCHAR(100) CHARACTER SET LATIN NOT CASESPECIFIC,\n",
    "      column_type VARCHAR(100) CHARACTER SET LATIN NOT CASESPECIFIC,\n",
    "      stats VARCHAR(1024) CHARACTER SET LATIN NOT CASESPECIFIC,\n",
    "      update_ts VARCHAR(200) CHARACTER SET LATIN NOT CASESPECIFIC)\n",
    "UNIQUE PRIMARY INDEX ( column_name );;\n",
    "'''\n",
    "try:\n",
    "    execute_sql(query)\n",
    "except:\n",
    "    db_drop_table('aoa_stats')\n",
    "    execute_sql(query) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c1c700-ba57-4c31-9fd2-3434023e6640",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Next is an optional step – if you want to see the status of databases/tables created and space used.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4b98a7-ed3a-4873-89b9-538bfbdc51a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../../run_procedure.py \"call space_report();\"        # Takes 10 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6047debc-f5c6-4fd3-850b-8c191b69e8b7",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p><b style = 'font-size:20px;font-family:Arial'>3. Define Training, Evaluation and Scoring functions </b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>We will need to create below 3 .py files to be used in the ModelOps cycle.</p>\n",
    "\n",
    "<li style = 'font-size:16px;font-family:Arial'><code>training.py</code>: The code using In-Db functions to train the model.</li>\n",
    "\n",
    "<li style = 'font-size:16px;font-family:Arial'><code>evaluation.py</code>: The code using In-Db functions to evaluate the model.</li>\n",
    "\n",
    "<li style = 'font-size:16px;font-family:Arial'><code>scoring.py</code>: The code using In-Db functions for scoring new data.</li>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>The steps below show the way of creating these 3 files and test the code before commiting it to the repository. After testing the code we set up various configuration files and than push the .py and configuration files to the git to be used from the ModelOps UI.</p> \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfea1e5-d74c-4b41-88f3-b40ed80f0d9b",
   "metadata": {
    "tags": []
   },
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p><b style = 'font-size:20px;font-family:Arial'>3.1 Define Training Function</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>The training function takes the following shape </p>\n",
    "\n",
    "```python\n",
    "def train(context: ModelContext, **kwargs):\n",
    "    aoa_create_context()\n",
    "    \n",
    "    # your training code using teradataml indDB function\n",
    "    model = <InDB Function>(...)\n",
    "    \n",
    "    # save your model\n",
    "    model.result.to_sql(f\"model_${context.model_version}\", if_exists=\"replace\")  \n",
    "    \n",
    "    record_training_stats(...)\n",
    "```\n",
    "<p style = 'font-size:16px;font-family:Arial'>You can execute this from the CLI or directly within the notebook as shown. The below code will created the training.py file in the local model path.</p>\n",
    "\n",
    "<p style = 'font-size:14px;font-family:Arial'><b><i>**Note: In the below cells You may get a warning that Feature stats are not available for various features. You can ignore the warnings as the Feature statistics will get collected in the UI as mentioned in Dataset creation steps.</i></b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7fb724",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile $model_local_path/model_modules/training.py\n",
    "from teradataml import (\n",
    "    DataFrame,\n",
    "    DecisionForest,\n",
    "    ScaleFit,\n",
    "    ScaleTransform,\n",
    "    OrdinalEncodingFit,\n",
    "    ColumnTransformer,\n",
    "    Shap\n",
    ")\n",
    "\n",
    "from aoa import (\n",
    "    record_training_stats,\n",
    "    aoa_create_context,\n",
    "    ModelContext\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def compute_feature_importance(feat_df):\n",
    "    df = feat_df.to_pandas()\n",
    "    df = df.T.reset_index()\n",
    "    df=df.rename(columns={'index': 'Feature', 0: 'Importance'})\n",
    "    df['Feature'] = df['Feature'].str.replace('TD_', '')\n",
    "    df['Feature'] = df['Feature'].str.replace('_SHAP', '')\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_feature_explain(explain_df):\n",
    "    explain_df = explain_df.drop(['CustomerID','label','tree_num'],axis=1)\n",
    "    shap_mean = explain_df.agg(['min', 'max'])\n",
    "    df = shap_mean.to_pandas()\n",
    "    df = df.T.reset_index()\n",
    "    df=df.rename(columns={'index': 'Feature', 0: 'Importance'})\n",
    "    mean_positive = df[df['Importance'] > 0]\n",
    "    mean_negative = df[df['Importance'] < 0]\n",
    "    mean_positive['Feature'] = mean_positive.loc[:,'Feature'].str.replace('max_TD_', '')\n",
    "    mean_positive['Feature'] = mean_positive.loc[:,'Feature'].str.replace('_SHAP', '')\n",
    "    mean_negative['Feature'] = mean_negative.loc[:,'Feature'].str.replace('min_TD_', '')\n",
    "    mean_negative['Feature'] = mean_negative.loc[:,'Feature'].str.replace('_SHAP', '')\n",
    "    # mean_positive['Feature'] = mean_positive['Feature'].str.replace('max_TD_', '')\n",
    "    # mean_positive['Feature'] = mean_positive['Feature'].str.replace('_SHAP', '')\n",
    "    # mean_negative['Feature'] = mean_negative['Feature'].str.replace('min_TD_', '')\n",
    "    # mean_negative['Feature'] = mean_negative['Feature'].str.replace('_SHAP', '')\n",
    "    return mean_positive,mean_negative\n",
    "    \n",
    "\n",
    "def plot_feature_importance(df, img_filename):\n",
    "    df = df.sort_values(by=\"Importance\", ascending=False)\n",
    "    # Plot the bar graph\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(x=\"Importance\",y=\"Feature\",data=df, palette=\"viridis\")\n",
    "    plt.title(\"Feature Importance\")\n",
    "    plt.xlabel(\"SHAP Importance Value\")\n",
    "    plt.ylabel(\"Features\")\n",
    "    plt.tight_layout()\n",
    "    fig = plt.gcf()\n",
    "    fig.savefig(img_filename, dpi=500)\n",
    "    plt.clf()\n",
    "    \n",
    "def plot_feature_explain(mean_positive,mean_negative, img_filename):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    bar_width = 0.35\n",
    "\n",
    "    ax.barh(mean_positive[\"Feature\"], mean_positive[\"Importance\"],color='salmon', label='-1 (positive)') \n",
    "    ax.barh(mean_negative[\"Feature\"], mean_negative[\"Importance\"],color='cyan', label='1 (negative)')\n",
    "    ax.set_xlabel(\"mean(|SHAP value|)\")\n",
    "    ax.set_title(\"Mean shap for all samples\")\n",
    "    ax.legend(title=\"sign\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    # plt.show()\n",
    "    fig = plt.gcf()\n",
    "    fig.savefig(img_filename, dpi=500)\n",
    "    plt.clf()    \n",
    "    \n",
    "def train(context: ModelContext, **kwargs):\n",
    "    aoa_create_context()\n",
    "    \n",
    "    # Extracting feature names, target name, and entity key from the context\n",
    "    feature_names = context.dataset_info.feature_names\n",
    "    target_name = context.dataset_info.target_names[0]\n",
    "    entity_key = context.dataset_info.entity_key\n",
    "\n",
    "    # Load the training data from Teradata\n",
    "    train_df = DataFrame.from_query(context.dataset_info.sql)\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "\n",
    "    # Train the model using XGBoost\n",
    "    model = DecisionForest(data = train_df,\n",
    "                input_columns = [\"Tenure\", \"InternetService\", \"OnlineSecurity\", \"SeniorCitizen\",\n",
    "                                    \"PaymentMethod\", \"OnlineBackup\", \"Dependents\", \"Partner\", \"MultipleLines\", \n",
    "                                    \"StreamingMovies\", \"Gender\", \"PhoneService\", \"TotalCharges\", \"Contract\", \n",
    "                                    \"MonthlyCharges\", \"DeviceProtection\", \"PaperlessBilling\", \"StreamingTV\", \n",
    "                                    \"TechSupport\"],\n",
    "                response_column = 'Churn',\n",
    "                family = context.hyperparams[\"family\"],\n",
    "                min_impurity = context.hyperparams[\"min_impurity\"],\n",
    "                max_depth = context.hyperparams[\"max_depth\"],\n",
    "                min_node_size = context.hyperparams[\"min_node_size\"],\n",
    "                num_trees = context.hyperparams[\"num_trees\"],\n",
    "                seed = context.hyperparams[\"seed\"],\n",
    "                tree_type = context.hyperparams[\"tree_type\"])\n",
    "    \n",
    "    \n",
    "    # Save the trained model to SQL\n",
    "    model.result.to_sql(f\"model_${context.model_version}\", if_exists=\"replace\")  \n",
    "    print(\"Saved trained model\")\n",
    "    \n",
    "    #Shap explainer \n",
    "    Shap_out = Shap(data=train_df, \n",
    "                object=model.result, \n",
    "                id_column='CustomerID',\n",
    "                training_function=\"TD_DecisionForest\", \n",
    "                model_type=\"Classification\",\n",
    "                input_columns=feature_names, \n",
    "                detailed=True)\n",
    "    \n",
    "    feat_df = Shap_out.output_data\n",
    "    explain_df = Shap_out.result\n",
    "    # print(explain_df)\n",
    "\n",
    " \n",
    "    df = compute_feature_importance(feat_df)\n",
    "    plot_feature_importance(df, f\"{context.artifact_output_path}/feature_importance\")\n",
    "    pos_expl_df, neg_expl_df = compute_feature_explain(explain_df)\n",
    "    # print(pos_expl_df)\n",
    "    # print(neg_expl_df)\n",
    "    plot_feature_explain(pos_expl_df,neg_expl_df, f\"{context.artifact_output_path}/feature_explainability\")\n",
    "\n",
    "    categorical=[\"Partner\", \"Dependents\",\"Contract\", \"SeniorCitizen\",\"PaymentMethod\", \"TechSupport\", \"StreamingTV\", \"OnlineSecurity\", \n",
    "                                        \"PhoneService\", \"OnlineBackup\", \"MultipleLines\", \"DeviceProtection\", \"Gender\", \n",
    "                                        \"PaperlessBilling\", \"StreamingMovies\", \"InternetService\", \"Churn\"]\n",
    "\n",
    "    record_training_stats(\n",
    "        train_df,\n",
    "        features=feature_names,\n",
    "        targets=[target_name],\n",
    "        categorical=categorical\n",
    "        ,\n",
    "        # feature_importance=feature_importance,\n",
    "        context=context\n",
    "    )\n",
    "    \n",
    "    print(\"All done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09955e13-76c3-491e-a945-06fe06e1a961",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'><b> Test the train </b><code>train(context: ModelContext, **kwargs) </code> <b>function</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The below code is for testing the train function and will not be a part of the git</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c437521",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the ModelContext to test with. The ModelContext is created and managed automatically by ModelOps \n",
    "# when it executes your code via CLI / UI. However, for testing in the notebook, you can define as follows\n",
    "\n",
    "# define the training dataset \n",
    "sql = \"\"\"\n",
    "SELECT \n",
    "    * from demo_user.transform_data_train;\n",
    "\"\"\"\n",
    "\n",
    "feature_metadata =  {\n",
    "    \"database\": \"DEMO_USER\",\n",
    "    \"table\": \"aoa_stats\"\n",
    "}\n",
    "hyperparams = {\n",
    "    \"family\": \"Binomial\",\n",
    "    \"min_impurity\": 0.0,\n",
    "    \"max_depth\": 5,\n",
    "    \"min_node_size\": 1,\n",
    "    \"num_trees\": -1,\n",
    "    \"seed\": 42,\n",
    "    \"tree_type\": \"Classification\"\n",
    "}\n",
    "\n",
    "entity_key = \"CustomerID\"\n",
    "target_names = [\"Churn\"]\n",
    "feature_names = [\"Tenure\", \"InternetService\", \"OnlineSecurity\", \"SeniorCitizen\", \"PaymentMethod\", \"OnlineBackup\", \"Dependents\", \n",
    "\"Partner\", \"MultipleLines\", \"StreamingMovies\", \"Gender\", \"PhoneService\", \"TotalCharges\", \"Contract\", \n",
    "\"MonthlyCharges\", \"DeviceProtection\", \"PaperlessBilling\", \"StreamingTV\", \"TechSupport\"]\n",
    " \n",
    "from aoa import ModelContext, DatasetInfo\n",
    "\n",
    "dataset_info = DatasetInfo(sql=sql,\n",
    "                           entity_key=entity_key,\n",
    "                           feature_names=[\"Tenure\",  \"MonthlyCharges\",  \"TotalCharges\"],\n",
    "                           target_names=target_names,\n",
    "                           categorical=[\"Partner\", \"Dependents\",\"Contract\", \"SeniorCitizen\",\"PaymentMethod\", \"TechSupport\", \"StreamingTV\", \"OnlineSecurity\", \n",
    "                                        \"PhoneService\", \"OnlineBackup\", \"MultipleLines\", \"DeviceProtection\", \"Gender\", \n",
    "                                        \"PaperlessBilling\", \"StreamingMovies\", \"InternetService\"],\n",
    "                           feature_metadata=feature_metadata)\n",
    "\n",
    "ctx = ModelContext(hyperparams=hyperparams,\n",
    "                   dataset_info=dataset_info,\n",
    "                   artifact_output_path=\"./artifacts\",\n",
    "                   model_version=\"indb_df_v1\",\n",
    "                   model_table=\"model_indb_df_v1\")\n",
    "\n",
    "sys.path.append(os.path.expanduser(f\"{model_local_path}/model_modules\"))\n",
    "import training\n",
    "training.train(context=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cf5f8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check the generated files\n",
    "!ls -lh artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bf4a4f-7e46-4b2a-b5a2-98a5afb42cfb",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p><b style = 'font-size:18px;font-family:Arial'>3.2 Define Evaluation Function</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>The evaluation function takes the following shape</p>\n",
    "\n",
    "```python\n",
    "def evaluate(context: ModelContext, **kwargs):\n",
    "    aoa_create_context()\n",
    "\n",
    "    # read your model from Vantage\n",
    "    model = DataFrame(f\"model_${context.model_version}\")\n",
    "    \n",
    "    # your evaluation logic\n",
    "    \n",
    "    record_evaluation_stats(...)\n",
    "```\n",
    "<p style = 'font-size:16px;font-family:Arial'>You can execute this from the CLI or directly within the notebook as shown. The below code will created the evaluation.py file in the local model path.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d751b6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile $model_local_path/model_modules/evaluation.py\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from teradataml import(\n",
    "    DataFrame, \n",
    "    copy_to_sql, \n",
    "    get_context, \n",
    "    get_connection, \n",
    "    OrdinalEncodingFit, \n",
    "    ScaleFit,\n",
    "    ColumnTransformer,\n",
    "    TDDecisionForestPredict, \n",
    "    ConvertTo, \n",
    "    ClassificationEvaluator,\n",
    "    ROC,\n",
    "    Shap\n",
    ")\n",
    "from aoa import (\n",
    "    record_evaluation_stats,\n",
    "    save_plot,\n",
    "    aoa_create_context,\n",
    "    ModelContext\n",
    ")\n",
    "\n",
    "import joblib\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "# Define function to plot a confusion matrix from given data\n",
    "def plot_confusion_matrix(cf, img_filename):\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
    "    ax.matshow(cf, cmap=plt.cm.Blues, alpha=0.3)\n",
    "    for i in range(cf.shape[0]):\n",
    "        for j in range(cf.shape[1]):\n",
    "            ax.text(x=j, y=i,s=cf[i, j], va='center', ha='center', size='xx-large')\n",
    "    ax.set_xlabel('Predicted labels');\n",
    "    ax.set_ylabel('True labels'); \n",
    "    ax.set_title('Confusion Matrix');\n",
    "    fig = plt.gcf()\n",
    "    fig.savefig(img_filename, dpi=500)\n",
    "    plt.clf()\n",
    "\n",
    "    \n",
    "def plot_roc_curve(roc_out, img_filename):\n",
    "    from teradataml import Figure\n",
    "    figure = Figure(width=500, height=400, heading=\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "    auc = roc_out.result.get_values()[0][0]\n",
    "    plot = roc_out.output_data.plot(\n",
    "        x=roc_out.output_data.fpr,\n",
    "        y=[roc_out.output_data.tpr, roc_out.output_data.fpr],\n",
    "        xlabel='False Positive Rate',\n",
    "        ylabel='True Positive Rate',\n",
    "        color='carolina blue',\n",
    "        figure=figure,\n",
    "        legend=[f'DF AUC = {round(auc, 4)}', 'AUC Baseline'],\n",
    "        legend_style='lower right',\n",
    "        grid_linestyle='--',\n",
    "        grid_linewidth=0.5\n",
    "    )\n",
    "    plot.save(img_filename)\n",
    "    # plot.show()\n",
    "    # fig = plt.gcf()\n",
    "    # fig.savefig(img_filename, dpi=500)\n",
    "    # plt.clf()    \n",
    "\n",
    "def evaluate(context: ModelContext, **kwargs):\n",
    "\n",
    "    aoa_create_context()\n",
    "\n",
    "    # Load the trained model from SQL\n",
    "    model = DataFrame(f\"model_${context.model_version}\")\n",
    "\n",
    "    feature_names = context.dataset_info.feature_names\n",
    "    target_name = context.dataset_info.target_names[0]\n",
    "    entity_key = context.dataset_info.entity_key\n",
    "\n",
    "    # Load the test data from Teradata\n",
    "    test_df = DataFrame.from_query(context.dataset_info.sql)\n",
    "\n",
    "    # Make predictions using the XGBoostPredict function\n",
    "    print(\"Evaluating ...........\")\n",
    "    predictions = TDDecisionForestPredict(object = model,\n",
    "                                        newdata = test_df,\n",
    "                                        id_column = \"CustomerID\",\n",
    "                                        detailed = False,\n",
    "                                        output_prob = True,\n",
    "                                        output_responses = ['0','1'],\n",
    "                                        accumulate=\"Churn\")\n",
    "\n",
    "    # Convert the predicted data into the specified format\n",
    "    # print(predictions.result)\n",
    "    predicted_data = ConvertTo(\n",
    "        data = predictions.result,\n",
    "        target_columns = [target_name,'prediction'],\n",
    "        target_datatype = [\"INTEGER\"]\n",
    "    )\n",
    "\n",
    "    # Evaluate classification metrics using ClassificationEvaluator\n",
    "    ClassificationEvaluator_obj = ClassificationEvaluator(\n",
    "        data=predicted_data.result,\n",
    "        observation_column=target_name,\n",
    "        prediction_column='prediction',\n",
    "        num_labels=2\n",
    "    )\n",
    "\n",
    "     # Extract and store evaluation metrics\n",
    "        \n",
    "    metrics_pd = ClassificationEvaluator_obj.output_data.to_pandas()\n",
    "\n",
    "    evaluation = {\n",
    "        'Accuracy': '{:.2f}'.format(metrics_pd.MetricValue[0]),\n",
    "        'Micro-Precision': '{:.2f}'.format(metrics_pd.MetricValue[1]),\n",
    "        'Micro-Recall': '{:.2f}'.format(metrics_pd.MetricValue[2]),\n",
    "        'Micro-F1': '{:.2f}'.format(metrics_pd.MetricValue[3]),\n",
    "        'Macro-Precision': '{:.2f}'.format(metrics_pd.MetricValue[4]),\n",
    "        'Macro-Recall': '{:.2f}'.format(metrics_pd.MetricValue[5]),\n",
    "        'Macro-F1': '{:.2f}'.format(metrics_pd.MetricValue[6]),\n",
    "        'Weighted-Precision': '{:.2f}'.format(metrics_pd.MetricValue[7]),\n",
    "        'Weighted-Recall': '{:.2f}'.format(metrics_pd.MetricValue[8]),\n",
    "        'Weighted-F1': '{:.2f}'.format(metrics_pd.MetricValue[9]),\n",
    "    }\n",
    "\n",
    "     # Save evaluation metrics to a JSON file\n",
    "    with open(f\"{context.artifact_output_path}/metrics.json\", \"w+\") as f:\n",
    "        json.dump(evaluation, f)\n",
    "        \n",
    "    # Generate and save confusion matrix plot\n",
    "    cm_df = ClassificationEvaluator_obj.result\n",
    "    # print(cm_df)\n",
    "    cm_df = cm_df.select(['CLASS_1','CLASS_2'])\n",
    "    # print(cm_df.get_values())\n",
    "    cm_df_t = cm_df.to_pandas().T\n",
    "    # print(cm_df_t.values)\n",
    "    cm = confusion_matrix(predicted_data.result.to_pandas()['Churn'], predicted_data.result.to_pandas()['prediction'])\n",
    "    # print(cm)\n",
    "    plot_confusion_matrix(cm_df_t.values, f\"{context.artifact_output_path}/confusion_matrix\")\n",
    "\n",
    "    # Generate and save ROC curve plot\n",
    "    roc_out = ROC(\n",
    "        data=predictions.result,\n",
    "        probability_column='prob_1',\n",
    "        observation_column=target_name,\n",
    "        positive_class='1',\n",
    "        num_thresholds=1000\n",
    "    )\n",
    "    plot_roc_curve(roc_out, f\"{context.artifact_output_path}/roc_curve\")\n",
    "\n",
    "    predictions_table = \"Telco_Churn_Predictions\"\n",
    "    copy_to_sql(df=predicted_data.result, table_name=predictions_table, index=False, if_exists=\"replace\", temporary=True)\n",
    "\n",
    "    # calculate stats if training stats exist\n",
    "    if os.path.exists(f\"{context.artifact_input_path}/data_stats.json\"):\n",
    "        record_evaluation_stats(\n",
    "            features_df=test_df,\n",
    "            predicted_df=DataFrame.from_query(f\"SELECT * FROM {predictions_table}\"),\n",
    "            # feature_importance=feature_importance,\n",
    "            context=context\n",
    "        )\n",
    "\n",
    "    print(\"All done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78762f2b-dc78-4dc8-be86-c2fe64be9167",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'><b> Test the evaluation </b><code>evaluate(context: ModelContext, **kwargs) </code> <b>function</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The below code is for testing the evaluation function and will not be a part of the git</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ba7a9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the ModelContext to test with. The ModelContext is created and managed automatically by ModelOps \n",
    "# when it executes your code via CLI / UI. However, for testing in the notebook, you can define as follows\n",
    "\n",
    "# define the evaluation dataset \n",
    "sql = \"\"\"\n",
    "SELECT \n",
    "   * from transform_data_test;\n",
    "\"\"\"\n",
    "\n",
    "dataset_info = DatasetInfo(sql=sql,\n",
    "                           entity_key=entity_key,\n",
    "                           feature_names=[\"Tenure\",  \"MonthlyCharges\",  \"TotalCharges\"],\n",
    "                           target_names=target_names,\n",
    "                           categorical=[\"Partner\", \"Dependents\",\"Contract\", \"SeniorCitizen\",\"PaymentMethod\", \"TechSupport\", \"StreamingTV\", \"OnlineSecurity\", \n",
    "                                        \"PhoneService\", \"OnlineBackup\", \"MultipleLines\", \"DeviceProtection\", \"Gender\", \n",
    "                                        \"PaperlessBilling\", \"StreamingMovies\", \"InternetService\"],\n",
    "                           feature_metadata=feature_metadata)\n",
    "\n",
    "ctx = ModelContext(hyperparams=hyperparams,\n",
    "                   dataset_info=dataset_info,\n",
    "                   artifact_output_path=\"./artifacts\",\n",
    "                   artifact_input_path=\"./artifacts\",\n",
    "                   model_version=\"indb_df_v1\",\n",
    "                   model_table=\"model_indb_df_v1\")\n",
    "\n",
    "import evaluation\n",
    "evaluation.evaluate(context=ctx)\n",
    "\n",
    "# view evaluation results\n",
    "import json\n",
    "with open(f\"{ctx.artifact_output_path}/metrics.json\") as f:\n",
    "    print(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce4ba2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check the generated files\n",
    "!ls -lh artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae441da-9858-4a91-8f70-d51b92e0ee90",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;background-color:#00233C;\">\n",
    "<p><b style = 'font-size:18px;font-family:Arial'>3.3 Define Scoring Function</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>The scoring function takes the following shape</p>\n",
    "\n",
    "```python\n",
    "def score(context: ModelContext, **kwargs):\n",
    "    aoa_create_context()\n",
    "\n",
    "    # read your model\n",
    "    model = DataFrame(f\"model_${context.model_version}\")\n",
    "    \n",
    "    # your evaluation logic\n",
    "    \n",
    "    record_scoring_stats(...)\n",
    "```\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>You can execute this from the CLI or directly within the notebook as shown. The below code will created the scoring.py file in the local model path.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4d4c1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile $model_local_path/model_modules/scoring.py\n",
    "from teradataml import (\n",
    "    copy_to_sql,\n",
    "    DataFrame,\n",
    "    TDDecisionForestPredict,\n",
    "    OrdinalEncodingFit,\n",
    "    ScaleFit,\n",
    "    ColumnTransformer,\n",
    "    ConvertTo,\n",
    "    translate\n",
    ")\n",
    "from aoa import (\n",
    "    record_scoring_stats,\n",
    "    aoa_create_context,\n",
    "    ModelContext\n",
    ")\n",
    "import pandas as pd\n",
    "from teradatasqlalchemy import INTEGER\n",
    "\n",
    "\n",
    "def score(context: ModelContext, **kwargs):\n",
    "    \n",
    "    aoa_create_context()\n",
    "\n",
    "    # Load the trained model from SQL\n",
    "    model = DataFrame(f\"model_${context.model_version}\")\n",
    "\n",
    "    # Extract feature names, target name, and entity key from the context\n",
    "    feature_names = context.dataset_info.feature_names\n",
    "    target_name = context.dataset_info.target_names[0]\n",
    "    entity_key = context.dataset_info.entity_key\n",
    "\n",
    "    # Load the test dataset\n",
    "    test_df = DataFrame.from_query(context.dataset_info.sql)\n",
    "    features_tdf = DataFrame.from_query(context.dataset_info.sql)\n",
    "\n",
    "    print(\"Scoring...\")\n",
    "    # Make predictions using the XGBoostPredict function\n",
    "    predictions = TDDecisionForestPredict(object = model,\n",
    "                                        newdata = test_df,\n",
    "                                        id_column = \"CustomerID\",\n",
    "                                        detailed = False,\n",
    "                                        output_prob = True,\n",
    "                                        output_responses = ['0','1'])\n",
    "    \n",
    "    # Convert predictions to pandas DataFrame and process\n",
    "    # predictions_pdf = predictions.result.to_pandas(all_rows=True).rename(columns={\"Prediction\": target_name}).astype(int)\n",
    "    predictions_df = predictions.result\n",
    "    # print(predictions_df)\n",
    "    predictions_pdf = predictions_df.assign(drop_columns=True,\n",
    "                                             job_id=translate(context.job_id),\n",
    "                                             CustomerID=predictions_df.CustomerID,\n",
    "                                             Churn=predictions_df.prediction.cast(type_=INTEGER),\n",
    "                                             json_report=translate(\"  \"))\n",
    "                                             \n",
    "    \n",
    "    \n",
    "    # print(predictions_pdf)\n",
    "    print(\"Finished Scoring\")\n",
    "    # print(predictions_pdf)\n",
    "\n",
    "    # store the predictions\n",
    "\n",
    "    copy_to_sql(\n",
    "        df=predictions_pdf,\n",
    "        schema_name=context.dataset_info.predictions_database,\n",
    "        table_name=context.dataset_info.predictions_table,\n",
    "        index=False,\n",
    "        if_exists=\"append\"\n",
    "    )\n",
    "    \n",
    "    print(\"Saved predictions in Teradata\")\n",
    "\n",
    "    # calculate stats\n",
    "    predictions_df = DataFrame.from_query(f\"\"\"\n",
    "        SELECT \n",
    "            * \n",
    "        FROM {context.dataset_info.get_predictions_metadata_fqtn()} \n",
    "            WHERE job_id = '{context.job_id}'\n",
    "    \"\"\")\n",
    "\n",
    "    record_scoring_stats(features_df=features_tdf, predicted_df=predictions_pdf, context=context)\n",
    "\n",
    "    print(\"All done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de37750-bf1c-4d3d-9ee4-510e64fec827",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'><b> Test the scoring </b><code>score(context: ModelContext, **kwargs) </code> <b>function</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The below code is for testing the score function and will not be a part of the git</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edf735d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the ModelContext to test with. The ModelContext is created and managed automatically by ModelOps \n",
    "# when it executes your code via CLI / UI. However, for testing in the notebook, you can define as follows\n",
    "\n",
    "# define the scoring dataset \n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT \n",
    "   * from transform_data_test;\n",
    "\"\"\"\n",
    "\n",
    "# where to store predictions\n",
    "predictions = {\n",
    "    \"database\": \"demo_user\",\n",
    "    \"table\": \"Telco_Churn_Predictions_tmp\"\n",
    "}\n",
    "\n",
    "import uuid\n",
    "job_id=str(uuid.uuid4())\n",
    "\n",
    "dataset_info = DatasetInfo(sql=sql,\n",
    "                           entity_key=entity_key,\n",
    "                           feature_names=[\"Tenure\",  \"MonthlyCharges\",  \"TotalCharges\"],\n",
    "                           target_names=target_names,\n",
    "                           categorical=[\"Partner\", \"Dependents\",\"Contract\", \"SeniorCitizen\",\"PaymentMethod\", \"TechSupport\", \"StreamingTV\", \"OnlineSecurity\", \n",
    "                                        \"PhoneService\", \"OnlineBackup\", \"MultipleLines\", \"DeviceProtection\", \"Gender\", \n",
    "                                        \"PaperlessBilling\", \"StreamingMovies\", \"InternetService\"],\n",
    "                           feature_metadata=feature_metadata,\n",
    "                           predictions=predictions)\n",
    "\n",
    "ctx = ModelContext(hyperparams=hyperparams,\n",
    "                   dataset_info=dataset_info,\n",
    "                   artifact_output_path=\"./artifacts\",\n",
    "                   artifact_input_path=\"./artifacts\",\n",
    "                   model_version=\"indb_df_v1\",\n",
    "                   model_table=\"model_indb_df_v1\",\n",
    "                   job_id=job_id)\n",
    "\n",
    "import scoring\n",
    "scoring.score(context=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27b2370",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DataFrame.from_query(f\"SELECT * FROM Telco_Churn_Predictions_tmp WHERE job_id='{job_id}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06db76e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "\n",
    "os.system('rm -f artifacts/*')\n",
    "\n",
    "try:\n",
    "    get_context().execute(f\"DROP TABLE model_indb_df_v1\")\n",
    "except: \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    get_context().execute(f\"DROP TABLE Telco_Churn_predictions_tmp\")\n",
    "except: \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5019e9-654f-4260-8a95-848539f709ba",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p><b style = 'font-size:20px;font-family:Arial'>4. Define Model Metadata</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Now let's create the configuration files.<br>Requirements file contains the various libraries/packages needed for execution of the code created above. We will specify the libraries with the dependencies and versions:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157dafbf-a633-43f6-94d7-68c0e89668b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile $model_local_path/model_modules/requirements.txt\n",
    "pandas==2.1.4\n",
    "matplotlib==3.8.2\n",
    "PyYAML==5.4.1\n",
    "scikit-learn==1.1.3\n",
    "teradataml==20.0.0.4 \n",
    "teradatasqlalchemy==20.0.0.4 \n",
    "teradatamodelops==7.0.6\n",
    "seaborn==0.12.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f9f7d1",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The config file will contain the hyper parameter configuration (default values): which will used in the training of the model.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8711bb8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile $model_local_path/config.json\n",
    "{\n",
    "   \"hyperParameters\": {\n",
    "    \"family\": \"Binomial\",\n",
    "    \"min_impurity\": 0.0,\n",
    "    \"max_depth\": 5,\n",
    "    \"min_node_size\": 1,\n",
    "    \"num_trees\": -1,\n",
    "    \"seed\": 42,\n",
    "    \"tree_type\": \"Classification\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ed3d03",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The model file will contain the configuration details of the model created</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224d5f9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile $model_local_path/model.json\n",
    "{\n",
    "    \"id\": \"8610f466-a406-479c-9950-be7c785ef3c9\",\n",
    "    \"name\": \"In-database Telco Churn Prediction DecisionForest\",\n",
    "    \"description\": \"In-database DecisionForest for Telco Customer Churn Prediction\",\n",
    "    \"language\": \"python\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336a5344-9860-4835-a156-19547b5f6930",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p><b style = 'font-size:20px;font-family:Arial'>5. Commit and Push to Git to let ModelOps manage</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>The files which are created in the code above will be committed to the git repository</p>\n",
    "\n",
    "<li style = 'font-size:16px;font-family:Arial'>training.py</li>\n",
    "<li style = 'font-size:16px;font-family:Arial'>evaluation.py</li>\n",
    "<li style = 'font-size:16px;font-family:Arial'>scoring.py</li>\n",
    "<li style = 'font-size:16px;font-family:Arial'>requirements.txt</li>\n",
    "<li style = 'font-size:16px;font-family:Arial'>config.json</li>\n",
    "<li style = 'font-size:16px;font-family:Arial'>model.json</li>\n",
    "\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Run the command below to commit and push changes to our forked repository, so ModelOps can fetch the changes to the model.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36decf79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cd $model_local_path/../.. && git add . && git commit -m \"Added Telco DecisionForest in database demo model \" && git push"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c7d023",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Now that changes are pushed, you can make the lifecycle inside <strong>ModelOps User Interface</strong>, plan for new trainings, evaluations, scorings. Compare models and operationalize into Production with automated Monitoring and alerting capabilities.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c9013c-31bb-4324-a1b4-1cb6065b4294",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p><b style = 'font-size:20px;font-family:Arial'>6. ModelOps full lifecycle till deployment</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fca11a0-5319-4454-af5f-b44e19e05821",
   "metadata": {},
   "source": [
    "<p style='font-size:16px;font-family:Arial'>Create a Project with the your git repository having the code created in the steps above.</p>\n",
    "\n",
    "<img src=\"images/ModelOps/ete_telco_01.png\" alt=\"Create Project\" />\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>Add a personal connection using the host, user and password for the Clearscape environment</p>\n",
    "\n",
    "<img src=\"images/ModelOps/ete_telco_02.png\" alt=\"Model Conn\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>Save the Project and we can see the model created above available in the catalog of models that can be used for future steps.</p>\n",
    "\n",
    "<img src=\"images/ModelOps/ete_telco_03.png\" alt=\"Model Catalog with inDB\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>Next step is to create the Dataset Template. Along with the Name , Decription etc. we have to specify the table that will contain statistics for the features. This table can be created and statistics generated using the options provided. Here we have already created the table in the steps above. Please make sure to specify the database as <b>demo_user</b> and select the table <b>aoa_stats</b> from the list of tables. </p>\n",
    "<img src=\"images/ModelOps/ete_telco_04.png\" alt=\"Create Dataset\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>The query that can be used to create the dataset template is:</p>\n",
    "<p style='font-size:16px;font-family:Arial'><code>Select * from demo_user.Transformed_data</code></p>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>Select the features which are to be used in model tarining. We have to specify the table that will contain statistics for the features. You will have to <code>deselect the target variable(\"Churn\" here)</code> as it is not a part of the features. There is an option of Validating the statistics and also Generate/Regenerate the statistics for all  features.</p>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'><b>**Note:</b> Please make sure that before generating the statics we have to specify the type for each feature column. Here the features <code>\"Tenure\", \"MonthlyCharges\" and \"TotalCharges\"</code> are to be specified as <b>\"Continuous\"</b> while the rest are to be specified as <b>\"Categorical\"</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2ab5b8-8753-432a-afa8-1080d9386cbb",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"display: flex; justify-content: center; gap: 20px; align-items: center; font-family: Arial, sans-serif;\">\n",
    "  <div style=\"text-align: center;\">\n",
    "    <h3>Select Features</h3>\n",
    "    <img src=\"images/ModelOps/ete_telco_05.png\" alt=\"Select Features\" width=\"500\" height=\"500\" style = \"border: 1px solid #00233C; border-radius: 10px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);\"/>\n",
    "  </div>\n",
    "  <div style=\"text-align: center;\">\n",
    "    <h3>Generate and Validate Statistics</h3>\n",
    "    <img src=\"images/ModelOps/ete_telco_06.png\" alt=\"Generate Stats\" width=\"500\" height=\"500\" style = \"border: 1px solid #00233C; border-radius: 10px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);\"/>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dcb1ca-e981-4fbb-9fc5-94a0a6d85f52",
   "metadata": {},
   "source": [
    "<p style='font-size:16px;font-family:Arial'>The query that can be used for Entity and Target is:</p>\n",
    "<p style='font-size:16px;font-family:Arial'><code>Select CustomerID, Churn from demo_user.Transformed_data</code></p>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>Similar to features statistics can be collected for Target variable. In the Predictions section please select the database and table to be used to save the predictions.</p>\n",
    "\n",
    "<img src=\"images/ModelOps/ete_telco_31.png\" alt=\"Model Catalog with inDB\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>Click Create and the Dataset template will get created.</p>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>Similarly, we create the Train and Test Dataset. The queries that can be used for creating the Train and Test Dataset are as below:</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea02200d-6376-4bd7-b1b0-ebc1feb56387",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: center; gap: 20px; align-items: center; font-family: Arial, sans-serif;\">\n",
    "  <div style=\"text-align: center;\">\n",
    "    <h5 style=\"color: #00233C;\"><code>Select * from demo_user.transform_data_train</code></h5>\n",
    "    <img src=\"images/ModelOps/ete_telco_09.png\" alt=\"Train Dataset\" width=\"500\" height=\"500\" style = \"border: 1px solid #00233C; border-radius: 10px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);\"/>\n",
    "  </div>\n",
    "  <div style=\"text-align: center;\">\n",
    "    <h5 style=\"color: #00233C;\"><code>Select * from demo_user.transform_data_test</code></h5>\n",
    "    <img src=\"images/ModelOps/ete_telco_10.png\" alt=\"Test Dataset\" width=\"500\" height=\"500\" style = \"border: 1px solid #00233C; border-radius: 10px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);\"/>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356d5ecc-e0c7-41af-a929-49c99200d566",
   "metadata": {},
   "source": [
    "<p style='font-size:16px;font-family:Arial'>The Train and Test Dataset get created.</p>\n",
    "<img src=\"images/ModelOps/ete_telco_12.png\" alt=\"Train Test\"/>\n",
    "\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>Go to the  Models to Select the Model and then click Train a new Model. Use default hyper-parameters. This will launch the training job with the training script we generated and pushed to Git.</p>\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; gap: 20px; align-items: center; font-family: Arial, sans-serif;\">\n",
    "  <div style=\"text-align: center;\">\n",
    "    <h4 style=\"color: #00233C;\"><code>Train Parameters</code></h4>\n",
    "    <img src=\"images/ModelOps/ete_telco_13.png\" alt=\"Train Model\" width=\"500\" height=\"500\" style = \"border: 1px solid #00233C; border-radius: 10px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);\"/>\n",
    "  </div>\n",
    "  <div style=\"text-align: center;\">\n",
    "    <h4 style=\"color: #00233C;\"><code>Train logs</code></h4>\n",
    "    <img src=\"images/ModelOps/ete_telco_14.png\" alt=\"Train Logs\" width=\"500\" height=\"500\" style = \"border: 1px solid #00233C; border-radius: 10px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);\"/>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "</p>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>When Model is trained a new Model Id is created and you can get inside the Model Lifecycle screen to review artifacts and other details. Now, let's evaluate the Model, click the button and select the evaluation dataset. This will launch the evaluation job with the training script we generated and pushed to Git.</p>\n",
    "\n",
    "<img src=\"images/ModelOps/ete_telco_15.png\" alt=\"Model lifecycle\"/>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>We can check the evaluation logs for the status of the evaluation job.</p>\n",
    "\n",
    "<img src=\"images/ModelOps/ete_telco_16.png\" alt=\"Evaluation\" width=\"400\" height=\"500\"/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src=\"images/ModelOps/ete_telco_17.png\" alt=\"Evaluation job\" width=\"400\" height=\"500\"/>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>When evaluation job is finished a Model evaluation Report is generated with the metrics and charts that evaluation script generates</p>\n",
    "\n",
    "<img src=\"images/ModelOps/ete_telco_26.png\" alt=\"Model Report\" />\n",
    "\n",
    "<img src=\"images/ModelOps/ete_telco_27.png\" alt=\"Evaluation\" width=\"450\" height=\"500\"/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src=\"images/ModelOps/ete_telco_29.png\" alt=\"Evaluation job\" width=\"450\" height=\"500\"/>\n",
    "\n",
    "<img src=\"images/ModelOps/ete_telco_28.png\" alt=\"Model Report\" />\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>Now, let's approve the model and provide an approval description</p>\n",
    "\n",
    "<img src=\"images/ModelOps/ete_telco_18.png\" alt=\"Approval\" />\n",
    "\n",
    "<img src=\"images/ModelOps/ete_telco_19.png\" alt=\"Deploy\" width=\"800\" height=\"500\"/>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>The model is ready to be deployed. Let's deploy using a Batch scheduling option - Run it manual</p>\n",
    "\n",
    "<img src=\"images/ModelOps/ete_telco_20.png\" alt=\"Deployment Engine\" width=\"900\" height=\"500\"/>\n",
    "\n",
    "<img src=\"images/ModelOps/ete_telco_21.png\" alt=\"Deployment Publish\" width=\"450\" height=\"500\"/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src=\"images/ModelOps/ete_telco_22.png\" alt=\"Deployment Schedule\" width=\"450\" height=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe60441-f62e-46c1-8baa-ff3cb0f1b809",
   "metadata": {},
   "source": [
    "<p style='font-size:16px;font-family:Arial'>Go and try this Step by yourself. Launch ModelOps from this button below:</p>\n",
    "<a href=\"/modelops\" style=\"display: inline-flex; align-items: center; justify-content: center; background-color: #007373; color: #FFFFFF; font-family: Arial, sans-serif; font-size: 16px; font-weight: bold; text-decoration: none; padding: 12px 24px; border: none; border-radius: 8px; box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1); cursor: pointer; transition: all 0.3s ease;\">\n",
    "  LAUNCH MODELOPS\n",
    "  <img src=\"https://img.icons8.com/ios-filled/50/ffffff/external-link.png\" alt=\"External Link Icon\" style=\"margin-left: 8px; width: 20px; height: 20px;\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bd6e3e-d0b4-43ff-a4d3-b8a369c0a19c",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p><b style = 'font-size:20px;font-family:Arial'>9. Cleanup</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d1318f-fe9e-4c73-9428-9734ec1fbb8f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<p style = 'font-size:16px;font-family:Arial'>If you are done with ModelOps usecase, please uncomment and run the below cleanup section.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71990bc-09a9-4e58-bd20-5969c83523c0",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial'><b>Work Tables</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Cleanup work tables to prevent errors next time.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420dd53e-f099-4f1b-a3f9-1757b47a4dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# db_drop_table(table_name = 'aoa_byom_models', schema_name = 'demo_user')\n",
    "# db_drop_table(table_name = 'Telco_Churn_predictions', schema_name = 'demo_user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1e45aa-131d-45c8-b494-7014d32a988b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "remove_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc01570c-13b8-4ba3-b07a-2245959a5a51",
   "metadata": {},
   "source": [
    "[<< Back to Traditional Approach Notebook](./Telco_Customer_Churn_Traditional_Approach.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e0f4cb-355e-433f-8f02-2b032ed97337",
   "metadata": {},
   "source": [
    "<footer style=\"padding-bottom:35px; border-bottom:3px solid #91A0Ab\">\n",
    "    <div style=\"float:left;margin-top:14px\">ClearScape Analytics™</div>\n",
    "    <div style=\"float:right;\">\n",
    "        <div style=\"float:left; margin-top:14px\">\n",
    "            Copyright © Teradata Corporation - 2025. All Rights Reserved\n",
    "        </div>\n",
    "    </div>\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
