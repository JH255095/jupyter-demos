{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26632eab-8349-4ce6-82f2-3e34a35887c2",
   "metadata": {},
   "source": [
    "<header>\n",
    "   <p  style='font-size:36px;font-family:Arial; color:#F0F0F0; background-color: #00233c; padding-left: 20pt; padding-top: 20pt;padding-bottom: 10pt; padding-right: 20pt;'>\n",
    "       IVSM Banking Customer Churn Model Install\n",
    "  <br>\n",
    "       <img id=\"teradata-logo\" src=\"https://storage.googleapis.com/clearscape_analytics_demo_data/DEMO_Logo/teradata.svg\" alt=\"Teradata\" style=\"width: 125px; height: auto; margin-top: 20pt;\">\n",
    "    </p>\n",
    "</header>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77679c3b-4db5-427e-b78d-640d05275800",
   "metadata": {},
   "source": [
    "<p style = 'font-size:20px;font-family:Arial'><b>Introduction</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>\n",
    "Hugging Face is a French-American company based in New York City that develops computation tools for building applications using machine learning. They are known for their <b>Transformers Library</b> which provides open-source implementations of transformer models for text, image, video, audio tasks including time-series. These models include well-known architectures like BERT and GPT. The library is compatible with PyTorch, TensorFlow, and JAX deep learning libraries. <br>\n",
    "    Deep Learning Models in HuggingFace are pre-trained by users/open source outfits/companies on various types of data – NLP, Audio, Images, Videos etc. Most popular tool of choice by users is PyTorch (open source python library) which helps create a Deep Learning model from scratch or take an existing model, retrain/fine-tune (Transfer Learning) on new set of data to be published in HF. Models can be inference with CPUs and GPUs with slight performance improvement for smaller models.<br>\n",
    "</p>\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>Why Vantage?</b></p>  \n",
    "<p style = 'font-size:16px;font-family:Arial'>As many Hugging Face models are available in <b>ONNX Runtime</b>, we can load them using the <b>BYOM</b> feature of Vantage and run them in Vantage. Because of <b>Graph Optimizations</b> on ONNX Runtime, there are proven benchmarks that show that inference with <b>ONNX Runtime will be 20% faster than a native PyTorch model on a CPU</b>. </p>\n",
    "    \n",
    "<p style = 'font-size:16px;font-family:Arial'><b>Vantage Parallelism</b> on top of boosted ONNX Runtime inference can turn a Vantage system as effective as inference on GPUs. If we have a <b>Vantage box with 72 AMPs</b>, assuming the table is perfectly distributed, it will <b>closely match the performance of a dedicated GPU and data never moves across the network saving time and I/O operations</b>. As parallelism increases with number of AMPs, the model inference will complete faster in Teradata Vantage with the same amount of text data vs a GPU. We can of course quantize the model (change float8 weights to int8/int4) for inference on CPU to go even faster with some tradeoff with accuracy. However, If Model size goes up GPU advantage will widen – example LLM like LLama3 and costs will be disproportionate with GPU but for smaller models we can get comparable performance. \n",
    "</p>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>Overall flow:</b></p>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c95843-85c5-402e-911e-d1f02698ee10",
   "metadata": {},
   "source": [
    "<center><img src=\"./images/pat1.png\" alt=\"Design pattern 1\" width=1200 height=900/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee0dba0-93e7-4b5a-be19-820c0c46e4ce",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none'>\n",
    "<b style = 'font-size:20px;font-family:Arial'>1. Configuring the environment</b>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>1.1 Install the required libraries</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a55daf-1401-4253-93b2-36ba13756146",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install optimum sentence_transformers==4.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaecd47b-ca12-44ea-bfc2-021a6cd48ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install --upgrade torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9061373e-60b6-488e-989f-ba421dcb025a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'><b>Note: </b><i>Please restart the kernel after executing these two lines. The simplest way to restart the Kernel is by typing zero zero: <b> 0 0</b>\n",
    "</i>\n",
    "    <br>You can remove or comment the <b>%%capture</b> is you want to observe what <i>!pip install</i> is doing. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb451e8f-55c0-49dd-90b3-35453d4a9e56",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none\">\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>1.2 Import the required libraries</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Here, we import the required libraries, set environment variables and environment paths (if required).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704ddfb5-2c6b-4df7-93f7-1135461dc442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Standard libraries\n",
    "import os\n",
    "import getpass\n",
    "import json\n",
    "\n",
    "# PDF loader and text processing\n",
    "# from langchain_community.document_loaders import PyPDFLoader\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from langchain.callbacks.manager import CallbackManager\n",
    "\n",
    "# # Language model\n",
    "# from langchain.llms import Ollama\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "import teradataml as tdml\n",
    "\n",
    "# ONNX runtime and tools\n",
    "import onnx\n",
    "import onnxruntime as rt\n",
    "from onnxruntime.tools.onnx_model_utils import *\n",
    "\n",
    "# Transformers and sentence similarity\n",
    "import transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "\n",
    "from teradataml import (\n",
    "    create_context,\n",
    "    delete_byom,\n",
    "    display,\n",
    "    execute_sql,\n",
    "    save_byom,\n",
    "    remove_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de29e63-cb44-4864-8a15-653c0838a64d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tdml.configure.val_install_location = \"val\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872e2efc-fe5a-4f09-88f1-b3c530600c8e",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none\">\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>2. Connect to Vantage</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>We will be prompted to provide the password. We will enter the password, press the Enter key, and then use the down arrow to go to the next cell. Begin running steps with Shift + Enter keys.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccd0aa4-27a3-4a1a-b3e4-7d2870345106",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run -i ../startup.ipynb\n",
    "eng = create_context(host = 'host.docker.internal', username='demo_user', password = password)\n",
    "print(eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bbd587-8db8-4a22-8d6d-dfe944a9cf65",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none\">\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>3. Creation of functions</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Below command will create the database and functions required for text summarization and embedding models using Huggingface PyTorch models in Vantage.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ef4906-78ef-4cca-9fe8-8466be36960e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"commands.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "for item in data[\"queries\"]:\n",
    "    try:\n",
    "        execute_sql(item[\"query\"])\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"The initialization steps have already been executed for this environment!\"\n",
    "        )\n",
    "        #print(f\"Error: {e}\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6af1548-d03d-4f33-ae1b-14b1c992ce09",
   "metadata": {},
   "source": [
    "<b style = 'font-size:18px;font-family:Arial'>3.1 Drop Tables (if exist)</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Attempts to drop <code>embeddings_models</code> and <code>embeddings_tokenizers</code> tables, ignoring errors if they don't exist.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcae44f-50f9-4fa3-aec8-973776454999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop embeddings-related tables if they exist\n",
    "SQL = [\n",
    "    \"DROP TABLE embeddings_models;\",\n",
    "    \"DROP TABLE embeddings_tokenizers;\"\n",
    "]\n",
    "\n",
    "for query in SQL:\n",
    "    try:\n",
    "        tdml.execute_sql(query)\n",
    "    except:\n",
    "        pass  # Suppress any errors if the tables do not exist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bd1fe9-81f0-4acb-9d75-a98f496b2f4a",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none\">\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>4. HuggingFace Model installation</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>In the below steps we will download and install the HuggingFace Model in Vantage.</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a681ec89-7a9b-4fa3-8c36-ecbd61262f51",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none\">\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>4.1 Download the Model using Optium utility</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>We will be using <a href = 'https://huggingface.co/BAAI/bge-small-en-v1.5'>BAAI/bge-small-en-v1.5</a><br> The bge-small-en model is a small-scale English text embedding model developed by BAAI (Beijing Academy of Artificial Intelligence) as part of their FlagEmbedding project.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3913a608-c215-4819-bd71-29d6a69dda68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!optimum-cli export onnx --opset 16 --trust-remote-code -m BAAI/bge-small-en-v1.5 bge-small-en-v1.5-onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d50061-e830-4db3-bc89-2f63d7ef034e",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none\">\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>4.2 Model Preparation</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>In the below steps we will fix dynamic dims, fix versions for compatibility, etc and prepare the model to load in Vantage.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88541fa5-ebc3-446d-b77f-d81a86fada11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the operator set version\n",
    "op = onnx.OperatorSetIdProto()\n",
    "op.version = 16\n",
    "\n",
    "# Load the original ONNX model\n",
    "model = onnx.load('bge-small-en-v1.5-onnx/model.onnx')\n",
    "\n",
    "# Create a new model with a specified IR version and opset\n",
    "model_ir8 = onnx.helper.make_model(\n",
    "    model.graph,\n",
    "    ir_version=8,\n",
    "    opset_imports=[op]\n",
    ")\n",
    "\n",
    "# Fix variable dimension sizes\n",
    "rt.tools.onnx_model_utils.make_dim_param_fixed(model_ir8.graph, \"batch_size\", 1)\n",
    "rt.tools.onnx_model_utils.make_dim_param_fixed(model_ir8.graph, \"sequence_length\", 512)\n",
    "rt.tools.onnx_model_utils.make_dim_param_fixed(model_ir8.graph, \"Divsentence_embedding_dim_1\", 384)\n",
    "\n",
    "# Remove the unnecessary \"token_embeddings\" output\n",
    "for node in model_ir8.graph.output:\n",
    "    if node.name == \"token_embeddings\":\n",
    "        model_ir8.graph.output.remove(node)\n",
    "\n",
    "# Save the updated model\n",
    "onnx.save(model_ir8, 'bge-small-en-v1.5-onnx/model_fixed.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27878c2c-b929-4306-8d5c-03ed97f81ad5",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none\">\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>4.3 Model Results validation</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Checking that everything works with ONNX format locally.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd45b72-f1d1-4733-930f-d37615bccfad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences_1 = u'How is the weather today?'\n",
    "sentences_2 = u'What is the current weather like today?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a243399-aa88-4ca6-aa9b-86b9de42e5e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the tokenizer and ONNX model session\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"./bge-small-en-v1.5-onnx\")\n",
    "predef_sess = rt.InferenceSession(\"bge-small-en-v1.5-onnx/model_fixed.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b9c5a6-1a60-4703-a52e-307e556192bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the first sentence\n",
    "enc = tokenizer(sentences_1, max_length=512, padding='max_length')\n",
    "\n",
    "# Run inference to get embeddings for the first sentence\n",
    "result = predef_sess.run(\n",
    "    None,\n",
    "    {\n",
    "        \"input_ids\": [enc.input_ids],\n",
    "        \"attention_mask\": [enc.attention_mask]\n",
    "    }\n",
    ")\n",
    "\n",
    "# Tokenize the second sentence\n",
    "enc2 = tokenizer(sentences_2, max_length=512, padding='max_length')\n",
    "\n",
    "# Run inference to get embeddings for the second sentence\n",
    "result2 = predef_sess.run(\n",
    "    None,\n",
    "    {\n",
    "        \"input_ids\": [enc2.input_ids],\n",
    "        \"attention_mask\": [enc2.attention_mask]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61193709-b4e2-4810-a72a-350f04c035da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(cos_sim(result[0][0], result2[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93b921f-2496-4358-ba16-7ab835a01b92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the SentenceTransformer model\n",
    "model = SentenceTransformer('BAAI/bge-small-en-v1.5')\n",
    "\n",
    "# Generate normalized embeddings for both sentences\n",
    "embeddings_1 = model.encode(sentences_1, normalize_embeddings=True)\n",
    "embeddings_2 = model.encode(sentences_2, normalize_embeddings=True)\n",
    "\n",
    "# Print the cosine similarity between the two embeddings\n",
    "print(cos_sim(embeddings_1, embeddings_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27927e93-7794-4ad3-8c63-12e73f50c8a4",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none\">\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>4.4 Save the Model</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>In above steps, we have checked that the model is working fine in ONNX format. Now we will save the model file.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c8a458-138f-418a-8379-aeb7afd95322",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    tdml.save_byom('bge-small-en-v1.5')\n",
    "except Exception as e:\n",
    "    print(f\"The model bge-small-en-v1.5 already exist.\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    tdml.save_byom('bge-small-en-v1.5-onnx/model_fixed.onnx')\n",
    "except Exception as e:\n",
    "    print(f\"The model bge-small-en-v1.5-onnx/model_fixed.onnx already exist.\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    tdml.save_byom('embeddings_models')\n",
    "except Exception as e:\n",
    "    print(f\"The model embeddings_models already exist.\")\n",
    "    pass\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f8169c-8827-44da-8d4e-39963cd2f91d",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none\">\n",
    "<b style = 'font-size:20px;font-family:Arial'>5. Cleanup</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The following code will remove the context.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bfae31-60de-4ff9-8a64-8e52c44efd14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tdml.remove_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dfbb00-2743-437d-8c0e-4c0d8506214c",
   "metadata": {},
   "source": [
    "<footer style=\"padding-bottom:35px; border-bottom:3px solid \">\n",
    "    <div style=\"float:left;margin-top:14px\">ClearScape Analytics™</div>\n",
    "    <div style=\"float:right;\">\n",
    "        <div style=\"float:left; margin-top:14px\">\n",
    "            Copyright © Teradata Corporation - 2025. All Rights Reserved\n",
    "        </div>\n",
    "    </div>\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
