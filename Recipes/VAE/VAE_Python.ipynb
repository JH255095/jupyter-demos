{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57bba21e-c91d-474f-917b-c1fc781e4ebe",
   "metadata": {},
   "source": [
    "<header>\n",
    "   <p  style='font-size:36px;font-family:Arial; color:#F0F0F0; background-color: #00233c; padding-left: 20pt; padding-top: 20pt;padding-bottom: 10pt; padding-right: 20pt;'>\n",
    "       Variational AutoEncoders (VAE) in Genarative AI\n",
    "  <br>\n",
    "       <img id=\"teradata-logo\" src=\"https://storage.googleapis.com/clearscape_analytics_demo_data/DEMO_Logo/teradata.svg\" alt=\"Teradata\" style=\"width: 125px; height: auto; margin-top: 20pt;\">\n",
    "    </p>\n",
    "</header>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155ed9fb-a80a-4d19-ac52-4a3b31dbd2aa",
   "metadata": {},
   "source": [
    "<p style = 'font-size:20px;font-family:Arial'><b>Introduction</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>A Variational Autoencoder (VAE) is a type of machine learning model designed to generate new data similar to what it has learned. Unlike traditional autoencoders, which compress data into fixed representations, VAEs encode data into probability distributions (mean and standard deviation). This means that instead of mapping an input to a single point, VAEs create a range of possible values and sample from this range to generate outputs.<br> This approach introduces randomness (stochasticity), allowing the model to create diverse and realistic outputs from the same input. This makes VAEs particularly useful for applications like image generation, data synthesis, and anomaly detection, where variety and flexibility are important.<br>\n",
    "<ol style = 'font-size:16px;font-family:Arial'>Architecture of VAE:\n",
    "<li>VAEs employ an encoder-decoder architecture, where the encoder transforms input data into a probability distribution in the latent space.</li>\n",
    "<li>The latent code is a probabilistic encoding representing a distribution of potential representations, not just a single point.</li>\n",
    "<li>The decoder reconstructs data from a sampled point in the latent distribution, refining parameters to minimize training reconstruction loss.</li>\n",
    "<li >Training balances reconstruction loss and a regularization term, like Kullback-Leibler divergence, to shape the latent space distribution.</li>\n",
    "<li>Adjusting parameters iteratively during training results in latent space representation, results in precise data reconstruction.</li>\n",
    "    <center><img src=\"images/vae.png\" alt=\"VAE\"></center>\n",
    "</ol><\n",
    "<ol style = 'font-size:16px;font-family:Arial'>Usecases of VAE:\n",
    "<li><b>Image Generation</b>: Generate new faces, fashion images, artwork, etc.</li>\n",
    "<li><b>Anomaly Detection</b>: Identify unusual patterns in data (e.g., fraud detection).</li>\n",
    "<li><b>Video Prediction</b>: Generate future frames based on past frames.               </li>\n",
    "<li><b>Data Compression</b>: Compress images, audio, and text efficiently.              </li>\n",
    "<li><b>Drug Discovery</b>: Generate new molecular structures in pharmaceuticals.        </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd65ace8-c698-43cd-9755-7c36da9f30b2",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;background-color:#00233C;'>\n",
    "<b style = 'font-size:20px;font-family:Arial'>1. Configuring the environment</b>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>1.1 Install the required libraries</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a5d314-c887-4819-a455-bbed8a126d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cc094d-1d5b-497a-b920-30c814c0dfb4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<p style = 'font-size:16px;font-family:Arial'><b>Note: </b><i>Please restart the kernel after executing these two lines. The simplest way to restart the Kernel is by typing zero zero: <b> 0 0</b></i></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0116a2-acd8-419e-ac8e-473ca802c4f0",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;background-color:#00233C;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>1.2 Import the required libraries</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Here, we import the required libraries, set environment variables and environment paths (if required).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9909c6c-84a2-4461-bc33-f8839bb72e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d3efc6-7158-4048-a5a5-93e9d7bc121d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from teradataml import *\n",
    "\n",
    "# Modify the following to match the specific client environment settings\n",
    "display.max_rows = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925bb3b9-ea4a-43a6-8f77-0c9b2c1c3dcf",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial'><b>1.3 Connect to Vantage</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We will be prompted to provide the password. We will enter the password, press the Enter key, and then use the down arrow to go to the next cell.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a63fc0-e93f-4f7e-b1ee-0e5e056dfbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../../UseCases/startup.ipynb\n",
    "eng = create_context(host = 'host.docker.internal', username='demo_user', password = password)\n",
    "print(eng)\n",
    "execute_sql('''SET query_band='DEMO=VAE_Python.ipynb;' UPDATE FOR SESSION;''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c491b430-a04e-4d34-9595-c12b9650116b",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;background-color:#00233C;'>\n",
    "<b style = 'font-size:20px;font-family:Arial'>2. Load Image Dataset</b>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>2.1 Setting device if GPU is available</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6440edf0-742c-4ece-8a22-f84c49365ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc8f21f-f8be-4ec5-b494-b984d8e21aa8",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial'><b>2.2 Loading the Dataset</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a222525-7b57-4406-918e-0b037220078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset Class\n",
    "class XrayDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.images = [os.path.join(root, fname) for fname in os.listdir(root) if self.is_image_file(fname)]\n",
    "        print(f\"Found {len(self.images)} images in {root}\")\n",
    "\n",
    "    def is_image_file(self, filename):\n",
    "        return any(filename.endswith(extension) for extension in ['jpeg', 'jpg', 'png'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.images[index]\n",
    "        with open(img_path, 'rb') as f:\n",
    "            img = Image.open(f).convert('L')  # Convert to grayscale\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, 0  # Return a dummy label since we are not using labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d242d354-09b8-4e6b-ab8f-5282d519612d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "data_dir = 'xrays'\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Resize images\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    # Keep pixel values in range [0, 1]\n",
    "])\n",
    "\n",
    "dataset = XrayDataset(root=data_dir, transform=transform)\n",
    "train_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "print(f\"Data loader created with {len(train_loader)} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20ba7a6-a3e3-460f-9103-be343c4101cd",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;background-color:#00233C;'>\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>3. Define the VAE model</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>\n",
    "A VAE consists of two parts:<br>\n",
    "Encoder: Compresses the image into a latent space.<br>\n",
    "Decoder: Reconstructs the image from the latent space.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca0e398-e960-4738-9cee-043bf16fcc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the VAE model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(128*128, 400)\n",
    "        self.fc21 = nn.Linear(400, 20)  # Mean\n",
    "        self.fc22 = nn.Linear(400, 20)  # Log variance\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 128*128)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 128*128))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a54d806-e8cf-4bce-8211-27d09487c199",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;background-color:#00233C;'>\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>4. Define Loss Function</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>\n",
    "The loss has two components::<br>\n",
    "Reconstruction Loss (MSE or BCE) → Ensures output image is similar to input.<br>\n",
    "Kullback-Leibler (KL) Divergence → Ensures meaningful latent space.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15da4bd1-d4e6-400e-84af-a6ff6436748f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction loss + KL divergence loss\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 128*128), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180b1bdb-fef7-43d2-b057-9194661d574b",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;background-color:#00233C;'>\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>5. Train the model</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>\n",
    "The loss has two components:<br>\n",
    "Reconstruction Loss (MSE or BCE) → Ensures output image is similar to input.<br>\n",
    "Kullback-Leibler (KL) Divergence → Ensures meaningful latent space.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce7f5a4-e716-454c-b648-2e2b6d7f6c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "vae = VAE().to(device)\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = vae(data)\n",
    "        \n",
    "        # Ensure data is in the correct range\n",
    "        data = torch.clamp(data, 0, 1)\n",
    "        \n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {train_loss / len(train_loader.dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85b28fa-e8bb-4878-89bd-60af035bc806",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;background-color:#00233C;'>\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>6. Generate new images</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>\n",
    "Now that we have trained our VAE Model, we can use this to create new images.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092282d0-5f24-4893-986f-5d22bc53f979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new X-ray images\n",
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(64, 20).to(device)\n",
    "    sample = vae.decode(z).cpu()\n",
    "    sample = sample.view(64, 1, 128, 128)\n",
    "\n",
    "    # Plot the generated images\n",
    "    fig, axes = plt.subplots(8, 8, figsize=(10, 10))\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        ax.imshow(sample[i].squeeze(), cmap='gray')\n",
    "        ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60bd2b9-3dd1-4e0e-9e41-f09ca7aedfb9",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>\n",
    "The next step in the process is to iterate on image size and loss functions to inhance the quality of the generated images. In the below code we are doing the same steps as above and changing the size and loss function and training loop. We can see the difference in the output images generated.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442b4816-534d-427b-b108-39be467d1002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading and transformation\n",
    "data_dir = 'xrays'\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Increase image resolution\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1] for grayscale\n",
    "])\n",
    "\n",
    "dataset = XrayDataset(root=data_dir, transform=transform)\n",
    "train_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "print(f\"Data loader created with {len(train_loader)} batches\")\n",
    "\n",
    "# Define the VAE model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(256*256, 512)\n",
    "        self.fc21 = nn.Linear(512, 100)  # Mean\n",
    "        self.fc22 = nn.Linear(512, 100)  # Log variance\n",
    "        self.fc3 = nn.Linear(100, 512)\n",
    "        self.fc4 = nn.Linear(512, 256*256)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 256*256)\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "# Reconstruction loss + KL divergence loss\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 256*256), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "# Training loop\n",
    "vae = VAE().to(device)\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)  # Adjusted learning rate\n",
    "\n",
    "##########################################################\n",
    "epochs = 200  # Reduced number of epochs for demonstration\n",
    "# VARIATIONAL AUTOENCODERS\n",
    "##########################################################\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = vae(data)\n",
    "        \n",
    "        # Ensure data is in the correct range\n",
    "        data = torch.clamp(data, 0, 1)\n",
    "        \n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {train_loss / len(train_loader.dataset)}')\n",
    "\n",
    "# Generate new X-ray images\n",
    "vae.eval()\n",
    "# Create a folder to store generated images\n",
    "output_folder = \"vae_generated_images\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(64, 100).to(device)  # Adjusted latent dimension\n",
    "    sample = vae.decode(z).cpu()\n",
    "    sample = sample.view(64, 1, 256, 256)\n",
    "    \n",
    "    # Save images individually\n",
    "    for i in range(64):\n",
    "        img_array = sample[i].squeeze().numpy()  # Convert to NumPy\n",
    "        plt.imsave(f\"{output_folder}/generated_{i}.png\", img_array, cmap=\"gray\")\n",
    "    \n",
    "    # Plot the generated images\n",
    "    fig, axes = plt.subplots(8, 8, figsize=(12, 12))\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        ax.imshow(sample[i].squeeze(), cmap='gray')\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "print(f\"Generated images saved in '{output_folder}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01ddf2c-98f3-4602-831a-b160b4185968",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;background-color:#00233C;'>\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>7. Convert the VAE to ONNX Model</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>\n",
    "Once we have created the VAE, we can convert it into an ONNX model.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e71a70-a36a-46ae-9d08-023044911902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.onnx\n",
    "\n",
    "# Ensure model is in evaluation mode\n",
    "vae.eval()\n",
    "\n",
    "# Freeze parameters (Important!)\n",
    "for param in vae.parameters():\n",
    "    param.requires_grad = False  # Disable gradients for all parameters\n",
    "\n",
    "# Define a wrapper module for the decoder\n",
    "class VAE_Decoder(nn.Module):\n",
    "    def __init__(self, vae):\n",
    "        super(VAE_Decoder, self).__init__()\n",
    "        self.decoder = vae.decode  # Extract only the decoder part\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "# Create the decoder module\n",
    "vae_decoder = VAE_Decoder(vae).to(device)\n",
    "\n",
    "# Define a dummy latent input (batch_size=1, latent_dim=100)\n",
    "dummy_input = torch.randn(1, 100, device=device).detach()  # Ensure tensor does not require gradients\n",
    "\n",
    "# Convert to ONNX format\n",
    "onnx_model_path = \"vae_decoder.onnx\"\n",
    "torch.onnx.export(\n",
    "    vae_decoder, \n",
    "    dummy_input, \n",
    "    onnx_model_path,\n",
    "    export_params=True,\n",
    "    opset_version=11,\n",
    "    do_constant_folding=True,\n",
    "    input_names=[\"latent_vector\"],\n",
    "    output_names=[\"generated_image\"],\n",
    "    dynamic_axes={\"latent_vector\": {0: \"batch_size\"}, \"generated_image\": {0: \"batch_size\"}}\n",
    ")\n",
    "\n",
    "print(f\"ONNX model saved at: {onnx_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbd6772-ad86-4c64-ab0f-6bd458f65ba1",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>We have saved our model as vae_decoder.onnx. We can use this model in any python script to create and store the images.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fb3b02-43d4-4d94-a02d-b33317aeeba0",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;background-color:#00233C;'>\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>8. Using ONNX Model to Generate Images</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>\n",
    "Below code can also be written as a separate script to run the onnx model.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a701f34-daf0-4296-9de3-de9e055d9fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load ONNX model\n",
    "session = ort.InferenceSession(\"vae_decoder.onnx\")\n",
    "\n",
    "# Generate random latent vectors\n",
    "latent_dim = 100  # Must match trained model\n",
    "latent_vectors = np.random.randn(10, latent_dim).astype(np.float32)  # Generate 10 samples\n",
    "\n",
    "# Run inference using the ONNX model\n",
    "outputs = session.run([\"generated_image\"], {\"latent_vector\": latent_vectors})\n",
    "\n",
    "# Convert and display images\n",
    "fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.imshow(outputs[0][i].reshape(256, 256), cmap=\"gray\")  # Adjust based on training setup\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d63e09-c49a-4651-93f6-829cc65f9d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9335e8-4573-413b-8035-6bb31b5f71ae",
   "metadata": {},
   "source": [
    "<footer style=\"padding-bottom:35px; border-bottom:3px solid #91A0Ab\">\n",
    "    <div style=\"float:left;margin-top:14px\">ClearScape Analytics™</div>\n",
    "    <div style=\"float:right;\">\n",
    "        <div style=\"float:left; margin-top:14px\">\n",
    "            Copyright © Teradata Corporation - 2025. All Rights Reserved\n",
    "        </div>\n",
    "    </div>\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
