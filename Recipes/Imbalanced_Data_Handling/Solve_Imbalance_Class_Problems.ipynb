{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "566f2242-7fa7-4bf5-be69-0269f6010913",
   "metadata": {},
   "source": [
    "<header>\n",
    "   <p  style='font-size:36px;font-family:Arial; color:#F0F0F0; background-color: #00233c; padding-left: 20pt; padding-top: 20pt;padding-bottom: 10pt; padding-right: 20pt;'>\n",
    "       Solve Imbalanced Class Problems with ClearScape Analytics\n",
    " <br>       \n",
    "       <img id=\"teradata-logo\" src=\"https://storage.googleapis.com/clearscape_analytics_demo_data/DEMO_Logo/teradata.svg\" alt=\"Teradata\" style=\"width: 150px; height: auto; margin-top: 20pt;\">\n",
    "  <br>\n",
    "    </p>\n",
    "</header>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c308d4a-d97c-48b4-bcc7-b2c3cc582ff7",
   "metadata": {},
   "source": [
    "<p style = 'font-size:20px;font-family:Arial'><b>Introduction</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Real-world datasets are very likely to have the problem of imbalanced classes. In scenarios like fraud detection, customer churn prediction, or medical diagnosis, one class is often heavily outnumbered by the other. The result? Predictive models that ignore the rare but critical minority class. Left unchecked, imbalanced data can distort your model’s performance and lead to poor decision-making. In this notebook we will describe the core challenges posed by imbalanced classes and how addressing them is key to building better models. This is a practitioner's guide on imbalanced class classification, with the focus of putting it into production using Teradata's ClearScape Analytics.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>To get a clearer picture of how imbalanced classes show up in real-world situations, let's check out some common examples. The table below shows cases from industries like finance, healthcare, and marketing, where the uneven split between majority and minority classes can cause costly errors. In each scenario, false positives and false negatives have different consequences, highlighting why it’s so important to tackle the imbalance and keep models performing well. These examples give you a practical sense of what’s at stake when working with imbalanced datasets.</p>\n",
    "\n",
    "<table style = 'font-size:16px;font-family:Arial;border: 1px solid black'>\n",
    "  <tr style = 'font-size:16px;font-family:Arial;border: 1px solid black'>\n",
    "    <th>Real-World Example</th>\n",
    "    <th>Source of Imbalance</th>\n",
    "    <th>False Positive</th>\n",
    "    <th>False Negative</th>  \n",
    "  </tr>\n",
    "  <tr style = 'font-size:14px;font-family:Arial;border: 1px solid black'>\n",
    "      <td><b>Fraud Detection (Credit Card)</b></td>\n",
    "      <td>Fraudulent transactions are rare</td>\n",
    "      <td>Flagging a legitimate transaction as fraud, causing customer inconvenience. </td>\n",
    "      <td>Missing a fraudulent transaction, leading to financial loss.</td> \n",
    "  </tr>\n",
    "  <tr style = 'font-size:14px;font-family:Arial;border: 1px solid black'>\n",
    "      <td><b>Medical Diagnosis (Cancer)</b></td>\n",
    "      <td>Most patients tested do not have cancer</td>\n",
    "      <td>Diagnosing a healthy person with cancer, leading to stress and unnecessary treatment.</td>\n",
    "      <td>Missing a cancer diagnosis, delaying treatment and reducing survival chances.</td>\n",
    "  </tr>\n",
    "  <tr style = 'font-size:14px;font-family:Arial;border: 1px solid black'>\n",
    "    <td><b>Spam Detection (Email)</b></td>\n",
    "      <td>Most emails are legitimate</td>\n",
    "      <td>Marking a legitimate email as spam, causing important communication to be missed.</td>\n",
    "      <td>Allowing spam into the inbox, increasing the risk of phishing or malware.</td>\n",
    "  </tr>\n",
    "  <tr style = 'font-size:14px;font-family:Arial;border: 1px solid black'>\n",
    "    <td><b>Loan Default Prediction</b></td>\n",
    "      <td>Most borrowers repay loans on time</td>\n",
    "      <td>Denying a loan to a creditworthy individual, losing potential business.</td>\n",
    "      <td>Approving a loan to a high-risk borrower, leading to financial loss from default.</td>\n",
    "  </tr>\n",
    "  <tr style = 'font-size:14px;font-family:Arial;border: 1px solid black'>\n",
    "    <td><b>Churn Prediction (Customer)</b></td>\n",
    "      <td>Most customers stay, only a few churn</td>\n",
    "      <td>Targeting a loyal customer for retention efforts, wasting resources.</td>\n",
    "      <td>Failing to identify a churn risk, losing a valuable customer.</td>\n",
    "  </tr>\n",
    "  <tr style = 'font-size:14px;font-family:Arial;border: 1px solid black'>\n",
    "    <td><b>Predictive Maintenance (Manufacturing)</b></td>\n",
    "      <td>Equipment failures are rare compared to normal operation</td>\n",
    "      <td>Incorrectly predicting a failure, causing unnecessary downtime and maintenance costs.</td>\n",
    "      <td>Failing to predict a true failure, leading to unexpected downtime, lost production, and costly repairs.</td>\n",
    "  </tr>  \n",
    "</table>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Now that we’ve got an idea on what imbalanced classes are and the challenges they bring, the next step is to understand different ways to tackle the problem. The diagram below gives an overview of methods across two dimensions: when in the modeling process they’re applied and how much effort it takes to put them into production. The diagram flows from left to right, covering the different workflow stages like data preprocessing, modeling, and post-processing. The vertical axis shows the effort needed to implement each method in production but, this is a subjective call.</p>\n",
    "\n",
    "<img src=\"images/effortvsstage.png\"> \n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Experienced data scientists usually approach this issue with a trial-and-error mindset, testing out methods to see how they impact model performance. There’s no one-size-fits-all fix for imbalanced data; every dataset is different, and what works for one might not work for another. In practice, we might get some quick wins by adjusting class weights or using resampling techniques, but more advanced methods like active learning or custom loss functions could be necessary too. By experimenting with these techniques, we can find a solution that strikes the right balance between performance and ease of implementation.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4866c641-2a89-4324-a204-b4b15948a267",
   "metadata": {
    "tags": []
   },
   "source": [
    "<hr style=\"height:2px;border:none;background-color:#00233C;\">\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>1. Connect to Vantage</b></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a77808-6115-4f93-bf0c-5668f3a701ca",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>In the section, we import the required libraries and set environment variables and environment paths (if required).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36faad44-0ddb-4c97-8aa1-c1e118de78ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import getpass\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from teradataml import *\n",
    "display.max_rows = 5\n",
    "# from teradataml import configure\n",
    "# configure.val_install_location = \"val\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8887bd9c-06d8-41af-af41-2b603f9c00b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>We will be prompted to provide the password. We will enter the password, press the Enter key, and then use the down arrow to go to the next cell. Begin running steps with Shift + Enter keys.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38a2e78-c239-4c36-bfb2-e5598251036c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run -i ../../UseCases/startup.ipynb\n",
    "eng = create_context(host = 'host.docker.internal', username = 'demo_user', password = password)\n",
    "print(eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98fe287-1088-493d-bd54-80b1016dc510",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "execute_sql('''SET query_band='DEMO=PP_Recipe_Solve_Imbalance_Class_Problems.ipynb;' UPDATE FOR SESSION; ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedea960-98bf-4371-8335-8bfcf6466b7f",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;background-color:#00233C;\">\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>2. Getting Data for This Demo</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We have provided data for this demo on cloud storage. We have the option of either running the demo using foreign tables to access the data without using any storage on our environment or downloading the data to local storage, which may yield somewhat faster execution. However, we need to consider available storage. There are two statements in the following cell, and one is commented out. We may switch which mode we choose by changing the comment string.</p>   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c134f0e7-58c9-4a40-b7af-9079049f690b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run -i ../../UseCases/run_procedure.py \"call get_data('DEMO_DataImbalance_cloud');\"  # Takes about 20 secs\n",
    "%run -i ../../UseCases/run_procedure.py \"call get_data('DEMO_DataImbalance_local');\"  # Takes about 50 secs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d197cef-4d8e-4541-b841-262747535859",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Next is an optional step – if you want to see status of databases/tables created and space used.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52108abd-36cf-4034-8a90-213f2bcfd673",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../../UseCases/run_procedure.py \"call space_report();\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f730d7d-5349-4707-beee-f2c8ad9a029f",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>3. Analyze the raw data set</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Let us start by creating a teradataml dataframe. A \"Virtual DataFrame\" that points directly to the dataset in Vantage.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91fb602-d623-4640-9c24-96dac560fabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=DataFrame(in_schema(\"DEMO_DataImbalance\",\"imbalanced_data\"))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dc8074-0f16-4c44-a50e-58cd095e0931",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>We are using synthethic data here for showing the various techniques that can be used on imbalanced data. As we can see above this dataset is a dummy dataset.</p>\n",
    "<p></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We will use the TrainTestSplit function to split the dataset into two.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42ae429-4d24-4df3-a053-ab2151cba30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df.columns\n",
    "cols.remove('row_id')\n",
    "cols.remove('classlabel')\n",
    "all_features = cols\n",
    "target = \"classlabel\"\n",
    "key = \"row_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8604c0-49cb-4a12-886d-5bf1440a5b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in training & test set 80 20 stratified\n",
    "DF_splitted = TrainTestSplit(\n",
    "    data = df, \n",
    "    id_column = 'row_id', \n",
    "    stratify_column='classlabel', \n",
    "    seed = 42, \n",
    "    train_size = 0.8\n",
    ").result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3ae64e-f86d-44ef-b532-f2db02d08d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_train = (\n",
    "    DF_splitted\n",
    "    .loc[DF_splitted.TD_IsTrainRow==1]\n",
    "    .drop(columns=[\"TD_IsTrainRow\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240c6bc3-353c-49fe-b72a-c3465fb8687e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_test = (\n",
    "    DF_splitted\n",
    "    .loc[DF_splitted.TD_IsTrainRow==0]\n",
    "    .drop(columns=[\"TD_IsTrainRow\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ad2528-8916-45db-b7b7-8f9813bc3fbb",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>3.1 Initial Analysis: Univariate Statistics, Visualization, and a Baseline Model</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>To start our analysis, we'll conduct some Exploratory Data Analysis (EDA) to get a better feel for the dataset and set up a baseline model. We’ll use <code>teradataml</code> as our Python client and keep the entire analysis within the database, taking full advantage of in-database processing for speed and scalability.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>EDA is super important when dealing with imbalanced classification problems because it helps us spot patterns, understand how features are distributed, and catch any anomalies that might affect model performance. For imbalanced datasets, it’s especially crucial to see how features relate to the minority class, as even small differences can have a big impact.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>We’ll start by calculating Cohen's d for each feature. This metric tells us the effect size, showing how much each feature separates the minority class from the majority class, helping us find the most relevant features. Then, we’ll visualize the feature with the strongest effect size using bivariate histograms to see how its distribution changes across classes.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>To keep things efficient, We’ve put together a Python module <code>(`tdimbalancedlearn`, imported as tdimbl)</code> with reusable functions for common tasks. We’ll just import and use these functions directly.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee531d2-c97b-4894-a62b-1a9e9af755e6",
   "metadata": {},
   "source": [
    "<p style = 'font-size:20px;font-family:Arial'><b>Cohen's D and Boxplots</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>To get a sense of how each feature affects our target variable, we’ll calculate Cohen’s d using <code>UnivariateStatistics</code> and a bit of DataFrame manipulation. [Cohen’s](https://rpsychologist.com/cohend/) d gives us a measure of effect size, helping us find out which features show the biggest difference between the minority and majority classes. This helps us focus on the features that matter most.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Next, we create a simple boxplot based on these statistics so we can visually compare the distributions. This makes it easy to quickly spot which features have the most significant impact.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2540abd-b69c-4653-aa2e-b09321f980ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tdimbalancedlearn as tdimbl\n",
    "DF_cohensd = tdimbl.get_cohens_d_stats(DF_train, all_features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d659d23-5e75-4782-bddd-8ad16a0dfddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "this_fig = tdimbl.plot_grouped_boxplot(\n",
    "    DF_cohensd.to_pandas().sort_values(by= \"Attribute\", key=lambda x: x.str.extract(r'(\\d+)')[0].astype(int)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8917043e-d950-40ff-ac52-6b130987436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_cohensd.select([\"Attribute\",\"cohens_d\",\"abs_cohens_d\"]).sort(\"abs_cohens_d\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd7d572-efb4-4eb0-afe9-54bde622d635",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>We can see that the <code>feature_19</code> stands out with the strongest effect size. We’ll focus on this feature next to create bivariate histograms, as it gives the most noticeable separation between the classes.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb59d67-5e10-4264-a384-f52c8e94b7af",
   "metadata": {},
   "source": [
    "<p style = 'font-size:20px;font-family:Arial'><b>Histograms: Understanding Feature Distributions</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>As observed <code>feature_19</code> is the most influential variable using Cohen’s d, we’ll visualize its distribution across classes using histograms. We should keep in mind, while Cohen’s d assumes normal distributions, real-world data—especially in imbalanced cases—doesn’t always play by those rules. Histograms let us check this assumption and see if the groups are actually distinguishable in practice.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Our <code>plot_histograms</code> function creates both absolute and relative frequency plots for <b>feature_19</b>. Below steps are a part of the function:</p>\n",
    "<li style = 'font-size:16px;font-family:Arial'><code>Histogram Calculation:</code> We use the in-database `tdml.Histogram` function to get the percentile-based distribution of the feature.</li>\n",
    "<li style = 'font-size:16px;font-family:Arial'><code>Pivoting the Table:</code> The result is then pivoted so we can easily compare the two classes.</li>\n",
    "<li style = 'font-size:16px;font-family:Arial'><code>In-Database Plotting:</code> Finally, we use in-database plotting with equal-width bins, making sure the bar chart accurately shows how the feature’s values are spread.</li>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>By plotting both absolute and relative frequencies, we get a fuller picture. The <code>absolute frequency plot</code> shows raw counts, which gives us a sense of the majority class’s dominance. The <code>relative frequency plot</code>, on the other hand, scales everything to percentages, so we can better compare the shapes of the distributions regardless of the class sizes.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Looking at the histograms, you can see there’s a lot of overlap between the majority (blue) and minority (red) classes, but there are also areas where the minority class stands out, especially in the negative range of the feature values.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9ea8a6-581c-41a6-addc-acf6f1890330",
   "metadata": {},
   "outputs": [],
   "source": [
    "histplot = tdimbl.plot_histograms(DF_train, \"feature_19\", target)\n",
    "histplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593b41d7-8a96-4c67-a8b0-8c3b008085b9",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>4. Baseline GLM Model and Performance Check: ROC & AUC</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>To set up a benchmark, we start with a simple <b><u>G</u></b>eneralized <b><u>L</u></b>inear <b><u>M</u></b>odel <b>(GLM)</b> as our baseline. This \"vanilla\" version of the GLM is trained without any special techniques to handle the class imbalance. By starting with this basic model, we can later compare its performance to more advanced methods designed to deal with imbalanced data. We’ll measure its effectiveness using the ROC curve and AUC values.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>We take advantage of in-database processing for both training and prediction. The model training uses the <code>GLM</code> function, and the predictions are handled through <code>TDGLMPredict</code>, so there’s no need to move data out of the database.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c00d4f0-e519-4660-bfc5-73d6948013e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLM_obj = GLM(data = DF_train, \n",
    "                   input_columns=all_features,\n",
    "                   response_column = target,\n",
    "                   family = \"BINOMIAL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b477d8-468e-4ac6-aaee-ef353a9e7138",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_pred = TDGLMPredict(\n",
    "                    object=GLM_obj, \n",
    "                    newdata=DF_test,\n",
    "                    id_column=key, \n",
    "                    accumulate=[target], \n",
    "                    output_prob=True,\n",
    "                    output_responses=\"1\").result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd053b07-06db-4920-85c1-b93b4449315a",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>To see how our model performs, we calculate classification metrics and ROC stats, storing everything in database tables for consistency. We use the in-database <code>ClassificationEvaluator</code> function, which gives us both a confusion matrix and a set of standard metrics like precision, recall, and F1-score.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>With imbalanced datasets, these metrics need to be taken with a grain of salt  as they might not show the whole picture. For example, accuracy can look high simply because the majority class dominates. That’s why looking at the confusion matrix or the ROC curve can be more helpful, as these give a more detailed view of how the model is doing, especially for the minority class.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Next, we calculate the ROC (Receiver Operating Characteristic) curve using the `tdml.ROC` function. The ROC curve shows how the true positive rate (sensitivity) and the false positive rate change across different threshold levels, giving us a visual sense of the model’s performance at various points.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>The AUC (Area Under the Curve) value gives us a single score to measure the model's performance, showing how likely the model is to rank a random positive example higher than a random negative one. The ROC and AUC are particularly useful because they help us choose the threshold that makes the most business sense. This threshold isn’t always set at 0.5 but instead is based on what maximizes value, considering the costs of false positives and false negatives.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8646d3b8-f2be-45d1-bc45-a4b3abc69a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdimbl.clean_classeval(eval_id = \"eval\")\n",
    "tdimbl.clean_roc(eval_id = \"eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e250761-2632-49a5-865d-806d197f3926",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that performs classification evaluation on the predicted data and save the results\n",
    "tdimbl.save_classeval(\n",
    "    DF_pred, \n",
    "    model_id = \"vanilla_glm\", \n",
    "    key= key, \n",
    "    target= target, \n",
    "    eval_id  = \"eval\", # Table prefix \n",
    "    prediction = \"prediction\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af129e5f-edc9-4af5-827f-7ce9ffe03661",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_conf,DF_metrics = tdimbl.get_classeval(eval_id  = \"eval\")\n",
    "DF_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba02908-58e8-4917-b265-4f1fe38731de",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f67bd87-2ea3-4a9c-aa37-2509e87c885c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save ROC statistics for the model\n",
    "tdimbl.save_roc(\n",
    "    DF_pred, \n",
    "    model_id =  \"vanilla_glm\", \n",
    "    key= key, \n",
    "    target = target, \n",
    "    probability = \"prob_1\", \n",
    "    eval_id = \"eval\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a801a6-91c9-4664-825d-5164d8359355",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_roc, DF_auc = tdimbl.get_rocauc(eval_id = \"eval\")\n",
    "tdimbl.plot_roc_curves(DF_roc, highlight_model=\"vanilla_glm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77fd8c6-ef55-49ec-aef4-25979a7de161",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d305247-0ff9-4c81-83a2-fe05694719fc",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Let’s take a look at the initial results: The ROC curve for the <code>vanilla_glm</code> model shows it’s only doing a little better than random guessing, which you can tell by how close it is to the diagonal line. The AUC score of 0.567 indicates that the model doesn’t have much power when it comes to telling the two classes apart. This isn’t too surprising, though, considering the model is quite basic and the dataset is imbalanced.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0055d06-2759-4f18-9438-d5f036103b45",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>5. Post Processing Methods</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Now that we’ve built a simple baseline model, it’s important to remember that a model’s (probability) predictions don’t automatically translate into business decisions. In real-world scenarios, predictions need to be interpreted and adjusted to match business goals and constraints. In this section, we’ll look at post-processing methods that can help fine-tune model outputs to make them more useful for decision-making.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>We’ll focus on two main techniques: <code>threshold tuning</code> and <code>cost-sensitive tuning</code>. These methods let us fine-tune the decision boundary and adjust the outputs based on the costs of false positives and false negatives. This way, we can make sure the model fits business needs and helps minimize risks.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53587e44-e5d1-43cb-9367-ef417e692d33",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>5.1 Threshold Tuning</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>In real-world scenarios, business requirements often set specific performance targets for models. We’ve kept our dataset use-case-neutral up until now, but for this section, let’s imagine it’s for predicting credit defaults. For instance, the goal might be to catch at least **70%** of credit defaults (true positive rate ≥ 0.70). In these cases, the classification threshold should be adjusted to hit that target while keeping the false positive rate (FPR) as low as possible.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>To find the best threshold, we filter the ROC dataframe for points where the true positive rate (TPR) is above 0.70, then choose the one with the lowest FPR. Here’s how you can do this using a <code>teradataml</code> DataFrame.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5503a51c-ac4b-4d76-a198-ff0d715c6836",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_roc.loc[DF_roc.tpr>0.70].sort(\"fpr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bc1d17-483b-41ee-a621-74ab868c0b1d",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>5.2 Cost-Sensitive Tuning</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Cost-sensitive tuning is all about optimizing model predictions based on the business costs and benefits tied to different outcomes. Let’s say our data represents a banking scenario where we’re predicting the probability of credit defaults and deciding whether or not to approve a customer’s credit application. The idea is to choose a threshold that maximizes overall business outcomes, taking into account the financial impact of true positives, false positives, true negatives, and false negatives.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>To do this, we evaluate the confusion matrix at each threshold and multiply the frequency of each outcome (e.g., true positive, false negative) by its business value. Adding these up gives us the overall outcome for each threshold, and our goal is to pick the one that gives the best result.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>The <code>tdimbl.calculate_cost</code> function does this calculation by looping through the ROC dataframe and finding the business outcome for each threshold. We use in-database plotting to see how different thresholds impact the overall outcome, and the `WhichMax` function helps us find the one with the highest return.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>In our example, here’s how we’ve defined the business values:</p>\n",
    "<li style = 'font-size:16px;font-family:Arial'><code>True Positive</code>: The customer is denied a credit they wouldn’t have been able to repay. No gain or loss. - Outcome: <b>0</b></li>\n",
    "    \n",
    "<li style = 'font-size:16px;font-family:Arial'><code>False Positive</code>: The customer is denied a credit they could have repaid. Again, no gain or loss. - Outcome: <b>0</b></li>\n",
    "    \n",
    "<li style = 'font-size:16px;font-family:Arial'><code>True Negative</code>: The customer is approved and repays, earning interest. - Outcome: <b>+600</b></li>\n",
    "    \n",
    "<li style = 'font-size:16px;font-family:Arial'><code>False Negative</code>: The customer is approved but defaults, leading to a loss. - Outcome: <b>-30,000</b></li></p>\n",
    "    \n",
    "<p style = 'font-size:16px;font-family:Arial'>By factoring these values into the cost-sensitive tuning, we make sure that our model’s threshold selection focuses on maximizing business value, rather than just looking at basic performance metrics.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784017d1-6c28-4952-a868-8c7860435672",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_tp =       0\n",
    "outcome_fp =       0\n",
    "outcome_tn = +   600 \n",
    "outcome_fn = - 30000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad38794-f037-4daf-aec3-356091585a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_cost = tdimbl.calculate_costs(\n",
    "    DF_test, \n",
    "    outcome_tp, \n",
    "    outcome_fp, \n",
    "    outcome_tn, \n",
    "    outcome_fn, \n",
    "    eval_id = \"eval\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbe988f-7081-4d93-aedf-a59658c657be",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_cost.plot(x = DF_cost.threshold_value, \n",
    "             y = DF_cost.total_outcome,\n",
    "                kind=\"line\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53018117-b193-48ed-9561-09ebef591638",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "threshold = (\n",
    "    WhichMax(\n",
    "        data=DF_cost.select([\"threshold_value\",\"total_outcome\"]),\n",
    "        target_column=\"total_outcome\")\n",
    "    .result.to_pandas().threshold_value.values[0]\n",
    ")\n",
    "threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f4a925-c4e0-4c84-93f7-446f7dfd7004",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Based on our cost-sensitive analysis, the best threshold from the <code>WhichMax</code> function is <code>0.0101</code>, which could bring in about <code>USD 643,200</code> for the 20,000 customers in our test set. This threshold strikes a balance between approving and denying credit applications, maximizing the overall business value.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>However, keep in mind that this threshold was tuned using the test set, so to make sure it holds up, it’s best to validate it on a separate evaluation set to see if it really works outside of the test data.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Even though the results are positive, this approach isn’t without issues. At this threshold, <code>14,787</code> out of 20,000 customers get denied credit, even though they could have paid it back. This high rejection rate comes from the fact that the cost of a default (USD -30,000) is way bigger than the profit from successful repayments (USD +600). So, the model focuses on avoiding defaults, even if that means rejecting a lot of good customers.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Doubts aside, deploying this threshold would be pretty simple. It just involves adding a column in the production setup that checks if the predicted probabilities meet the <code>0.0101</code> threshold, making sure the decisions match up with the business value goal.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32638ad-c248-4b6d-a99b-5bf3e10ab165",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_pred.assign(credit_denied = case([(DF_pred.prob_1 > threshold,1)], else_=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d673ff6-022e-4a96-a7f0-45b2c10e8c95",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>6. Undersample & Oversample</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Next, we focus on the data prep stage and how to fix the imbalance using some simple sampling techniques. The easiest ones are random undersampling, oversampling, and a combo of both. These methods balance the classes by either reducing the majority class (undersampling) or boosting the minority class (oversampling).</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>These methods are easy to apply, but they come with trade-offs. Undersampling helps shrink the dataset, which speeds up training and saves resources. However, it can also drop important data points from the majority class, leading to increased variance. Oversampling, meanwhile, duplicates samples from the minority class to balance things out. This way, you don’t lose any data, but there’s a risk the model might overfit by memorizing the repeated samples.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>A combination of the two tries to find a middle ground by trimming the majority class and boosting the minority class, which can help build a more balanced model. But it’s still important to manage the bias and variance carefully. On the technical side, we can easily use the <code>sample</code> function on <code>DF_train</code> for these methods. For undersampling, <code>sample</code> the majority class without replacement. For oversampling, set <code>replace=True</code> to sample with replacement and make sure you have enough examples. Also, set <code>randomize=True</code> to keep the sampling process random and consistent.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc24dbc-0442-42ca-8733-1bf557b31964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# undersample, we know we have 1600 of the minortiy class. \n",
    "# we want to have a ratio of 4:1\n",
    "(DF_train\n",
    "     .loc[DF_train[target] == 0]\n",
    "     .sample(n=4*1600, randomize=True, seed=42, id_column=key).drop(columns=[\"sampleid\"])\n",
    "     .to_sql(\"undersampled_train\", if_exists=\"replace\", primary_index=key))\n",
    "\n",
    "(DF_train\n",
    "     .loc[DF_train[target] == 1]\n",
    "     .to_sql(\"undersampled_train\", if_exists=\"append\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff67666-ee17-4574-848f-f63da534ec43",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_undersampled_train = DataFrame(\"undersampled_train\")\n",
    "DF_undersampled_train.select([\"row_id\",\"classlabel\"]).groupby(\"classlabel\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e147d68a-330e-44f8-bf51-5fe087d6a1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversample, we know we have 78400 of the majority class. \n",
    "# we want to have a ratio of 4:1\n",
    "(DF_train\n",
    "     .loc[DF_train[target] == 0]\n",
    "     .to_sql(\"oversampled_train\", if_exists=\"replace\", primary_index=key))\n",
    "\n",
    "(DF_train\n",
    "     .loc[DF_train[target] == 1]\n",
    "     .sample(n=int(78400/4), \n",
    "             replace = True,\n",
    "             randomize=True, seed=42, id_column=key)\n",
    "     .drop(columns=[\"sampleid\"])\n",
    "     .to_sql(\"oversampled_train\", if_exists=\"append\"))\n",
    "\n",
    "## continue with machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca933610-6314-452e-888a-6a5f4bb97ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_oversampled_train = DataFrame(\"oversampled_train\")\n",
    "DF_oversampled_train.select([\"row_id\",\"classlabel\"]).groupby(\"classlabel\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083678f9-98a8-4ea4-8dd8-bae9bf2e8cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample, # we want to have a ratio of 4:1, check if this makes sense\n",
    "\n",
    "(DF_train.sample(case_when_then = {\n",
    "                     DF_train[target] == 0: 40000,\n",
    "                     DF_train[target] == 1: 10000, },\n",
    "             replace = True,\n",
    "             randomize=True, seed=42, id_column=\"row_id\")\n",
    "      .drop(columns=[\"sampleid\"])\n",
    "     .to_sql(\"resampled_train\", if_exists=\"replace\"))\n",
    "\n",
    "## continue with machine learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29f83c3-ea7c-4448-a747-1920bc4752b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_resampled_train = DataFrame(\"resampled_train\")\n",
    "DF_resampled_train.select([\"row_id\",\"classlabel\"]).groupby(\"classlabel\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e78f9e-6e16-4144-9aa3-ea2a70a7d5c6",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Next, we train the GLM model again using the resampled datasets, just like we did before. We then evaluate its performance using the original, non-resampled <code>DF_test</code> dataset to see how the different resampling methods affect the results compared to our baseline model.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e16b6c2-6921-4dbf-acd1-e69a7ff1963d",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLM_undersampled_obj = GLM(data = DF_undersampled_train, \n",
    "                   input_columns=all_features,\n",
    "                   response_column = target,\n",
    "                   family = \"BINOMIAL\",\n",
    "                   volatile = True)\n",
    "\n",
    "GLM_oversampled_obj = GLM(data = DF_oversampled_train, \n",
    "                   input_columns=all_features,\n",
    "                   response_column = target,\n",
    "                   family = \"BINOMIAL\",\n",
    "                   volatile = True)\n",
    "\n",
    "GLM_resampled_obj = GLM(data = DF_resampled_train, \n",
    "                   input_columns=all_features,\n",
    "                   response_column = target,\n",
    "                   family = \"BINOMIAL\",\n",
    "                   volatile = True)\n",
    "\n",
    "DF_pred_undersampled = TDGLMPredict(\n",
    "        object=GLM_undersampled_obj, \n",
    "        newdata=DF_test,\n",
    "        id_column=key, \n",
    "        accumulate=[target], \n",
    "        output_prob=True,\n",
    "        output_responses=\"1\", \n",
    "        volatile = True).result\n",
    "tdimbl.save_roc(DF_pred_undersampled, model_id=\"undersampled_glm\", key=key, target=target )\n",
    "\n",
    "DF_pred_oversampled = TDGLMPredict(\n",
    "        object=GLM_oversampled_obj, \n",
    "        newdata=DF_test,\n",
    "        id_column=key, \n",
    "        accumulate=[target], \n",
    "        output_prob=True,\n",
    "        output_responses=\"1\", \n",
    "        volatile = True).result\n",
    "tdimbl.save_roc(DF_pred_oversampled, model_id=\"oversampled_glm\" , key=key, target=target )\n",
    "\n",
    "DF_pred_resampled = TDGLMPredict(\n",
    "        object=GLM_resampled_obj, \n",
    "        newdata=DF_test,\n",
    "        id_column=key, \n",
    "        accumulate=[target], \n",
    "        output_prob=True,\n",
    "        output_responses=\"1\", \n",
    "        volatile = True).result\n",
    "tdimbl.save_roc(DF_pred_resampled, model_id=\"resampled_glm\" , key=key, target=target )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f789b97-18ca-4bd6-ad23-4e2955db98c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698c740c-8abc-4f46-9419-e3d0d95df95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# highlight undersampled because of best AUC value\n",
    "tdimbl.plot_roc_curves(DF_roc, highlight_model= \"undersampled_glm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c15649a-7ae6-40b7-ad47-df4b3891b85e",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Interestingly, we see quite some improvement, with the undersampling approach delivering the highest AUC among all the methods we’ve tried so far. The ROC curves clearly show that undersampling gives a solid boost in performance compared to the baseline and other resampling techniques.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>This shows that even a simple method like undersampling can lead to better results, but it’s worth noting that these improvements depend on the specific dataset and might not work in every situation. Also, it’s important to point out that the evaluation was done on the original, non-resampled test dataset to make sure the results are a true reflection of real-world performance.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd108d6-cf74-4832-8c68-8dcb3b638b86",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>7. SMOTE and ADASYN</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>SMOTE (Synthetic Minority Over-sampling Technique) and ADASYN (Adaptive Synthetic Sampling) are more advanced than simple methods like random oversampling or undersampling. Instead of just duplicating existing samples, these techniques create synthetic instances for the minority class based on the feature space, giving us more realistic and varied examples. This helps reduce the overfitting risk that comes with basic oversampling, as the model gets new examples rather than seeing the same ones over and over.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>The big advantage of SMOTE and ADASYN is that they can enrich the minority class with more diverse and useful samples, which can lead to better decision boundaries and overall model performance. SMOTE works by creating synthetic data points by interpolating between existing samples from the minority class, while ADASYN takes it a step further by focusing on areas where the minority class is underrepresented, making the model tackle the tough spots.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>That said, these methods aren’t without their downsides. By generating synthetic data, there’s a risk of adding noise or patterns that don’t actually exist in real-world scenarios, which can lead to overfitting and make the model less generalizable. Plus, if the minority class isn’t well represented in the first place, these synthetic examples might not accurately reflect the data’s true distribution, which could result in biased models.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>To see how effective SMOTE and ADASYN are, we’ll use the `imbalanced-learn` python package to resample our dataset. We’ll start by generating synthetic samples with SMOTE, then use ADASYN to check out how each approach enriches the minority class differently. Once we’ve done that, we’ll move back to the database, train a GLM model on the newly balanced datasets, and test its performance using the test set, just like we did with the other models.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630f76c9-5b4e-42d5-96e0-0f478b8a6b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import  SMOTE, ADASYN\n",
    "df_train = DF_train.to_pandas(all_rows = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139b1f36-3535-42e3-86a6-2165b5aa9f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE( sampling_strategy = 20000.0/80000.0, random_state=42)\n",
    "X_res, y_res = sm.fit_resample(df_train[all_features], df_train[target])\n",
    "\n",
    "df_train_smote = pd.DataFrame(X_res, columns=all_features)\n",
    "df_train_smote[target] = y_res\n",
    "df_train_smote = df_train_smote.reset_index().rename(columns={'index': key})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e566d3c-69fa-4477-b1f7-adf8d8d15983",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_to_sql(df_train_smote, \"df_train_smote\",if_exists='replace', primary_index=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d366f91c-e2d1-493c-bc6b-22b20cafa656",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_train_SMOTE = DataFrame(\"df_train_smote\")\n",
    "DF_train_SMOTE.select([\"row_id\",\"classlabel\"]).groupby(\"classlabel\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e27432-4ab2-4906-a32e-d0a02aef2ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = ADASYN(sampling_strategy=20000.0/80000.0, random_state=42)\n",
    "X_res, y_res = sm.fit_resample(df_train[all_features], df_train[target])\n",
    "\n",
    "df_train_adasyn = pd.DataFrame(X_res, columns=all_features)\n",
    "df_train_adasyn[target] = y_res\n",
    "df_train_adasyn = df_train_adasyn.reset_index().rename(columns={'index': key})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b294acc-2241-4431-8d42-eb487f168088",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_to_sql(df_train_adasyn, \"df_train_adasyn\",if_exists='replace', primary_index=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cae3a82-2225-45aa-b00b-4691a75dcd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_train_ADASYN = DataFrame(\"df_train_adasyn\")\n",
    "DF_train_ADASYN.select([\"row_id\",\"classlabel\"]).groupby(\"classlabel\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7266a72-6418-442f-91cb-0fa601191e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLM_SMOTE_obj = GLM(data = DF_train_SMOTE, \n",
    "                   input_columns=all_features,\n",
    "                   response_column = target,\n",
    "                   family = \"BINOMIAL\",\n",
    "                   volatile = True)\n",
    "\n",
    "GLM_ADASYN_obj = GLM(data = DF_train_ADASYN, \n",
    "                   input_columns=all_features,\n",
    "                   response_column = target,\n",
    "                   family = \"BINOMIAL\",\n",
    "                   volatile = True)\n",
    "\n",
    "DF_pred_SMOTE = TDGLMPredict(\n",
    "        object=GLM_SMOTE_obj, \n",
    "        newdata=DF_test,\n",
    "        id_column=key, \n",
    "        accumulate=[target], \n",
    "        output_prob=True,\n",
    "        output_responses=\"1\", \n",
    "        volatile = True).result\n",
    "tdimbl.save_roc(DF_pred_SMOTE, model_id=\"SMOTE_glm\", key=key, target=target )\n",
    "\n",
    "DF_pred_ADASYN = TDGLMPredict(\n",
    "        object=GLM_ADASYN_obj, \n",
    "        newdata=DF_test,\n",
    "        id_column=key, \n",
    "        accumulate=[target], \n",
    "        output_prob=True,\n",
    "        output_responses=\"1\", \n",
    "        volatile = True).result\n",
    "tdimbl.save_roc(DF_pred_ADASYN, model_id=\"ADASYN_glm\" , key=key, target=target )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890b3690-88b5-463b-9f84-6270ba6902ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a179318-9df8-4fb2-940e-7ac27d8146c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdimbl.plot_roc_curves(DF_roc, highlight_model= \"ADASYN_glm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ddfade-cc15-4a58-ad1d-c6c54a5a77f5",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Well, it looks like the results for **SMOTE** and **ADASYN** aren’t that different from plain  random undersampling. The AUC scores are nearly the same, and you can see from the ROC curves that they don't significantly outperform. This suggests that in this case, the more sophisticated sampling techniques didn’t provide any clear advantage. It goes to show that sometimes, keeping it simple works just as well, and the dataset characteristics really play a huge role in determining which method will shine.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Outlook: If you’re using a VantageCloud Lake environment, there’s a way to make things even more efficient by skipping the data movement between pandas and the database. The in-database <code>TD_SMOTE</code> function allows you to apply SMOTE directly within Vantage, saving time and streamlining the workflow. You can check out how it works in the <a href='https://docs.teradata.com/r/Teradata-VantageCloud-Lake/Analyzing-Your-Data/Analytics-Database-Analytic-Functions/Feature-Engineering-Transform-Functions/TD_SMOTE'>TD_SMOTE documentation.</a></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954552a4-918d-4b93-8fd2-93d2117e963f",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>8. Modeling with Adjusted Class Weights</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>We’ve already covered post-processing and data preprocessing techniques for handling imbalanced classes. Now, let’s move on to adjusting the models themselves, specifically for imbalanced learning. A simple and effective way to do this is by using algorithms that support class weights, so the model can put more focus on the minority class during training.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>When an algorithm has class weights, it tweaks the penalty for getting each class wrong. By giving more weight to the minority class, the model learns to pay closer attention to those cases, boosting its accuracy on those tougher instances.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Teradata supports class-weighted models using:</p>\n",
    "<li style = 'font-size:16px;font-family:Arial'><code>SVM (Support Vector Machine)</code>: Teradata’s SVM can use class weights to help set better decision boundaries when the classes aren’t balanced. You can find more details <a href='https://docs.teradata.com/r/Enterprise_IntelliFlex_VMware/Database-Analytic-Functions/Model-Training-Functions/TD_SVM/TD_SVM-Syntax'>here</a></li>\n",
    "<li style = 'font-size:16px;font-family:Arial'><code>GLM (Generalized Linear Model)</code>: Teradata’s GLM also allows class weights, making it simple to adjust the model to focus more on the minority class. The <a href='https://docs.teradata.com/r/Enterprise_IntelliFlex_VMware/Database-Analytic-Functions/Model-Training-Functions/TD_GLM/TD_GLM-Syntax'>documentation</a> has more info.</li></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Since our synthetic dataset is already standardized and centered, we won’t need to do any extra preprocessing. We can go straight into applying the algorithms with the adjusted class weights. Finding the best weights can be tricky, so we’ll use an in-database grid search, another feature from ClearScape, to test different weight setups. This lets us efficiently find the best configuration without having to move any data out of Vantage. The code below sets up the grid and runs the search entirely in the database.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a0fd41-415a-44aa-9fea-e39a4330e32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"input_columns\":all_features, \n",
    "    \"response_column\":target, \n",
    "    \"model_type\":'Classification', \n",
    "    \"class_weights\":('0:0.5, 1:1.0',\n",
    "                  '0:0.2, 1:1.0',\n",
    "                   '0:0.1, 1:1.0',\n",
    "                   '0:0.05, 1:1.0',\n",
    "                  ), \n",
    "}\n",
    "eval_params = {\"id_column\": key,\n",
    "               \"accumulate\": target}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194eb220-ac13-43e0-a9ec-ddf75f87ce37",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_obj = GridSearch(func=SVM, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e349a02-14d1-4570-839b-9cae474566cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_obj.fit(data=DF_train, **eval_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995d2252-3680-4c13-a0a4-9aabd8d8c958",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_obj.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6409c0-90bc-493d-bee5-fc1d238b8a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_obj.model_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409b0217-25a0-446a-8719-6b68c7b959a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = list(gs_obj.model_stats.MODEL_ID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdffd3a-7bb6-4222-b937-e796da43c6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    db_drop_table(\"svm_grid_predictions\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684a7297-8beb-47a8-88b8-b159d85df653",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_id in all_models:\n",
    "    gs_obj.set_model(model_id)\n",
    "    (gs_obj\n",
    "         .predict(newdata=DF_test, **eval_params, \n",
    "                   output_prob=True, output_responses =\"1\")\n",
    "         .result\n",
    "         .assign(model_id = model_id)\n",
    "         .to_sql(\"svm_grid_predictions\", if_exists=\"append\")   \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb251e92-4f57-49c4-894a-16a869c807e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DF_pred_gridSVM = DataFrame(\"svm_grid_predictions\")\n",
    "DF_pred_gridSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24c6fdb-a05f-465a-ac1f-193344e73f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_id in all_models:\n",
    "    DF_ = DF_pred_gridSVM.loc[DF_pred_gridSVM.model_id == model_id]\n",
    "    tdimbl.save_roc(DF_, model_id=model_id , key=key, target=target )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969f030f-cc30-497e-b636-0345b7575da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdimbl.plot_roc_curves(DF_roc, all_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8d4293-a0be-447e-a754-b5724b9d93f9",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The ROC curves for the SVM models show no real difference, with all of them performing at almost the same level as random guessing. This tells us that the SVM didn’t manage to effectively separate the classes in this case.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>This result isn’t too surprising, though. SVMs usually work best when the classes are clearly separated or have distinct boundaries. But when the data points overlap a lot or exist in a continuous space—like we have here—SVMs often struggle to draw a clear line. This outcome suggests that the dataset’s characteristics might just not be a good fit for this algorithm. It’s a reminder that not every model works for every kind of data, so it’s important to choose one that matches the data’s distribution for the best results.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72906122-abc6-4094-b1ac-0c9d3d8712b7",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>9. Modelling with Ensembles that come with Embedded Rebalancing</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Our next method uses ensemble algorithms that are designed to handle imbalanced classes automatically. The <code>imbalanced-learn</code> Python package, which is built on top of scikit-learn, has several powerful ensemble methods just for this. We’ll give these algorithms a try by training the models locally in Python, then deploying them back in Vantage using the Bring Your Own Model (BYOM) setup. This way, we get the best of both worlds—using advanced models externally while still benefiting from Vantage’s capabilities.</p>\n",
    "\n",
    "For those interested in learning more about the BYOM workflow, you can explore this [notebook](https://github.com/martinhillebrand/ClearscapeCookbook/blob/main/03_sklearn_ONNX/BYOM_recipe_sklearn_ONNX_tdprepview.ipynb) or review the [official documentation](https://docs.teradata.com/r/Enterprise_IntelliFlex_Lake_VMware/Teradata-VantageTM-Bring-Your-Own-Model-User-Guide/Welcome-to-Bring-Your-Own-Model).\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Before we get into the deployment process, let’s take a look at the ensemble algorithms available in <code>imbalanced-learn</code>:</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'><b>Bagging Algorithms:</b></p>\n",
    "    <li style = 'font-size:16px;font-family:Arial'><code>BalancedRandomForestClassifier</code>: A version of random forest that balances classes during the tree-building phase.</li></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'><b>Boosting Algorithms:</b></p>\n",
    "    <li style = 'font-size:16px;font-family:Arial'><code>RUSBoostClassifier</code>: A twist on AdaBoost that adds random under-sampling (RUS) to the boosting process.</li></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>These algorithms work to balance the classes right during the training phase, ensuring that each subset of data is balanced and doesn’t let one class dominate. This helps build models that are more robust and well-suited for handling imbalanced data. In the next steps, we’ll train these models, evaluate how they perform, and show how to deploy them using Vantage.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f783a26d-d01a-41c1-b929-2d4bc7183ec3",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>9.1 Models Training</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>We start by loading the training dataset <code>DF_train</code> into a local pandas DataFrame. Next, we use the <code>imbalanced-learn</code> package to set up some ensemble algorithms that are built to work well with imbalanced data.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2caea9-9096-4629-9223-12e7f59c88cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = DF_train.to_pandas(all_rows=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f64df87-fdb8-4b5c-846e-52f79ea6dd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.ensemble import (\n",
    "    BalancedRandomForestClassifier, \n",
    "    RUSBoostClassifier)\n",
    "\n",
    "model_rusboost = RUSBoostClassifier(algorithm='SAMME',random_state=42)\n",
    "\n",
    "model_balancedrandomforest = BalancedRandomForestClassifier(random_state=42)\n",
    "\n",
    "\n",
    "model_rusboost.fit(df_train[all_features], df_train[target])\n",
    "\n",
    "model_balancedrandomforest.fit(df_train[all_features], df_train[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b1b94d-80f3-4bf3-8271-1738ee2d8826",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>9.2 imblearn to sklearn models</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>To deploy our models with BYOM, we need to convert them to ONNX format. The catch? The ONNX conversion tools only work with standard scikit-learn models—not directly with the <code>imbalanced-learn<code> classifiers.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>But no worries, we’ve got a simple workaround. We \"rewrap\" the fitted `imbalanced-learn` models into their equivalent scikit-learn versions. Since these models are basically built off scikit-learn, swapping them like this keeps their prediction behavior the same while making them ONNX-friendly. The key here is that <code>imbalanced-learn</code> only changes the training process—when it comes to prediction, they work just like regular scikit-learn models. So, this trick lets us convert them without any fuss.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Here’s how the conversions look:</p>\n",
    "<li style = 'font-size:16px;font-family:Arial'>RUSBoostClassifier → sklearn.ensemble.AdaBoostClassifier</li>\n",
    "<li style = 'font-size:16px;font-family:Arial'>BalancedRandomForestClassifier → sklearn.ensemble.RandomForestClassifier</li><p></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Once we’ve done the swap, we test <code>predict_proba()</code> on both versions to make sure they give the same results. This way, they’re all set for ONNX export and ready to deploy in Vantage. If you’re curious about how this works, check out the code in <code>tdimbalancedlearn.py</code>.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcf9bd6-d907-4714-883e-ec346d5b4a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\"rusboost\":{\"imblearnmodel\":model_rusboost},\n",
    "              \"balancedrandomforest\":{\"imblearnmodel\":model_balancedrandomforest},\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e717e2-7fb8-4736-80f0-7d2269c45286",
   "metadata": {},
   "outputs": [],
   "source": [
    "for modelid  in model_dict.keys():\n",
    "    print(modelid)\n",
    "    imblearn_model =  model_dict[modelid][\"imblearnmodel\"]\n",
    "    new_sklearn_model = tdimbl.convert_imblearn2sklearn(imblearn_model)\n",
    "    \n",
    "    same_predictions = tdimbl.compare_predictproba(df_train[all_features],\n",
    "           imblearn_model, new_sklearn_model)\n",
    "    assert same_predictions\n",
    "    model_dict[modelid][\"sklearnmodel\"] = new_sklearn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718f0e89-f72e-4fd9-abd2-6d4413427478",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b76cba-153d-4aae-9f56-c86b4863da06",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>9.3 ONNX Conversion</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Next up, we need to convert our scikit-learn models into ONNX format to get them ready for deployment. ONNX (Open Neural Network Exchange) is an open format that makes it easy to use models across different machine learning platforms, which is perfect for deploying them inside Teradata Vantage. If you want to dig into the details of the conversion process, you can check out more info <a href='https://onnx.ai/sklearn-onnx/'>here</a></p>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3b86d7-d8d1-44a3-8be1-e1985e897c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skl2onnx import convert_sklearn, to_onnx\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "\n",
    "TARGET_OPSET = 12 \n",
    "\n",
    "# Define the input type based on the number of features in your dataset\n",
    "n_features = len(all_features)\n",
    "n_features\n",
    "initial_type = [('float_input', FloatTensorType([None, n_features]))]\n",
    "initial_type\n",
    "\n",
    "for modelid  in model_dict.keys():\n",
    "    print(modelid)\n",
    "    sklearn_model = model_dict[modelid][\"sklearnmodel\"]\n",
    "    onnx_model = convert_sklearn(\n",
    "        sklearn_model, \n",
    "        modelid,\n",
    "        initial_types=initial_type,\n",
    "        target_opset=TARGET_OPSET)\n",
    "\n",
    "    model_dict[modelid][\"onnxmodel\"] = onnx_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404d4f69-ec08-4dca-8401-0242f5a45bb3",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>9.4 Local ONNX Test</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>To check that the ONNX models work as they should, we run them through an <code>onnxruntime</code> session to see if they produce the same predictions as the scikit-learn ones. This step ensures the conversion was successful and everything is functioning properly before deploying to Vantage. The tests show consistent results, so we’re confident the models are solid.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ff1db9-5cf0-47a3-9387-6467cfd4d542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from onnxruntime import InferenceSession\n",
    "\n",
    "for modelid  in model_dict.keys():\n",
    "    print(modelid)\n",
    "    onnx_model = model_dict[modelid][\"onnxmodel\"]\n",
    "    sess = InferenceSession(\n",
    "        onnx_model.SerializeToString(), providers=[\"CPUExecutionProvider\"]\n",
    "    )\n",
    "    input_name = sess.get_inputs()[0].name\n",
    "    label_name = sess.get_outputs()[1].name # proba\n",
    "    pred_onx = sess.run(\n",
    "        [label_name], \n",
    "        {input_name: df_train[all_features].head(1).astype(np.float32).values})[0]\n",
    "    \n",
    "    model_dict[modelid][\"model_pred\"] = pred_onx\n",
    "    \n",
    "\n",
    "for modelid  in model_dict.keys():\n",
    "    print(modelid)\n",
    "    print(model_dict[modelid][\"imblearnmodel\"].predict_proba(df_train[all_features].head(1)))\n",
    "    print(model_dict[modelid][\"model_pred\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c656f289-74ea-4607-b0b3-41267dfea03c",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>9.5 Save to Files, Upload to Vantage, Predict & Evaluate</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>The ONNX models are saved to files and uploaded into Vantage. From there, we use <code>ONNXPredict</code> for in-database predictions and evaluate how the models perform by looking at the ROC chart.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb1d03e-cc90-42e7-978a-55526ae1bd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this needs to set, so that Vantage can find the Scoring functions. Very often it is \"mldb\"\n",
    "configure.byom_install_location = \"mldb\"\n",
    "\n",
    "for modelid  in model_dict.keys():\n",
    "    with open(f\"{modelid}.onnx\", \"wb\") as f:\n",
    "        f.write(model_dict[modelid][\"onnxmodel\"].SerializeToString())\n",
    "\n",
    "try:\n",
    "    db_drop_table(\"model_repo\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for modelid  in model_dict.keys():\n",
    "    save_byom(\n",
    "        modelid,\n",
    "        f\"{modelid}.onnx\",\n",
    "        table_name=\"model_repo\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f6b769-ef0a-4960-9ee6-354c96509b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_test.to_sql(\"df_test\", if_exists=\"replace\")\n",
    "DF_test = DataFrame(\"df_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3ee280-9b8a-41da-b2f6-5a5940fe355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import literal_column\n",
    "for modelid  in model_dict.keys():\n",
    "    print(modelid)\n",
    "    myonnxmodel = retrieve_byom(modelid,\"model_repo\")\n",
    "    DF_pred = ONNXPredict(\n",
    "        newdata=DF_test,\n",
    "        modeldata=myonnxmodel, \n",
    "        accumulate=[key, target], \n",
    "        overwrite_cached_models = \"true\",\n",
    "        model_input_fields_map=\"float_input=\"+\",\".join(all_features),\n",
    "        volatile = True\n",
    "    ).result\n",
    "    \n",
    "    DF_pred_onnx = DF_pred.assign(\n",
    "        json_report_json =  literal_column(f\"NEW JSON (json_report)\"),\n",
    "        prob_distribution = literal_column(f\"json_report_json.JSONExtract('$.output_probability[0].value')\"),\n",
    "        prob_1 =  literal_column(f\"CAST(prob_distribution.JSONExtractValue('$[0].1') AS FLOAT)\"), \n",
    "    )\n",
    "    \n",
    "    tdimbl.save_roc(DF_pred_onnx, model_id=modelid , key=key, target=target )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc45d8e-3c0e-4579-96fa-809b8f59a49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_auc.sort(\"AUC\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6624b9-845c-4094-99b1-1fdd8c6f65fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdimbl.plot_roc_curves(DF_roc, model_list = list(model_dict.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0a13d1-0dec-4b23-b77e-2b18177d70b0",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Now, it’s time to compare the models, and the results speak for themselves. The ensemble models really shine, with <code>BalancedRandomForest</code> leading the way with the highest AUC scores. This shows just how effective specialized algorithms can be for handling imbalanced data. In a real-world setting, these kinds of improvements could mean the right customers get the credit they need, while the bank reduces losses and boosts profits—a win-win for everyone.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f72c0d-cbbe-4264-83e2-695bb832e602",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>10. Reframe as Anomaly Detection</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Sometimes, it makes more sense to treat an imbalanced classification problem as an anomaly detection task. This works well when you don’t have any known outliers and assume that all the existing data shows \"normal\" behavior. The idea is to train the model using only the majority class, ignoring the minority class, and then flag anything that looks different as an anomaly. We’ll try two methods here: an in-database One-Class SVM and an open-source approach with Isolation Forest, which we later deploy using ONNX and BYOM.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>The <code>One-Class SVM</code> tries to create a boundary that captures most of the \"normal\" data, marking anything outside this boundary as an anomaly. Even though SVM methods haven’t worked great on this dataset so far, it’s still worth testing it in this anomaly detection setup.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>The <code>Isolation Forest</code> is a tree-based method that splits features randomly to isolate data points. It’s great at spotting anomalies because it identifies them early on, making it effective even with imbalanced datasets like ours. This method works well when the minority class has very different behavior from the majority.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>By switching to an anomaly detection approach, we can use algorithms designed to spot unusual patterns, which might be more effective at catching rare events like credit defaults when traditional classification methods don’t quite cut it.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd473d67-2178-4752-9b7b-d81703b97cff",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>10.1 In-DB OneClassSVM</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>The OneClassSVM is trained directly in the database, using only the majority class data. We then evaluate the model with the ROC curve to see how well it detects anomalies, treating the minority class instances as outliers or deviations from the norm.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396dc21e-2f67-4960-9769-41be715eabcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model fitting\n",
    "svm_obj = OneClassSVM(\n",
    "    data=DF_train.loc[DF_train[target]==0],     \n",
    "    input_columns=all_features, \n",
    "    volatile=True\n",
    ")\n",
    "\n",
    "# predictions\n",
    "svm_pred_obj = OneClassSVMPredict(\n",
    "    object=svm_obj, \n",
    "    newdata=DF_test,\n",
    "    id_column=key,\n",
    "    accumulate=[target], output_prob=True,\n",
    "    output_responses= [\"0\",\"1\"],\n",
    "   volatile=True\n",
    ")\n",
    "\n",
    "DF_pred_svm = (svm_pred_obj.result\n",
    " .drop(columns = [\"prediction\", \"prob_1\" ])\n",
    " .assign(prob_1 = svm_pred_obj.result.prob_0)\n",
    " .drop(columns = [ \"prob_0\" ]))\n",
    "\n",
    "tdimbl.save_roc(DF_pred_svm, model_id=\"oneclass_svm\" , key=key, target=target )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848bd7b7-f3c2-476a-a199-ec3236e93b51",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>10.2 IsolationForest with BYOM</b></p>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'>We train the Isolation Forest model using scikit-learn and then deploy it with BYOM. Unlike regular classifiers, the Isolation Forest gives an anomaly score instead of a probability. Negative scores mean outliers (anomalies), and positive scores mean normal instances.</p>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'>To turn these scores into something usable for setting thresholds, we apply a log transformation to scale them between 0 and 1. While these values aren’t true probabilities, they still work well for setting thresholds that make sense for business decisions.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d80136-70e7-4dab-8ce3-fb73ad6d46dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "modelid = \"isolation_forest\"\n",
    "# Filter majority class (0) for training the Isolation Forest\n",
    "df_majority = df_train[df_train[target] == 0]\n",
    "\n",
    "# Train Isolation Forest only on majority class\n",
    "iso_forest = IsolationForest(contamination=0.02, random_state=42)\n",
    "iso_forest.fit(df_majority[all_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0466b4c6-767c-4300-84df-8e4d4b609529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skl2onnx import to_onnx\n",
    "\n",
    "\n",
    "\n",
    "iso_forest_onnx = to_onnx(\n",
    "        iso_forest, \n",
    "        name= modelid,\n",
    "        initial_types=initial_type,\n",
    "        target_opset={\"\": TARGET_OPSET, \"ai.onnx.ml\": 3}\n",
    ")\n",
    "\n",
    "with open(f\"{modelid}.onnx\", \"wb\") as f:\n",
    "        f.write(iso_forest_onnx.SerializeToString())\n",
    "\n",
    "save_byom(\n",
    "    modelid,\n",
    "    f\"{modelid}.onnx\",\n",
    "    table_name=\"model_repo\"\n",
    ")\n",
    "\n",
    "myonnxmodel = retrieve_byom(modelid,\"model_repo\")\n",
    "\n",
    "DF_pred = ONNXPredict(\n",
    "    newdata=DF_test,\n",
    "    modeldata=myonnxmodel, \n",
    "    accumulate=[key, target], \n",
    "    overwrite_cached_models = \"true\",\n",
    "    model_input_fields_map=\"float_input=\"+\",\".join(all_features),\n",
    "    volatile = True\n",
    ").result.assign(\n",
    "    json_report_json = literal_column(f\"NEW JSON (json_report)\"),\n",
    "    prob_distribution =  literal_column(f\"json_report_json.JSONExtract('$.scores[0]')\"),\n",
    "    anomaly_score =  literal_column(f\"CAST(prob_distribution.JSONExtractValue('$[0].0') AS FLOAT)\"),   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c92214-a3b3-4d6c-83f3-1a3fa05d707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# translating anomaly_score to a probabilty\n",
    "DF_pred_onnx = DF_pred.assign(\n",
    " prob_1 = literal_column(f\"1- 1 / (1 + EXP(-10 * (anomaly_score)))\"), \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2717603-b729-4cc6-b1fb-5ee11825a397",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdimbl.save_roc(DF_pred_onnx, model_id=modelid , key=key, target=target )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005244a2-23e4-41e4-8f1b-ba603ac2d790",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>10.3 Compare Model Performance</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9dc594-b1e5-4687-8492-102a38b36249",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_auc.loc[DF_auc.model_id.isin([\"isolation_forest\",\"oneclass_svm\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869fa029-74d9-4186-bc17-60f2f519a1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdimbl.plot_roc_curves(DF_roc, model_list = [\"isolation_forest\",\"oneclass_svm\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f1d560-5131-4e97-818b-25506b6cbbad",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>In this case, Isolation Forest didn’t quite work, but that doesn’t mean it’s not worth trying in other scenarios. It’s great when you’re dealing with anomalies that are rare, clearly separated, and structurally different from the norm—like detecting network intrusions, spotting fraud, or identifying rare equipment breakdowns. Isolation Forest is built for finding these rare, isolated cases (hence the name) by splitting the data with random cuts. It works best when anomalies are grouped in distinct clusters or stand apart in simpler spaces. However, based on our EDA, we saw that the classes aren’t very separable, with Cohen’s d only reaching 0.5 in the best features, which likely explains why it didn’t do well here. In the end, the data itself will point you toward the right approach.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ececce0-57a6-4f3b-9bb5-4dda8c479c82",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>11 Comparison of All Approaches</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Time for the showdown! We’ve reached the final stage of our tests, where we see how all the models compare when dealing with the imbalanced dataset. Below is the table showing AUC and Gini scores for each one. The results are clear—ensemble methods designed for imbalanced data, like <code>BalancedRandomForest</code> and <code>BalancedBagging</code>, come out on top with the best AUC and Gini scores. This suggests they’re the most effective at telling the classes apart.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>In contrast, resampling methods like undersampling, SMOTE and ADASYN with GLM offer some improvement but don’t reach the same level as the specialized ensembles. This indicates that while resampling is an easy way to balance the classes, it might not be enough for more complex decision boundaries.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>The <code>OneClass SVM</code> and <code>Isolation Forest</code>, which we framed as anomaly detection models, didn’t perform well here. The low AUC scores show that these methods might not be the best choice when both classes are present but have different distributions.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Lastly, the <code>SVMs</code> with class weights didn’t perform well either, which isn’t surprising since the data didn't offer an easy way to split the classes.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48146b4-917d-4aa6-af3d-f9c1fc8f65d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_auc.to_pandas().sort_values(\"AUC\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9931ed-e9de-4f65-8ed4-a33dee5e4651",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>Conclusion</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>We’ve covered a lot of ground in this post, exploring different strategies for handling imbalanced classification—from simple sampling techniques and quick post-processing tricks to more advanced ensemble methods and anomaly detection. The key takeaway? It’s all about the data—knowing its patterns, behavior, and how your model’s predictions impact the business. But staying practical is crucial: while advanced methods can definitely improve things, sometimes the simpler approach works just as well with way less effort. It’s all about finding that sweet spot between effort and results to build models that deliver real value.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ddb562-b704-4c10-bf14-02ee9878146f",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>8. Cleanup</b></p>\n",
    "<p style = 'font-size:18px;font-family:Arial;color:##00233C'><b>Work Tables</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cd15a8-62fa-4a61-be58-108ce23bf659",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_tobedeleted =['model_repo',\n",
    " 'oversampled',\n",
    " 'oversampled_train',\n",
    " 'resampled_train',\n",
    " 'resampled_train',\n",
    " 'df_train_smote',\n",
    " 'df_train_adasyn',\n",
    " 'sampling_predictions',\n",
    " 'svm_grid_predictions',\n",
    " 'undersampled',\n",
    " 'undersampled_train',\n",
    " 'df_test',\n",
    " 'eval_auc',\n",
    " 'eval_confusion',\n",
    " 'eval_metrics',\n",
    " 'eval_roc',\n",
    " 'iris_enc_sample']\n",
    "for t in tables_tobedeleted:\n",
    "    try:\n",
    "        db_drop_table(t)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c8689d-0e69-4499-89b1-01975ca23cb2",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial'><b>Databases and Tables</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The following code will clean up tables and databases created above.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07449c80-70f9-4942-a1af-5b9594817bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../../UseCases/run_procedure.py \"call remove_data('DEMO_DataImbalance');\" \n",
    "#Takes 40 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e267c9-0af2-4913-bc56-4ece73891c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bcc3cc-6f2d-4f48-9c4b-93835d19ddc5",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>High-Effort Approaches: What’s Next?</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>In this notebook, we covered a bunch of methods for dealing with imbalanced data, focusing on techniques that are easy to implement and deploy in real-world scenarios. But as our initial diagram showed, there are some more advanced approaches out there that, while they need more time and resources, can be super effective for the right use cases. These methods are complex, but here’s a quick overview of these high-effort strategies and when they might be worth diving into.</p>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>Advanced Feature Engineering</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Feature engineering is a game-changer, especially with tricky data like time series or mixed data types. If you’re working with time series data, transforming it into the frequency domain (think Fourier or Wavelet Transforms) can reveal hidden patterns you wouldn’t spot otherwise. These features can seriously boost the model’s ability to tell the classes apart. But heads up—advanced feature engineering needs a lot of domain knowledge and experimenting, so it’s resource-heavy and might require teaming up with domain experts.</p>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>Advanced EDA on Unused Data Sources</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Exploring and using unused or unstructured data sources can totally change the game when detecting rare events. There might be logs, text reports, or sensor data just waiting to be tapped into. Doing some deep-dive EDA on these sources can uncover new patterns that could boost your model’s performance. For example, text mining or NLP techniques could help you extract useful insights from reports. But integrating these sources means investing in data cleaning, processing, and possibly building new ways to capture and handle the data—definitely not a quick fix.</p>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>New Data Sources: Stepping Up Data Collection</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Sometimes, the data we’ve got just doesn’t cut it. In manufacturing, for instance, adding new sensors or cameras to the production line can give a fuller view of the process, helping spot product defects you’d otherwise miss. But bringing in new data sources often comes with big costs—new hardware, software, and a solid pipeline to manage all that new info. It’s worth it if accuracy is a top priority, but it’s not a small investment.</p>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>Advanced Data Synthesis</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>If we can’t fix data collection issues or class imbalance through traditional methods, advanced data synthesis might be the way to go. Techniques like GANs (using packages like `ctgan`) can generate synthetic data points that mimic the minority class, bulking up your dataset without needing real-world observations. It’s powerful but tricky—making sure the synthetic data is realistic and unbiased takes some serious work. Plus, it needs a lot of computing power and expertise, so it’s not for quick, easy wins.</p>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>Custom Cost Function Integration</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Another route is to create custom cost functions that fit your business needs and build them right into your model. This let's us optimize for what really matters to your business, rather than generic metrics like accuracy or AUC. For example, if false negatives are a huge issue, a custom loss function can make the model penalize those mistakes more heavily. But this approach needs a deep understanding of the business and often requires adjusting algorithms, making it a pretty technical and time-consuming solution.</p>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>Active Learning</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>When labeled data is scarce or expensive, active learning can be a lifesaver. It’s a smart approach where the model actively picks the most useful samples to label, improving performance with minimal data. It’s great for fields like medical diagnostics or legal review, where labeling can get pricey. But active learning isn’t simple—it needs iterative model training, human-in-the-loop interactions, and clever sampling strategies, which all add up to a lot of time and effort.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0a715d-29e2-4b68-afd9-0011fdfedaa4",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'><b>Links:</b></p>\n",
    "<ul style = 'font-size:16px;font-family:Arial'>\n",
    "    <li>Medium Blog Posts: <a href = 'https://medium.com/teradata'>here</a></li>\n",
    "    \n",
    "    \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77e30ce-920a-4b36-ab36-1cbe54eda37a",
   "metadata": {},
   "source": [
    "<footer style=\"padding-bottom:35px; border-bottom:3px solid #91A0Ab\">\n",
    "    <div style=\"float:left;margin-top:14px\">ClearScape Analytics™</div>\n",
    "    <div style=\"float:right;\">\n",
    "        <div style=\"float:left; margin-top:14px\">\n",
    "            Copyright © Teradata Corporation - 2025. All Rights Reserved\n",
    "        </div>\n",
    "    </div>\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
