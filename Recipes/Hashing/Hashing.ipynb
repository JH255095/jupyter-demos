{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "566f2242-7fa7-4bf5-be69-0269f6010913",
   "metadata": {},
   "source": [
    "<header>\n",
    "   <p  style='font-size:36px;font-family:Arial; color:#F0F0F0; background-color: #00233c; padding-left: 20pt; padding-top: 20pt;padding-bottom: 10pt; padding-right: 20pt;'>\n",
    "       Hashing your data\n",
    " <br>       \n",
    "       <img id=\"teradata-logo\" src=\"https://storage.googleapis.com/clearscape_analytics_demo_data/DEMO_Logo/teradata.svg\" alt=\"Teradata\" style=\"width: 150px; height: auto; margin-top: 20pt;\">\n",
    "  <br>\n",
    "    </p>\n",
    "</header>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b463c94-9b39-4865-b9b3-87cce3ded8f6",
   "metadata": {},
   "source": [
    "<p style = 'font-size:20px;font-family:Arial'><b>Introduction</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>A hash function is a special mathematical algorithm that takes input data of any size and produces a fixed-size string of characters, which typically looks like a random sequence of letters and numbers. Think of it as a unique digital fingerprint for the data. No matter how large or small the input is, the hash function generates a fixed-length output. For example, whether you're hashing a single word like \"hello\" or an entire book, the output (hash) will be of a consistent length.\n",
    "<br>\n",
    "Hashing lies at the heart of Teradata technology, particularly its capacity for massive parallel processing (MPP). The <a href='https://www.teradata.com/resources/white-papers/born-to-be-parallel-and-beyond'>documentation</a> has more info. This capability hinges on efficient data access and retrieval, powered by a robust hashing function. While the mechanics of hashing might remain behind the scenes for most users, gaining an understanding of how it works can be incredibly beneficial. \n",
    "Some key properties of the Teradata hash function are:\n",
    "    <ul style = 'font-size:16px;font-family:Arial'>\n",
    "        <li><b>Deterministic</b>: The same input will always produce the same output.</li>\n",
    "        <li><b>Fast computation</b>: It's quick to calculate the hash for any given data, hence insertion/ reading will be fast</li>\n",
    "        <li><b>Non-invertible</b>: It's practically impossible to reverse the process, meaning you can't easily figure out the original input from the hash output.</li>\n",
    "        <li><b>Collision-resistant</b>: It's extremely unlikely (though not impossible) that two different inputs will produce the same output hash. This depends on the length of the output token. When converted to an integer, the results from the HASHROW function can have over 4 billion different codes, 4,294,967,295 hash codes to be precise</li>\n",
    "        <li><b>Uniform</b>: When your input unique, such as a primary key, the output will be uniform, once you process it further with the modulo operator.</li>\n",
    "        </ul>\n",
    "<p style = 'font-size:16px;font-family:Arial'>        \n",
    " This notebook demonstrate  four use cases on how hashing can be a game-changer in the workflow of a data scientist. These four use cases are:\n",
    "    <ol style = 'font-size:16px;font-family:Arial'>\n",
    "        <li>Pseudonymize a categorical feature</li>\n",
    "        <li>Splitting data into random subsets for train, evaluate and test</li>\n",
    "        <li>Encode a categorical feature with unknown number of values in buckets</li>\n",
    "        <li>Encode a categorical feature with known number of values without collisions</li>\n",
    "       </ol>\n",
    "       </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e235b158-ac6d-4d10-86df-11d64d81f189",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "\n",
    "<p style = 'font-size:20px;font-family:Arial;'><b>1. Connect to Vantage</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;'>In the section, we import the required libraries and set environment variables and environment paths (if required).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02f957a-e16a-4652-921d-875c1879f5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import getpass\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from teradataml import *\n",
    "display.max_rows = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7be28b0-8258-498d-8f27-e4ee2a00ee75",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;'>We will be prompted to provide the password. We will enter the password, press the Enter key, and then use the down arrow to go to the next cell. Begin running steps with Shift + Enter keys.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933e5f21-ad89-4742-a8dc-1fef456973d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../../UseCases/startup.ipynb\n",
    "eng = create_context(host = 'host.docker.internal', username = 'demo_user', password = password)\n",
    "print(eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc921395-ecf3-4be6-b4e3-af3512faabba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "execute_sql('''SET query_band='DEMO=PP_Recipe_Hashing.ipynb;' UPDATE FOR SESSION; ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01e6d8c-1573-4cab-8ac9-16bc905f8107",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial;'><b>2. Getting Data for This Demo</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;'>We have provided data for this demo on cloud storage. We have the option of either running the demo using foreign tables to access the data without using any storage on our environment or downloading the data to local storage, which may yield somewhat faster execution. However, we need to consider available storage. There are two statements in the following cell, and one is commented out. We may switch which mode we choose by changing the comment string.</p>   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96fdd2a-c8d4-4bf9-86a8-1cbb72f3eeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run -i ../../UseCases/run_procedure.py \"call get_data('DEMO_Hashing_cloud');\"  # Takes about 20 secs\n",
    "%run -i ../../UseCases/run_procedure.py \"call get_data('DEMO_Hashing_local');\"  # Takes about 50 secs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b09609-b553-4706-9d54-7c602e513bcf",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;'>Next is an optional step – if you want to see status of databases/tables created and space used.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d24178-ef4a-43c2-8929-1f94129b0142",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../../UseCases/run_procedure.py \"call space_report();\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a960a6-589a-4d21-b46e-2571e03ef66d",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>3. Use Case 1: Hidden by Hash </b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>In exploring how hash functions benefit data science, we start with <b>anonymizing categorical variables</b>. This technique is essential for protecting data privacy. By using hash functions, we transform sensitive details into anonymized forms. This protects personal and confidential information while allowing us to still carry out meaningful data analysis.<br>Let's consider an example where we want to combine three categorical variables—relationship, race, and sex—into one anonymized variable using hash encryption.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c521e5ce-1ca8-4e77-8d38-c1267e391966",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = DataFrame(in_schema(\"DEMO_Hashing\",\"Census_Income\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051e9c3d-b3f6-4082-9602-d608f155c574",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import func as f\n",
    "DF_encrypted = (DF\n",
    "    .select([\"row_id\", \"relationship\", \"race\", \"sex\"])\n",
    "    .assign(demographic_encrypted = \n",
    "         f.abs(f.from_bytes(f.hashrow(\n",
    "                                 DF.relationship.expression, DF.race.expression, DF.sex.expression ), \n",
    "                            \"base10\" \n",
    "         ).cast(type_=INTEGER)\n",
    "              )))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63517967-9836-4402-ae06-7be9f8c0a7bf",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Breaking the code to understand each step:\n",
    "<ul style = 'font-size:16px;font-family:Arial'>\n",
    "    <li><b>hashrow</b>: This function taps into the Teradata Vantage's built-in hashing capability, taking specified columns as input and returning a hexadecimal value. <a href='https://docs.teradata.com/search/all?query=Hashrow&content-lang=en-US'>Teradata Documentation on HASHROW</a></li>\n",
    "    <li><b>from_bytes</b>: With the <code>base10</code> argument, this function converts the hexadecimal value into a numeric float value. <a href='https://docs.teradata.com/r/Enterprise_IntelliFlex_VMware/SQL-Data-Types-and-Literals/Data-Type-Conversion-Functions/FROM_BYTES'>Teradata Documentation on FROM_BYTES</a></li> \n",
    "    <li><b>abs</b>: This function is used to eliminate any negative sign that might appear in the process.\n",
    "        <a href='https://docs.teradata.com/r/Enterprise_IntelliFlex_VMware/SQL-Functions-Expressions-and-Predicates/Arithmetic-Trigonometric-Hyperbolic-Operators/Functions/ABS'>Teradata Documentation on ABS</a></li> \n",
    "    <li><b>cast</b>: This final step ensures the output is formatted as an <code>INTEGER</code>.\n",
    "        <a href='https://docs.teradata.com/r/Enterprise_IntelliFlex_VMware/SQL-Data-Types-and-Literals/Data-Type-Conversions'>Teradata Documentation on Data Type Conversions</a></li></ul>\n",
    "<p style = 'font-size:16px;font-family:Arial'>A handy tool in teradataml is the <code>show_query()</code> function. It can be attached to any DataFrame expression, allowing us to peek at the resulting SQL query. In our case, here's what it looks like:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e0ad2c-5b5c-457b-a93a-e68f19f0c348",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DF_encrypted.show_query())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008c1d16-2699-4162-8dad-2c97e3157b95",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>\n",
    "If the hashing process avoids any collisions, it creates a consistent mapping. This means if a row has the same values across the three selected columns for hashing, the resulting hashed value will be the same as well. The key benefit here is that hashing obscures the original clear text values, which might often be sufficient for privacy purposes. However, it's worth noting that if someone is familiar with the original categories' (multivariate) distributions, they could attempt to backtrack to the original values.<br>\n",
    "Moreover, should the model ever be exposed, it becomes ineffective without knowledge of the specific characteristics of the hash function used. This adds an extra layer of security, as the model's utility is closely tied to the unique properties of the hashing technique employed.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77408e21-2be4-44a7-89b4-f3ca28fcf7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_encrypted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1960ad6-e88a-4f04-9a0c-aded90daa053",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>4. Use Case 2: Precision in Partition </b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>As we move to our second exploration of hash functions in data science, we turn our attention to effectively dividing datasets into <b>training, validation, and test sets</b>. By applying a hash function to a unique primary key for this division, we achieve not only incredible efficiency but also a level of reproducibility and consistency that enhances data analysis projects. This technique smoothly generates distinct subsets of data. Thanks to the predictable behavior of hash functions, we can ensure that each piece of data consistently finds its way into the same subset, allowing for accurate comparisons and solid evaluations of model performance. Here's the breakdown:</p>\n",
    "<ol style = 'font-size:16px;font-family:Arial'>\n",
    " <li><b>Hashing Identifiers</b>: Start by calculating a hash value for each record's unique identifier. This could be a singular ID, a mix of different fields, or any attribute that uniquely defines a record.</li>\n",
    "    <li><b>Determining the Split</b>: Transform the hash value into a numerical range (for instance, by applying modulo 6 to the hash value). Then, assign the record to the training, evaluation, or test set based on its range. For instance:\n",
    "        <ul style = 'font-size:16px;font-family:Arial'>\n",
    "            <li>Assign records with a value of 0 to the test set (making up 16.7%).</li>\n",
    "            <li>Assign records with a value of 1 to the validation set (also 16.7%).</li>\n",
    "            <li>Assign records with values from 2 to 5 to the training set (comprising 66.7%).</li>\n",
    "        </ul>\n",
    "        </ol>\n",
    "        </p>\n",
    " <p style = 'font-size:16px;font-family:Arial'>  Now, applying this to the census dataset:\n",
    "    <ol style = 'font-size:16px;font-family:Arial'> \n",
    "        <li><b>Hashing Identifiers</b>: <code>row_id</code> serves as our primary key.</li>\n",
    "        <li><b>Determining the Split</b>: We'll allocate two-thirds of our data to training and one-sixth to both validation and testing. This involves taking the modulo 6 of our integer hash value to ensure even distribution.</li>\n",
    "        </ol>\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1f8013-fb75-4028-856b-fa249c83d06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_fold = DataFrame.from_query(\n",
    "\"\"\"\n",
    "SELECT\n",
    "    -- create 6 equally sized buckets\n",
    "    MOD(\n",
    "        ABS(CAST(from_bytes(hashrow(row_id), 'base10') AS INTEGER)), \n",
    "        6) as rowid_hashbin,\n",
    "    -- assign to folds as per requirement\n",
    "    CASE rowid_hashbin \n",
    "        WHEN 0 THEN 'test' \n",
    "        WHEN 1 THEN 'evaluate' \n",
    "        ELSE 'train'\n",
    "    END as fold,\n",
    "    t.*\n",
    "FROM\n",
    "     DEMO_Hashing.Census_Income t\n",
    "\"\"\")\n",
    "DF_fold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1e6161-33c5-48c4-809e-27f361e33ef7",
   "metadata": {},
   "source": [
    " <p style = 'font-size:16px;font-family:Arial'> \n",
    "Let's check to make sure our data splits are fair, meaning they don't have uneven distributions of the target labels. This step highlights the flexibility of teradataml, which seamlessly blends SQL and pandas-style syntax for an intuitive workflow. Given that our aggregated DataFrame has just 6 rows, we'll move it over to pandas for  visualization. This allows us to take a closer look and ensure our model training is based on balanced and unbiased data.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5484adaf-0ded-46c5-b635-84b617472a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_fold_counts = DF_fold.select([\"fold\",\"income\",\"row_id\"]).groupby([\"fold\",\"income\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6e4fd2-aa21-436f-8a42-a42021961c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "pddf_fold_counts_pd = DF_fold_counts.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8e2052-e53c-4b3e-947a-6d13086b426b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df = pddf_fold_counts_pd.pivot(index='fold', columns='income', values='count_row_id').fillna(0)\n",
    "ax = pivot_df.plot(kind='bar', stacked=True, figsize=(10, 6))\n",
    "for bar in ax.patches:\n",
    "    x = bar.get_x() + bar.get_width() / 2\n",
    "    y = bar.get_height()/2 + bar.get_y()\n",
    "    value = int(bar.get_height())\n",
    "    ax.text(x, y, str(value), ha='center', )#va='bottom')\n",
    "\n",
    "plt.title('Distribution of Income Groups by Fold')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Number of Rows')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Income')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adb0d4d-d39d-4786-b27e-e6c78ceb6447",
   "metadata": {},
   "source": [
    " <p style = 'font-size:16px;font-family:Arial'> \n",
    "The chart displayed paints a clear picture of our data split, confirming that we've met our goals for both the size of the splits and the evenness of the distribution. It shows the three subsets—train, test, and evaluate—each with a proportional mix of income categories, both <code><=50K and >50K</code>. The 'train' fold is the largest, as intended, with the 'test' and 'evaluate' folds being smaller yet similar in size to each other. The balance across these folds suggests that our hash function has done its job well, assigning data points to each subset in a way that mirrors the overall composition of our dataset. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b0b767-08b0-463c-9952-dd21e93a58ad",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>5. Use Case 3: Encode a categorical feature with unknown number of values in buckets </b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>\n",
    "    In our journey through the practical uses of hash functions in data science, let's look at a challenge that often comes up with categorical data, like the <b>native_country</b> column in the census income dataset. This column has 43 different countries, and sometimes new ones appear that weren't seen during the model's training phase.<br>Other common methods for dealing with categories, like label encoding and one-hot encoding, have some drawbacks. Label encoding requires a fixed-size lookup-table. One-hot encoding creates a new column for each category, which can make our dataset much bigger and harder to work with, especially when new categories show up.<br>\n",
    "Hashing provides a clever way around these issues. It lets us <b>put many categories into a smaller number of groups</b>, even if that means some different categories end up in the same group. This is okay because it keeps our dataset manageable and our models flexible, able to handle new categories without needing a complete overhaul. For example, countries like Italy, Peru, and Portugal might all end up in the same group, but this simplicity helps us keep our model running fast and smoothly. Let's see how using hashing this way can make our models more straightforward and ready for whatever new data comes their way.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74578ed1-8038-44d3-941e-051c8787a1e2",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>\n",
    "Our census income dataset contains some categorical variables, and the one that stands out as a candidate for feature is the native_country column. Currently there 43 distinct countries. In future during model deployment, there could be countries not seen during training, and the worst thing would be that our algorithm fails.<br>\n",
    "For a start, we accept colissions, and we would like to only use 10 buckets derived from hashing, leading to 4.3 countries per bucket on average<br>\n",
    "In practical situations, we'll likely need to apply hash-encoding to more than just a single variable. So, the next step is to craft code that can handle this efficiently. We've learned that to achieve our transformation, we need to link together several functions. Fortunately, we can embody the spirit of good software practice—specifically, the DRY principle (Don't Repeat Yourself)—by designing a function that generates these derived columns for us.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26eb4c8e-7adb-43bb-aebc-f95ded43f69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_hashbucket(thisDF, column_name, num_buckets=10):\n",
    "    return f.abs(f.from_bytes(f.hashrow(thisDF[column_name].expression), \"base10\" \n",
    "                             ).cast(type_=INTEGER)) % num_buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d915ff85-f8e1-4acd-89bd-131aa454ec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_encode = [\"relationship\", \"race\", \"sex\", \"native_country\"]\n",
    "my_kwargs = {(f\"{colname}_encoded\"):get_feature_hashbucket(DF,colname,10)\n",
    "                for colname in columns_to_encode}\n",
    "\n",
    "DF_hashbin = (DF\n",
    "    .select([\"row_id\"]+ columns_to_encode)\n",
    "    .assign(**my_kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e3d01b-3bbe-4e73-98c2-4338a00c009f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_hashbin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56e255e-8447-468f-b261-a01476d482a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DF_hashbin.show_query())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5e3396-bbb1-4c5a-b23e-a21424dcfb3c",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>\n",
    "The sample output doesn't reveal any collisions, but that might be due to certain values being more prevalent than others. To get a clearer view, we'll need to aggregate the table. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ea0415-2e97-41e2-a4ca-02de62db7e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_collisions = DataFrame.from_query(\n",
    "\"\"\"\n",
    "SELECT\n",
    "    native_country_hashbin,\n",
    "    COUNT ( native_country) no_countries_bin,\n",
    "    TRIM(TRAILING ' ' FROM (XMLAGG(TRIM(native_country)|| ','\n",
    "                           ORDER BY native_country) (VARCHAR(1000)))) as countries_list \n",
    "FROM (\n",
    "    SELECT \n",
    "        DISTINCT (native_country),\n",
    "        MOD(ABS(CAST(from_bytes(hashrow(native_country), 'base10') AS INTEGER)),10) as native_country_hashbin\n",
    "    FROM\n",
    "        DEMO_Hashing.Census_Income t\n",
    ") t\n",
    "GROUP BY native_country_hashbin\n",
    "\"\"\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab69b07a-1a23-472a-a43f-a9f6404ec457",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_collisions.sort(\"native_country_hashbin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dc87d4-6305-4abf-921a-42009a50dcb7",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>We've run into collisions, which isn't surprising. As we saw, Italy, Peru, and Portugal all share hashbucket number 2. Before we pivot to our next use case, let's address a significant point: the choice of how many hash buckets to use.<br>If you're working with a modest number of bins and categories, you'll probably want to examine any collisions to decide if they're acceptable. If they're not, consider increasing your bucket count. Whether this extra step is worth it depends on how much it could speed up data preparation against your specific use case needs.<br>\n",
    "Think of the number of hash buckets for each feature as a dial you can turn in your data science process—it's essentially a hyperparameter you can tune!<br>When it comes to best practices for setting the size of hash buckets, it's all about the context and balancing act between performance and computational demands. More buckets mean fewer collisions but a larger feature space, which can bump up memory and processing requirements. On the flip side, a hash space that's too snug could lead to collisions that mask important details. A good rule of thumb is to begin with a hash space around ten times the size of the number of unique values you expect in your variable. From there, you can tweak as needed, based on real-world results and the computational power at your disposal. The sweet spot for hash bucket size is where you minimize information loss from collisions without unnecessary growth in dimensionality.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8e0fec-cd16-49d5-8f1a-60b2db947416",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>6. Use Case 4: Encode a categorical feature with known number of values without collisions </b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>\n",
    "    We've already seen that finding the right way to turn categories into numbers that our models can understand can be a challenge. Label encoding and one-hot encoding are common choices, but they're not perfect. They can struggle with a lot of different categories, either by needing a big table to keep track of them all (label encoding) or by making our dataset huge with too many columns (one-hot encoding). Plus, they don't handle new, unseen categories very well.<br>The same applies to hash encoding too. However, sometimes we really need the best of both worlds: a way to encode features efficiently without mixing up the categories we already know.<br>\n",
    "We've mentioned a simple solution: use more buckets. But there's another clever technique by attaching extra text to our category values, we create a kind of chaos that changes how they're sorted into buckets. Why does this help? Because with the right amount of shuffling, we can avoid mixing up our known categories in the same bucket.<br>\n",
    "To get this right, we need to understand a bit of probability theory - don't worry, it's not as scary as it sounds. Think about the birthday paradox, which shows us how likely it is for people in a group to share a birthday. It's a bit like our categories and buckets: the chance of two categories ending up in the same bucket (a \"collision\") depends on how we shuffle them and how many buckets we have. With the right adjustments, we can keep our known categories from colliding, making our data easier to work with and our models more accurate. Let's explore how this technique can help us manage our categories more effectively, even when they're numerous or new ones show up.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0235c61-e336-4a30-bcfd-e9ae49deba13",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>First up, let's see how likely it is for categories to end up in the same bucket, a.k.a., a collision. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c2da9a-39e4-4dbc-bf56-253a9dc91e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import factorial\n",
    "\n",
    "def category_collision(num_buckets, num_categories):\n",
    "    if num_buckets < num_categories:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 1.0 - (factorial(num_buckets) / (factorial(num_buckets - num_categories) * (num_buckets ** num_categories)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be45117-4196-4fd0-a16e-a1104c0fc7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_buckets_range = range(1, 2001)\n",
    "num_categories_list = [10, 20, 30, 40, 43, 50]\n",
    "\n",
    "# Create a plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot each curve for the different number of categories\n",
    "for num_categories in num_categories_list:\n",
    "    probabilities = [category_collision(num_buckets, num_categories) for num_buckets in num_buckets_range]\n",
    "    plt.plot(num_buckets_range, probabilities, label=f'{num_categories} categories')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Number of Hash Buckets')\n",
    "plt.ylabel('Probability of Category Collision')\n",
    "plt.title('Birthday Paradox applied to Category Collision Probability')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0753315-036c-428b-9e95-5d4ff2ddc6d4",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>\n",
    "In our case, we're zooming in on the scenario with 43 categories because that's how many different <b>native_countries</b> we have in our dataset. Suppose we decide on 250 as our magic number of buckets. According to the graph (and the math behind it), if we do a simple hash with these 250 buckets, there's a 97.84% chance we'll see at least one overlap.<br>But what if we don't settle for just one try? What if we experiment with 100 different ways to assign these buckets by mixing in 100 unique \"salts\"? This strategy boosts our odds of hitting a combination without any collisions to 88.71%. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9ee7af-6b92-40c5-b77e-487712af5596",
   "metadata": {},
   "outputs": [],
   "source": [
    "salts = ['TableSalt',  'SeaSalt',  'HimalayanPinkSalt',  'KosherSalt',  'CelticSeaSalt',  'FleurdeSel',  'BlackSaltKalaNamak',  'RedHawaiianSalt',  'BlackHawaiianSalt', \n",
    " 'SmokedSalt',  'FlakeSalt',  'SelGris',  'EpsomSalt',  'DeadSeaSalt',  'BolivianRoseSalt',  'PersianBlueSalt',  'AlaeaSalt',  'MaldonSalt',  'MurrayRiverSalt', \n",
    " 'CyprusBlackLavaSalt',  'DanishSmokedSalt',  'ChardonnayOakSmokedSalt',  'HawaiianBambooJadeSalt',  'SicilianSeaSalt',  'PeruvianPinkSalt',  'SelMelange', \n",
    " 'ApplewoodSmokedSalt',  'CherrywoodSmokedSalt',  'VanillaBeanSalt',  'SzechuanPepperSalt',  'LemonFlakeSalt',  'VintageMerlotSalt',  'GhostPepperSalt', \n",
    " 'LavenderRosemarySalt',  'MatchaGreenTeaSalt',  'TruffleSalt',  'PorciniMushroomSalt',  'GarlicSalt',  'OnionSalt',  'CelerySalt',  'HabaneroSalt', \n",
    " 'EspressoSalt',  'CinnamonSpiceSalt',  'IndianBlackSalt',  'BlueCheeseSalt',  'HickorySalt',  'AlderwoodSmokedSalt',  'AnchoChileSalt',  'BasilSalt',\n",
    " 'ChiliLimeSalt',  'ChocolateSalt',  'CoconutGulaJawaSalt',  'CuminSalt',  'CurrySalt',  'FennelSalt',  'GingerSalt',  'HerbesdeProvenceSalt',  'JalapenoSalt', \n",
    " 'LimeSalt',  'MapleSalt',  'OrangeSalt',  'RoseSalt',  'SaffronSalt',  'SageSalt',  'SrirachaSalt',  'SumacSalt',  'TurmericSalt',  'WasabiSalt',\n",
    " 'WhiskeySmokedSalt',  'WineSalt',  'YuzuSalt',  \"ZaatarSalt\",  'SmokedApplewoodSalt',  'BeechwoodSmokedSalt',  'NorwegianSeaSalt',  'BrittanySeaSalt', \n",
    " 'CornishSeaSalt',  'IcelandicSeaSalt',  'KoreanBambooSalt',  'MalaysianPyramidSalt',  'MexicanSeaSalt',  'NewZealandSeaSalt',  'PortugueseSeaSalt',\n",
    " 'SouthAfricanSeaSalt',  'SpanishSeaSalt',  'ThaiFleurdeSel',  'VikingSmokedSalt',  'WelshSeaSalt',  'YakimaApplewoodSmokedSalt',  'OakSmokedSalt',  \n",
    " 'PinkPeppercornSalt',  'LemonHerbSalt',  'ChipotleSalt',  'BourbonBarrelSmokedSalt',  'AguniSeaSalt',  'AmabitoNoMoshioSeaweedSalt', \n",
    " 'BlackTruffleSeaSalt',  'CaviarSalt',  'HarvestSalt',  'HawaiianRedAlaeaSalt',  'ItalianBlackTruffleSalt',  'JapaneseMatchaSalt', \n",
    " 'OliveSalt',  'PumpkinSpiceSalt',  'RosemarySalt',  'ShiitakeMushroomSalt',  'SicilianWhiteSalt',  'TibetanRoseSalt']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442de13b-87e1-483a-8545-862c719de154",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>\n",
    "Next, we'll set up a temporary table listing all the distinct countries. We'll tweak our earlier function to consider our chosen \"salt\" by tacking it onto the end of each country name. Then, we'll run a check to see if we've managed to dodge any collisions with our new, salt-enhanced hashing method.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69b80f9-fc23-4be2-b36a-b8ae0f4b675a",
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_sql(\"\"\"\n",
    "CREATE MULTISET VOLATILE TABLE countries_t AS \n",
    "(SELECT native_country FROM DEMO_Hashing.Census_Income GROUP BY native_country )\n",
    "WITH DATA NO PRIMARY INDEX\n",
    "ON COMMIT PRESERVE ROWS\n",
    "\"\"\")\n",
    "\n",
    "DF_countries = DataFrame(\"countries_t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f81f98-6424-4eb0-8ae0-0d4202711168",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c3c27e-f277-452f-9665-2037962aa85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_hashbucket_salted(thisDF, column_name, num_buckets=10, salt = \"\"):\n",
    "    return f.abs(f.from_bytes(f.hashrow(f.concat(thisDF[column_name].expression, salt)), \"base10\" \n",
    "                             ).cast(type_=INTEGER)) % num_buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7455a715-03c4-4f3f-94fe-6ec1781fd7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_kwargs = {(f\"native_country_{salt}\") : get_feature_hashbucket_salted(DF_countries, \"native_country\",250, salt) \n",
    "                     for salt in salts}\n",
    "\n",
    "DF_countries_hashbucket = (DF_countries\n",
    "    .assign(**my_kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2d79a2-6e33-4cbe-9d64-81aa64cf574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_countries_hashbucket.to_pandas().nunique().sort_values().tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db0f119-3dc4-439b-ac40-7f03bdf1156b",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>\n",
    "Great news: we've got options on the table! Just like picking between table salt, kosher salt, matcha green tea salt, olive salt, or fennel salt to flavor our dishes, we can choose our \"salt\" for hashing to get that perfect, collision-free categorical encoding. And the best part? We don't need a massive number of buckets to make it happen. It's all about your preference now, like choosing the right seasoning for your meal.<br>\n",
    "Sure, it might sound like extra steps to take, but it's absolutely worth it when you're aiming to fine-tune your model or speed things up in production, especially when there are strict performance requirements to meet. Think of it as the secret ingredient that could give your model the edge it needs, ensuring it runs smoothly and quickly, just when you need it to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7407fdbb-6163-4404-8d46-f89300860dec",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>Conclusion</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>\n",
    "    Hashing might not be a familiar concept to everyone, but understanding and leveraging it can be a real game-changer if you know how to use it and when. In this blog post, we've taken a deep dive into how hashing works and why it's so important, especially when dealing with huge amounts of data in Teradata Vantage. We explored four key use cases: anonymizing data to protect privacy, splitting data sets for model training, and two ways of encoding categorical data to make it easier for machines to understand.<br>\n",
    "We started with the basics, showing how hashing turns any input into a fixed-size string, a bit like giving every piece of data its own unique fingerprint. This process is crucial for handling data quickly and safely. From there, we saw how hashing helps keep personal information private, ensures data is divided fairly for machine learning, and simplifies complex data into a format that's easy to work with, even introducing a clever \"salt\" trick to avoid mixing up different pieces of data.<br>\n",
    "Overall, we have shown that while hashing might seem a bit technical or obscure, it's actually a powerful tool in data science. It can make big data tasks more manageable, secure, and efficient, proving its value across a range of scenarios. So, the next time you're working with data, consider how hashing might help you achieve your goals more effectively.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1e1836-a045-499a-ac47-6b0a1f58c01a",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>8. Cleanup</b></p>\n",
    "<p style = 'font-size:18px;font-family:Arial;color:##00233C'><b>Work Tables</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbca9b4c-bf18-4ed8-84ab-e4a1390c60de",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    db_drop_table(countries_t)\n",
    "except:\n",
    "     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3d3ac5-1f5f-4f0e-bba3-710323633eb2",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial'><b>Databases and Tables</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The following code will clean up tables and databases created above.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93704149-5c5f-4076-bb83-9fd8680f3372",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../../UseCases/run_procedure.py \"call remove_data('DEMO_Hashing');\" \n",
    "#Takes 40 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360d0f97-3b07-4f91-bdb0-0922ca0ca089",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fba8bf6-1173-4a1d-baeb-6fa97d036e96",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "<p style = 'font-size:16px;font-family:Arial'><b>Dataset</b><br>We have used Adult dataset, also known as the \"Census Income\" dataset from the UCI Machine Learning Repository <a href='https://archive.ics.uci.edu/dataset/2/adult'>here</a>. It comprises 48,842 instances with 14 features, aimed at predicting whether an individual's income exceeds $50,000 per year based on census data. The dataset includes a mix of categorical and integer feature types, covering demographic attributes such as age, work class, education, marital status, occupation, relationship, race, and sex.</p>\n",
    "<p style = 'font-size:16px;font-family:Arial'><b>Links:</b></p>\n",
    "<ul style = 'font-size:16px;font-family:Arial'>\n",
    "    <li>Medium Blog Posts: <a href = 'https://medium.com/teradata'>here</a></li>\n",
    "    \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ef60a9-850f-4df9-b252-5f5c6cc71edd",
   "metadata": {},
   "source": [
    "<footer style=\"padding-bottom:35px; border-bottom:3px solid #91A0Ab\">\n",
    "    <div style=\"float:left;margin-top:14px\">ClearScape Analytics™</div>\n",
    "    <div style=\"float:right;\">\n",
    "        <div style=\"float:left; margin-top:14px\">\n",
    "            Copyright © Teradata Corporation - 2025. All Rights Reserved\n",
    "        </div>\n",
    "    </div>\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
